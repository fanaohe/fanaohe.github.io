<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo进阶加部署]]></title>
    <url>%2F2019%2F06%2F28%2FHexo%E8%BF%9B%E9%98%B6%E5%8A%A0%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1. 创建“分类”页面 新建分类页面 1hexo new page categories 给分类页面添加类型 我们在source文件夹中的categories文件夹下找到index.md文件，并在它的头部加上type属性。 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot; #这部分是新添加的--- 给模板添加分类属性 现在我们打开scarffolds文件夹里的post.md文件，给它的头部加上categories:，这样我们创建的所有新的文章都会自带这个属性，我们只需要往里填分类，就可以自动在网站上形成分类了。 1234title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags: 给文章添加分类 现在我们可以找到一篇文章，然后尝试给它添加分类 12345layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express] 2. 创建“标签”页面创建”标签”页的方式和创建“分类”一样。 新建“标签”页面 1hexo new page tags 给标签页面添加类型 我们在source文件夹中的tags文件夹下找到index.md文件，并在它的头部加上type属性。 123title: tagsdate: 2018-08-06 22:48:29type: &quot;tags&quot; #新添加的内容 给文章添加标签 有两种写法都可以，第一种是类似数组的写法，把标签放在中括号[]里，用英文逗号隔开 12345layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: [node.js, express] 第二种写法是用-短划线列出来 1234567layout: poststitle: 写给小白的express学习笔记1： express-static文件静态管理date: 2018-06-07 00:38:36categories: 学习笔记tags: - node.js- express 部署域名紧接着我们就可以把这些内容添加到Github页面上，然后生成我们自己的博客了。 部署Github 首先你必须有一个github账号 然后新建一个仓库，这一有第一个坑，项目名一定要是用户名.github.io的形式 这个时候github页面其实就生成好了，但是我们的内容还需要同步到github上，所以打开hexo文件夹里的配置文件config.yml，添加部署路径 1234deploy: type: git repository: https://github.com/qYinchunhui/qyinchunhui.github.io.git branch: master 这里注意两小点： 属性和内容之间一定要有一个空格，配置文件有自己的格式规范 如果你之前没有用git关联过自己的github库，需要配置SSH等参数，否则无法成功，这部分搜git就有很多相关教程 123hexo clean #清除缓存hexo g #打包hexo d #部署]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建个人博客]]></title>
    <url>%2F2019%2F06%2F28%2Fhexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[安装前提安装Hexo之前，必须保证自己的电脑中已经安装好了Node.js和Git。因为这两个软件我之前都安装过，这里就不重复安装过程了，检验方式如下： 12git --versionnode -v 安装Hexo安装好node.js和git后，可以通过npm来安装Hexo。 1npm install -g hexo-cli 建站12hexo init #新建文件夹，初始化hexo项目npm install #如果提示下载依赖运行此命令，没提示不用运行 config.yml博客的配置文件，博客的名称、关键词、作者、语言、博客主题…设置都在里面。 package.json应用程序信息，新添加的插件内容也会出现在这里面，我们可以不修改这里的内容。 scaffoldsscaffolds就是脚手架的意思，这里放了三个模板文件，分别是新添加博客文章（posts）、新添加博客页（page）和新添加草稿（draft）的目标样式。 这部分可以修改的内容是，我们可以在模板上添加比如categories等自定义内容 sourcesource是放置我们博客内容的地方，里面初始只有两个文件夹，一个是drafts（草稿），一个posts（文章），但之后我们通过命令新建tags（标签）还有categories（分类）页后，这里会相应地增加文件夹。 themes放置主题文件包的地方。Hexo会根据这个文件来生成静态页面。 初始状态下只有landscape一个文件夹，后续我们可以添加自己喜欢的。 Hexo命令1234hexo new &quot;名称&quot; #新建md文件hexo generate / hexo g #生成静态页面hexo deploy / hexo d #部署hexo server / hexo s #启动 localhost:4000 访问自己博客]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码中遇到的时区问题]]></title>
    <url>%2F2019%2F06%2F10%2F%E7%BC%96%E7%A0%81%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[编码中遇到的时区问题最近在编码的过程中遇到这样一个问题： 我需要获取用户今天是否刷了卡，如果已经刷了一次就禁止再刷，也就是让一张卡一天只能刷一次，但发现用户在早上8点之前无法刷卡，过了8点之后就可以刷一次了。 最终发现原来是服务器时区设置问题，之前我获取当前日期的代码为 import timetime.strftime(“%Y-%m-%d”, time.localtime())这样获取的是本机设置的时区时间，如果本机设置的时区不是东八区那就悲剧了。 于是改进一下： time.strftime(“%Y-%m-%d”, time.gmtime(int(time.time()+8*3600)))这样就固定获取东八区时间，也就是北京时间了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM——百度百科]]></title>
    <url>%2F2019%2F05%2F18%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E2%80%94%E2%80%94%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%2F</url>
    <content type="text"><![CDATA[支持向量机SVM——百度百科支持向量机（英语：Support Vector Machine, 简称SVM)，是一种有监督学习方法，可被广泛应用于统计分类以及线性回归。 Vapnik等人在多年研究统计学习理论基础上对线性分类器提出了另一种设计最佳准则。其原理也从线性可分说起，然后扩展到线性不可分的情况。甚至扩展到使用非线性函数中去，这种分类器被称为支持向量机。 主要思想 ⑴ 它是针对线性可分情况进行分析，对于线性不可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征 空间使其线性可分，从而使得高维特征空间采用线性算法对样本的非线性特征进行线性分析成为可能； 举例：如下图：将1维的“线性不可分”上升到2维后就成为线性可分了。 ⑵ 它基于结构风险最小化理论之上在特征空间中建构最优分割超平面，使得学习器得到全局最优化，并且在整个样本空间的期望风险以某个概率满足一定上界。 一般特征 ⑴ SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。 ⑵ SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。 ⑶ 通过对数据中每个分类属性引入一个哑变量，SVM可以应用于分类数据。 ⑷ SVM一般只能用在二类问题，对于多类问题效果不好。 原理简介 SVM方法是通过一个非线性映射p，把样本空间映射到一个高维乃至无穷维的特征空间中（Hilbert空间），使得在原来的样本空间中非线性可分的问题转化为在特征空间中的线性可分的问题。简单地说，就是升维和线性化。升维，就是把样本向高维空间做映射，一般情况下这会增加计算的复杂性，甚至会引起“维数灾难”，因而人们很少问津。但是作为分类、回归等问题来说，很可能在低维样本空间无法线性处理的样本集，在高维特征空间中却可以通过一个线性超平面实现线性划分（或回归）。一般的升维都会带来计算的复杂化，SVM方法巧妙地解决了这个难题：应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”．这一切要归功于核函数的展开和计算理论。 常用核函数 ⑴ 线性核函数K(x,y)=x·y； ⑵ 多项式核函数K(x,y)=[(x·y)+1]d； ⑶ 径向基函数K(x,y)=exp(-|x-y|^2/d^2） ⑷ 二层神经网络核函数K(x,y)=tanh(a(x·y)+b）．]]></content>
      <categories>
        <category>svm</category>
      </categories>
      <tags>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonC3线性算法]]></title>
    <url>%2F2019%2F04%2F19%2FpythonC3%E7%BA%BF%E6%80%A7%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[pythonC3线性算法在介绍算法之前，我们首先约定需要使用的符号。我们用 C1 C2⋯CN head(C1 C2⋯CN)=C1 为了方便做列表连接操作，我们记： 假设类 C 这个公式表明 C 接下来我们介绍 C3 线性化中最重要的操作 merge 选取 merge令 h=head(K)否则，设置 K如果 merge上面的过程看起来好像很复杂，我们用一个例子来具体执行一下，你就会觉得其实还是挺简单的。假设我们有如下的一个类继承关系： class A(object): def __init__(self): print(&quot;enter A&quot;) super(A, self).__init__() print(&quot;leave A&quot;)class B(object): def __init__(self): print(&quot;enter B&quot;) super(B, self).__init__() print(&quot;leave B&quot;)class C(object): def __init__(self): print(&quot;enter C&quot;) super(C, self).__init__() print(&quot;leave C&quot;)class D(A): def __init__(self): print(&quot;enter D&quot;) super(D, self).__init__() print(&quot;leave D&quot;)class E(A, C): def __init__(self): print(&quot;enter E&quot;) super(E, self).__init__() print(&quot;leave E&quot;)class F(D, B): def __init__(self): print(&quot;enter F&quot;) super(F, self).__init__() print(&quot;leave F&quot;)class G(E, F): def __init__(self): print(&quot;enter G&quot;) super(G, self).__init__() print(&quot;leave G&quot;)“””l[F(D, B)] = F + merge(l(D), l(B), DB) F + merge([D, A], B, [D, B]) [F, D, A, B]l[G(E, F)] = G + merge(l(E), l(F), EF) G + merge([E, A, C], [F, D, A, B], [E, F]) [G, E, F, D, A, C, B] “”” print(G.mro())]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means]]></title>
    <url>%2F2019%2F04%2F16%2FK-means%2F</url>
    <content type="text"><![CDATA[K-means介绍K-means算法是是最经典的聚类算法之一，它的优美简单、快速高效被广泛使用。它是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。 图示 步骤从N个文档随机选取K个文档作为质心对剩余的每个文档测量其到每个质心的距离，并把它归到最近的质心的类重新计算已经得到的各个类的质心迭代2～3步直至新的质心与原质心相等或小于指定阈值，算法结束优点 k-平均算法是解决聚类问题的一种经典算法，算法简单、快速。对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O（nkt），其中n是所有对象的数目，k是簇的数目，t是迭代的次数。通常k&lt;&lt;n。这个算法经常以局部最优结束。算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，而簇与簇之间区别明显时，它的聚类效果很好。缺点 K 是事先给定的，这个 K 值的选定是非常难以估计的；对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。一旦初始值选择的不好，可能无法得到有效的聚类结果；该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。不适合于发现非凸面形状的簇，或者大小差别很大的簇；对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。改进 http://tech.ddvip.com/2013-12/1387352834207177.html]]></content>
      <categories>
        <category>K-means</category>
      </categories>
      <tags>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang开发环境]]></title>
    <url>%2F2019%2F04%2F09%2Fgolang%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[golang开发环境1.1 go环境安装 1、go下载安装 官方：https://golang.org/dl 国内： https://golang.google.cn/dl/ https://www.golangtc.com/download 调整go环境变量 我的电脑 –&gt; 高级 –&gt; 环境变量 1. 首先要把go安装目录下的bin目录放到Path环境变量中，并添加我们代码的存放位置（系统变量path路径） C:\Go\bin # go安装路径 C:\onlineGo # 我们代码存放路径 2. 指定go编译器安装的目录（GOROOT 默认安装时就是这个路径 用户变量） C:\go 3. 我自己写的代码要放到这个变量中配置的目录中，go编译器才会找到并编译（GOPATH 用户变量） C:\onlineGo # 指定GOPATH路径 C:\onlineGo\src # src 我们写的代码路径 C:\onlineGo\pkg # pkg 生成的临时文件、库文件 C:\onlineGo\bin # go build 生成的bin文件都在这里 4. 确认我们修改的路径（go env） 2、goland开发环境（ide） 1. goland开发环境： https://www.jetbrains.com/go/download/#section=windows]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7配置iscsi]]></title>
    <url>%2F2019%2F03%2F19%2Fcentos7%E9%85%8D%E7%BD%AEiscsi%2F</url>
    <content type="text"><![CDATA[centos7配置iscsiiSCSI stands for Internet Small Computer Systems Interface, an IP-based storage, works on top of internet protocol by carrying SCSI commands over IP network. iSCSI transports block-level data between an iSCSI initiator on a client machine and an iSCSI target on a storage device (server). Environment:Server: server.itzgeek.localIP Address: 192.168.12.20Client: node1.itzgeek.localIP Address: 192.168.12.11Storage Configuration:Here, we will create 5GB of LVM disk on the target server to use as a shared storage for clients. Let’s list the available disks attached to the target server using below command. If you want to use the whole disk for LVM, skip the below step. [root@server ~]# fdisk -l | grep -i sd Disk /dev/sda: 107.4 GB, 107374182400 bytes, 209715200 sectors /dev/sda1 * 2048 1026047 512000 83 Linux /dev/sda2 1026048 209715199 104344576 8e Linux LVM Disk /dev/sdb: 10.7 GB, 10737418240 bytes, 20971520 sectorsFrom the above output, you can see that my system has a 10GB of disk (/dev/sdb). We will create a 5GB partition on the above disk and will use it for LVM. [root@server ~]# fdisk /dev/sdb Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0x173dfa4d. Command (m for help): n –&gt; New partition Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p –&gt; Pimary partition Partition number (1-4, default 1): 1 - -&gt; Partition number First sector (2048-20971519, default 2048): –&gt; Just enter Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-20971519, default 20971519): +5G –&gt; Enter the size Partition 1 of type Linux and of size 5 GiB is set Command (m for help): t –&gt; Change label Selected partition 1 Hex code (type L to list all codes): 8e –&gt; Change it as LVM label Changed type of partition ‘Linux’ to ‘Linux LVM’ Command (m for help): w –&gt; Save The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks.Create a LVM with /dev/sdb1 partition (replace /dev/sdb1 with your disk name) [root@server ~]# pvcreate /dev/sdb1[root@server ~]# vgcreate vg_iscsi /dev/sdb1[root@server ~]# lvcreate -l 100%FREE -n lv_iscsi vg_iscsiCreating iSCSI target:Now you have an option of creating target either with authentication or without authentication. In this article, you can find steps for both scenarios. It is up to you to decide which one is suitable for your environment. Configuring iSCSI Targets with CHAP authentication:Install the targetcli package on the server. [root@server ~]# yum install targetcli -yOnce you installed the package, enter below command to get a iSCSI CLI for an interactive prompt. [root@server ~]# targetcliWarning: Could not load preferences file /root/.targetcli/prefs.bin.targetcli shell version 2.1.fb41Copyright 2011-2013 by Datera, Inc and others.For help on commands, type ‘help’. Now use an existing logical volume (/dev/vg_iscsi/lv_iscsi) as a block-type backing store for storage object “scsi_disk1_server“. /&gt; cd backstores/block/backstores/block&gt; create scsi_disk1_server /dev/vg_iscsi/lv_iscsiCreated block storage object scsi_disk1_server using /dev/vg_iscsi/lv_iscsi.Create a target. /backstores/block&gt; cd /iscsi/iscsi&gt; create iqn.2016-02.local.itzgeek.server:disk1Created target iqn.2016-02.local.itzgeek.server:disk1.Created TPG 1.Global pref auto_add_default_portal=trueCreated default portal listening on all IPs (0.0.0.0), port 3260./iscsi&gt;Create ACL for client machine (It’s the IQN which clients use to connect). /iscsi&gt; cd /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1/acls/iscsi/iqn.20…sk1/tpg1/acls&gt; create iqn.2016-02.local.itzgeek.server:node1node2Created Node ACL for iqn.2016-02.local.itzgeek.server:node1node2Set CHAP authentication. /iscsi/iqn.20…sk1/tpg1/acls&gt; cd iqn.2016-02.local.itzgeek.server:node1node2/iscsi/iqn.20…er:node1node2&gt; set auth userid=userParameter userid is now ‘user’./iscsi/iqn.20…er:node1node2&gt; set auth password=passwordParameter password is now ‘password’.Create a LUN under the target, The LUN should use the previously mentioned backing storage object named “scsi_disk1_server” /iscsi/iqn.20…er:node1node2&gt; cd /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1/luns/iscsi/iqn.20…sk1/tpg1/luns&gt; create /backstores/block/scsi_disk1_serverCreated LUN 0.Created LUN 0-&gt;0 mapping in node ACL iqn.2016-02.local.itzgeek.server:node1node2Verify the target server configuration. /iscsi/iqn.20…/tpg1/portals&gt; cd //&gt; lso- / …………………………………………………………………………………………………………. […] o- backstores ……………………………………………………………………………………………….. […] | o- block …………………………………………………………………………………….. [Storage Objects: 1] | | o- scsi_disk1_server ………………………………………….. [/dev/vg_iscsi/lv_iscsi (5.0GiB) write-thru activated] | o- fileio ……………………………………………………………………………………. [Storage Objects: 0] | o- pscsi …………………………………………………………………………………….. [Storage Objects: 0] | o- ramdisk …………………………………………………………………………………… [Storage Objects: 0] o- iscsi ……………………………………………………………………………………………… [Targets: 1] | o- iqn.2016-02.local.itzgeek.server:disk1 …………………………………………………………………. [TPGs: 1] | o- tpg1 ………………………………………………………………………………….. [no-gen-acls, no-auth] | o- acls ……………………………………………………………………………………………. [ACLs: 1] | | o- iqn.2016-02.local.itzgeek.server:node1node2 …………………………………………………. [Mapped LUNs: 1] | | o- mapped_lun0 …………………………………………………………… [lun0 block/scsi_disk1_server (rw)] | o- luns ……………………………………………………………………………………………. [LUNs: 1] | | o- lun0 ……………………………………………………… [block/scsi_disk1_server (/dev/vg_iscsi/lv_iscsi)] | o- portals ………………………………………………………………………………………. [Portals: 1] | o- 0.0.0.0:3260 ……………………………………………………………………………………….. [OK] o- loopback …………………………………………………………………………………………… [Targets: 0]Save and exit from target CLI. /&gt; saveconfigLast 10 configs saved in /etc/target/backup.Configuration saved to /etc/target/saveconfig.json/&gt; exitGlobal pref auto_save_on_exit=trueLast 10 configs saved in /etc/target/backup.Configuration saved to /etc/target/saveconfig.jsonEnable and restart the target service. [root@server ~]# systemctl enable target.service[root@server ~]# systemctl restart target.serviceConfigure the firewall to allow iSCSI traffic. [root@server ~]# firewall-cmd –permanent –add-port=3260/tcp[root@server ~]# firewall-cmd –reloadConfigure Initiator with CHAP authentication:Now it’s the time to configure a client machine to use this target as a storage, install below package on the client machine (node1). [root@node1 ~]# yum install iscsi-initiator-utils -yEdit below file and add iscsi initiator name. [root@node1 ~]# vi /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2016-02.local.itzgeek.server:node1node2Discover the target using below command. [root@node1 ~]# iscsiadm -m discovery -t st -p 192.168.12.20 192.168.12.20:3260,1 iqn.2016-02.local.itzgeek.server:disk1Enable CHAP authentication in iscsid.conf file, uncomment and modify the green colored lines as per your environment. [root@node1 ~]# vi /etc/iscsi/iscsid.conf To enable CHAP authentication set node.session.auth.authmethodto CHAP. The default is None.node.session.auth.authmethod = CHAP To set a CHAP username and password for initiatorauthentication by the target(s), uncomment the following lines:node.session.auth.username = usernode.session.auth.password = passwordRestart and enable the initiator service. [root@node1 ~]# systemctl restart iscsid.service[root@node1 ~]# systemctl enable iscsid.serviceLogin to the discovered target. [root@node1 ~]# iscsiadm -m node -T iqn.2016-02.local.itzgeek.server:disk1 -p 192.168.12.20 -lLogging in to [iface: default, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] (multiple)Login to [iface: default, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] successful.Skip to creating File System. Configuring iSCSI Targets without CHAP Authentication:Install the targetcli package on the server. [root@server ~]# yum install targetcli -yOnce you installed the package, enter below command to get a iSCSI CLI for an interactive prompt. [root@server ~]# targetcliWarning: Could not load preferences file /root/.targetcli/prefs.bin.targetcli shell version 2.1.fb41Copyright 2011-2013 by Datera, Inc and others.For help on commands, type ‘help’. Now use an existing logical volume (/dev/vg_iscsi/lv_iscsi) as a block-type backing store for storage object “scsi_disk1_server“. /&gt; cd backstores/block/backstores/block&gt; create scsi_disk1_server /dev/vg_iscsi/lv_iscsiCreated block storage object scsi_disk1_server using /dev/vg_iscsi/lv_iscsi.Create a target. /backstores/block&gt; cd /iscsi/iscsi&gt; create iqn.2016-02.local.itzgeek.server:disk1Created target iqn.2016-02.local.itzgeek.server:disk1.Created TPG 1.Global pref auto_add_default_portal=trueCreated default portal listening on all IPs (0.0.0.0), port 3260./iscsi&gt;Create ACL for client machine (It’s the IQN which clients use to connect). /iscsi&gt; cd /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1/acls/iscsi/iqn.20…sk1/tpg1/acls&gt; create iqn.2016-02.local.itzgeek.server:node1node2Created Node ACL for iqn.2016-02.local.itzgeek.server:node1node2By default authentication is enabled, disable it. /iscsi/iqn.20…sk1/tpg1/acls&gt; cd /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1//iscsi/iqn.20…er:disk1/tpg1&gt; set attribute authentication=0Parameter authentication is now ‘0’./iscsi/iqn.20…er:disk1/tpg1&gt; set attribute generate_node_acls=1Parameter generate_node_acls is now ‘1’.Create a LUN under the target, The LUN should use the previously mentioned backing storage object named “scsi_disk1_server” /iscsi/iqn.20…er:disk1/tpg1&gt; cd /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1/luns/iscsi/iqn.20…sk1/tpg1/luns&gt; create /backstores/block/scsi_disk1_serverCreated LUN 0.Created LUN 0-&gt;0 mapping in node ACL iqn.2016-02.local.itzgeek.server:node1node2Verify the target server configuration. /iscsi/iqn.20…/tpg1/portals&gt; cd //&gt; lso- / …………………………………………………………………………………………………………. […] o- backstores ……………………………………………………………………………………………….. […] | o- block …………………………………………………………………………………….. [Storage Objects: 1] | | o- scsi_disk1_server ………………………………………….. [/dev/vg_iscsi/lv_iscsi (5.0GiB) write-thru activated] | o- fileio ……………………………………………………………………………………. [Storage Objects: 0] | o- pscsi …………………………………………………………………………………….. [Storage Objects: 0] | o- ramdisk …………………………………………………………………………………… [Storage Objects: 0] o- iscsi ……………………………………………………………………………………………… [Targets: 1] | o- iqn.2016-02.local.itzgeek.server:disk1 …………………………………………………………………. [TPGs: 1] | o- tpg1 …………………………………………………………………………………….. [gen-acls, no-auth] | o- acls ……………………………………………………………………………………………. [ACLs: 1] | | o- iqn.2016-02.local.itzgeek.server:node1node2 …………………………………………………. [Mapped LUNs: 1] | | o- mapped_lun0 …………………………………………………………… [lun0 block/scsi_disk1_server (rw)] | o- luns ……………………………………………………………………………………………. [LUNs: 1] | | o- lun0 ……………………………………………………… [block/scsi_disk1_server (/dev/vg_iscsi/lv_iscsi)] | o- portals ………………………………………………………………………………………. [Portals: 1] | o- 0.0.0.0:3260 ……………………………………………………………………………………….. [OK] o- loopback …………………………………………………………………………………………… [Targets: 0]Save and exit from target CLI./&gt; saveconfigLast 10 configs saved in /etc/target/backup.Configuration saved to /etc/target/saveconfig.json/&gt; exitGlobal pref auto_save_on_exit=trueLast 10 configs saved in /etc/target/backup.Configuration saved to /etc/target/saveconfig.jsonEnable and restart the target service. [root@server ~]# systemctl enable target.service[root@server ~]# systemctl restart target.serviceConfigure the firewall to allow iSCSI traffic. [root@server ~]# firewall-cmd –permanent –add-port=3260/tcp[root@server ~]# firewall-cmd –reloadConfigure Initiator without CHAP authentication:Now it’s the time to configure a client machine to use this target as a storage, install below package on the client machine (node1). [root@node1 ~]# yum install iscsi-initiator-utils -yEdit below file and add iscsi initiator name. [root@node1 ~]# vi /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2016-02.local.itzgeek.server:node1node2Discover the target using below command. [root@node1 ~]# iscsiadm -m discovery -t st -p 192.168.12.20 192.168.12.20:3260,1 iqn.2016-02.local.itzgeek.server:disk1Restart and enable the initiator service. [root@node1 ~]# systemctl restart iscsid.service[root@node1 ~]# systemctl enable iscsid.serviceLogin to the discovered target. [root@node1 ~]# iscsiadm -m node -T iqn.2016-02.local.itzgeek.server:disk1 -p 192.168.12.20 -lLogging in to [iface: default, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] (multiple)Login to [iface: default, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] successful.Creating File System:After login (connecting) to discovered target, have a look at messages file. You would find similar output like below, from where you can find a name of the disk. [root@node1 ~]# cat /var/log/messagesFeb 23 14:54:47 node2 kernel: sd 34:0:0:0: [sdb] 10477568 512-byte logical blocks: (5.36 GB/4.99 GiB)Feb 23 14:54:47 node2 kernel: sd 34:0:0:0: [sdb] Write Protect is offFeb 23 14:54:47 node2 kernel: sd 34:0:0:0: [sdb] Write cache: disabled, read cache: enabled, doesn’t support DPO or FUAFeb 23 14:54:48 node2 kernel: sdb: unknown partition tableFeb 23 14:54:48 node2 kernel: sd 34:0:0:0: [sdb] Attached SCSI diskFeb 23 14:54:48 node2 iscsid: Could not set session2 priority. READ/WRITE throughout and latency could be affected.Feb 23 14:54:48 node2 iscsid: Connection2:0 to [target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] through [iface: default] is operational now List down the attached disks. [root@node1 ~]# cat /proc/partitionsmajor minor #blocks name 8 0 104857600 sda 8 1 512000 sda1 8 2 104344576 sda2 11 0 1048575 sr0 253 0 2113536 dm-0 253 1 52428800 dm-1 253 2 49799168 dm-2 8 16 5238784 sdbFormat the new disk (for sake of article, I have formated whole disk instead of creating partition) root@node1 ~]# mkfs.xfs /dev/sdbmeta-data=/dev/sdb isize=256 agcount=8, agsize=163712 blks = sectsz=512 attr=2, projid32bit=1 = crc=0data = bsize=4096 blocks=1309696, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=0log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 Mount the disk. [root@node1 ~]# mount /dev/sdb /mntverify the disk is mounted using below command. [root@node1 ~]# df -hTFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/centos-root xfs 50G 955M 50G 2% /devtmpfs devtmpfs 908M 0 908M 0% /devtmpfs tmpfs 914M 54M 861M 6% /dev/shmtmpfs tmpfs 914M 8.5M 905M 1% /runtmpfs tmpfs 914M 0 914M 0% /sys/fs/cgroup/dev/mapper/centos-home xfs 48G 33M 48G 1% /home/dev/sda1 xfs 497M 97M 401M 20% /boot/dev/sdb xfs 5.0G 33M 5.0G 1% /mntIn case you want to de-attach the added disk, please follow the procedure (unmount and logout). [root@node1 ~]# umount /mnt/[root@node1 ~]# iscsiadm -m node -T iqn.2016-02.local.itzgeek.server:disk1 -p 192.168.12.20 -uLogging out of session [sid: 1, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260]Logout of [sid: 1, target: iqn.2016-02.local.itzgeek.server:disk1, portal: 192.168.12.20,3260] successful.That’s All.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7+mysql5.7.11实现主从复制]]></title>
    <url>%2F2019%2F02%2F28%2Fcentos7%2Bmysql5.7.11%E5%AE%9E%E7%8E%B0%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[centos7+mysql5.7.11实现主从复制1 首先检测当前的系统是否已经安装了MySQL yum list installed | grep mysql 如果有的话，删除 2 下载rpm库资源，在网页 https://dev.mysql.com/downloads/repo/yum/ 中可以自己选择合适的版本， 然后安装 因为本人的版本是centos 7 ，所以： wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm yum localinstall mysql57-community-release-el7-11.noarch.rpm 如果不想安装最新版本的mysql，可以参考 https://translate.google.cn/#en/zh-CN/MySQL Cluster CGE 进行设置。 3 安装并启动mysql yum install mysql-community-server service mysqld start 4 初始的可能的mysql配置 mysql安装后默认会生成一个随机密码，我们可以通过以下命令进行查看： grep password /var/log/mysql.log 然后使用登录mysql mysql -u root -p 修改密码： SET PASSWORD = PASSWORD(“新密码”) 由于mysql可能(默认)加载了密码验证插件，所以如果新密码过于简单可能会报错。 解决方法： set global validate_password_policy = 0; // 这样将验证规则改为基于长度 set global validate_password_length = 1; // 设置密码最短长度，本例中设置为1 还有其他解决办法，详情请看：https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html 创建主从复制用户并授权： grant replication slave on . to mncu@hostname identified by ‘password’; 5 关闭selinux setenforce 0 vim /etc/selinux/config 将SELINUX参数设置为disabled 6 防火墙设置 因为centos默认的防火墙是Firewall而不是iptables，所以设置一下。 firewall-cmd –add-port=3306/tcp firewall-cmd –permanent –add-port=3306/tcp firewall-cmd –reload success 7 配置mysql配置文件。 主： vim /etc/my.cnf 添加以下几行： log-bin=mysql-bin # slave会基于此log-bin来做replication server-id=1 # master的标示 innodb_flush_log_at_trx_commit=1 sync_binlog=1 从： vim /etc/my.cnf 添加该行： server-id=2 #slave的标示8 配置主从： 主： show master status; 会显示如下信息： File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +——————+———-+————–+——————+——————-+ | mysql-bin.000001 | 329 | | | 从： stop slave; change master to master_user=’mncu’,master_password=’123456’,master_host=’192.168.1.11’,master_port=3306,master_log_file=’mysql-bin.000001’,master_log_pos=329; start slave;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql索引问题]]></title>
    <url>%2F2019%2F02%2F21%2Fmysql%E7%B4%A2%E5%BC%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[mysql索引问题索引用于快速找出在某个列中有一特定值的行。不使用索引，MySQL必须从第1条记录开始然后读完整个表直到找出相关的行，还需要考虑每次读入数据页的IO开销。而如果采取索引，则可以根据索引指向的页以及记录在页中的位置，迅速地读取目标页进而获取目标记录。 大多数情况下都（默认）采用B树来构建索引。只是空间列类型的索引使用R-树，并且MEMORY表还支持hash索引。B树是平衡多叉树，每个节点存放多少个值取决于值所占的空间，这与每一张数据页存放多少条记录与记录信息量有关同理。节点中的值是以非降序进行排列的，节点中的值总是小于等于指向它的结点中的值。 MySQL使用B树构造索引的情况下，是由叶子指向具体的页和记录的。并且一个叶子有一个指针指向下一个叶子。 使用索引需要注意： ⑴只对WHERE和ORDER BY需要查询的字段设置索引，避免无意义的硬盘开销； ⑵组合索引支持前缀索引； ⑶更新表的时候，如增删记录，MySQL会自动更新索引，保持树的平衡；因此更多的索引意味着更多的维护成本 索引的字段类型问题 text类型，也可建索引（需指定长度）myisam存储引擎索引键长度综合不能超过1000字节用来筛选的值尽量保持和索引列同样的数据类型索引分四类： index —-普通的索引,数据可以重复 fulltext—-全文索引，用来对大表的文本域(char，varchar，text)进行索引。语法和普通索引一样。 unique —-唯一索引,唯一索引,要求所有记录都唯一 primary key —-主键索引,也就是在唯一索引的基础上相应的列必须为主键 like 不能用索引？ 尽量减少like，但不是绝对不可用，”xxxx%” 是可以用到索引的，想象一下，你在看一本成语词典，目录是按成语拼音顺序建立，查询需求是，你想找以 “一”字开头的成语（”一%“），和你想找包含一字的成语（“%一%”） 除了like，以下操作符也可用到索引：&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN &lt;&gt;，not in ，！=则不行 原则 1，单表数据太少，索引反而会影响速度；更新非常频繁的数据不适宜建索引 2，where后的条件，order by ,group by 等这样过滤时，后面的字段最好加上索引。根据实际情况，选择PRIMARY KEY、UNIQUE、INDEX等索引，但是不是越多越好，要适度 3，联合查询，子查询等多表操作时关连字段要加索引 ps：数据量特别大的时候，最好不要用联合查询，即使你做了索引 多列查询该如何建索引?一次查询只能用到一个索引，所以 首先枪毙 a，b各建索引方案 a还是b？ 谁的区分度更高（同值的最少），建谁！ 当然，联合索引也是个不错的方案，ab，还是ba，则同上，区分度高者，在前 联合索引的问题?where a = “xxx” 可以使用 AB 联合索引where b = “xxx” 则不可 （再想象一下，这是书的目录？） 所以，大多数情况下，有AB索引了，就可以不用在去建一个A索引了 详解： 联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持a | a,b| a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。 两个或更多个列上的索引被称作复合索引。利用索引中的附加列，您可以缩小搜索的范围，但使用一个具有两列的索引 不同于使用两个单独的索引。复合索引的结构与电话簿类似，人名由姓和名构成，电话簿首先按姓氏对进行排序，然后按名字对有相同姓氏的人进行排序。如果您知 道姓，电话簿将非常有用；如果您知道姓和名，电话簿则更为有用，但如果您只知道名不姓，电话簿将没有用处。所以说创建复合索引时，应该仔细考虑列的顺序。对索引中的所有列执行搜索或仅对前几列执行搜索时，复合索引非常有用；仅对后面的任意列执行搜索时，复合索引则没有用处。如：建立 姓名、年龄、性别的复合索引。 create table test(a int,b int,c int,KEY a(a,b,c)); 优: select * from test where a=10 and b&gt;50差: select * from test where a50 优: select * from test order by a差: select * from test order by b差: select * from test order by c 优: select * from test where a=10 order by a优: select * from test where a=10 order by b差: select * from test where a=10 order by c 优: select * from test where a&gt;10 order by a差: select * from test where a&gt;10 order by b差: select * from test where a&gt;10 order by c 优: select * from test where a=10 and b=10 order by a优: select * from test where a=10 and b=10 order by b优: select * from test where a=10 and b=10 order by c 优: select * from test where a=10 and b=10 order by a优: select * from test where a=10 and b&gt;10 order by b差: select * from test where a=10 and b&gt;10 order by c 1.索引越少越好原因：主要在修改数据时，第个索引都要进行更新，降低写速度。2.最窄的字段放在键的左边3.避免file sort排序，临时表和表扫描.哪些常见情况不能用索引?like “%xxx”not in ， ！=对列进行函数运算的情况（如 where md5(password) = “xxxx”）WHERE index=1 OR A=10存了数值的字符串类型字段（如手机号），查询时记得不要丢掉值的引号，否则无法用到该字段相关索引，反之则没关系也即 select * from test where mobile = 13711112222; 可是无法用到mobile字段的索引的哦（如果mobile是char 或 varchar类型的话） btw，千万不要尝试用int来存手机号（为什么？自己想！要不自己试试） 覆盖索引(Covering Indexes)拥有更高效率索引包含了所需的全部值的话，就只select 他们，换言之，只select 需要用到的字段，如无必要，可尽量避免select * NULL 的问题NULL会导致索引形同虚设，所以在设计表结构时应避免NULL 的存在（用其他方式表达你想表达的NULL，比如 -1？） 如何查看索引信息，如何分析是否正确用到索引?show index from tablename;explain select ……; 关于explain，改天可以找个时间专门写一篇入门帖，在此之前，可以尝试 google 了解自己的系统，不要过早优化! 过早优化，一直是个非常讨厌而又时刻存在的问题，大多数时候就是因为不了解自己的系统，不知道自己系统真正的承载能力 比如：几千条数据的新闻表，每天几百几千次的正文搜索，大多数时候我们可以放心的去like，而不要又去建一套全文搜索什么的，毕竟cpu还是比人脑厉害太多 最后：永远别忘记的关键词 sql注入]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)深入理解javascript连续赋值表达式]]></title>
    <url>%2F2019%2F02%2F08%2F(%E8%BD%AC)%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3javascript%E8%BF%9E%E7%BB%AD%E8%B5%8B%E5%80%BC%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[(转)深入理解javascript连续赋值表达式引入 今天逛园子的时候看到一道javascript面试题，是关于连续赋值的，正好最近读jQuery源码经常看到这种连续赋值的表达式，所以很感兴趣。 废话不多说，来看题： var a = {n: 1}var b = a;a.x = a = {n: 2}console.log(a.x);console.log(b.x)答案： console.log(a.x); // undefinedconsole.log(b.x) //{n:2}看到这个答案，我真是百思不得解。。。。 于是网上搜了搜，整理如下： 以下转自：http://www.iteye.com/topic/785445 1、引用(Reference)与GetValue &amp; PutValue 引用A Reference is a reference to a property of an object. A Reference consists of two components, the base object and the property name. “引用”是引用某个对象的一个属性（可能这个对象并没有这个属性），一个引用含“根对象”与“属性名”两个成员。后面以“(根对象,属性名)”来表达一个引用 引用GetValue (V) If Type(V) is not Reference, return V. Call GetBase(V). If Result(2) is null, throw a ReferenceError exception. Call the [[Get]] method of Result(2), passing GetPropertyName(V) for the property name. Return Result(4). GetValue，即取值操作，返回的是确定的值，而不是引用。（可以理解为变量与变量的值，或指针与指针指向的对象） 引用PutValue (V, W) If Type(V) is not Reference, throw a ReferenceError exception. Call GetBase(V). If Result(2) is null, go to step 6. Call the [[Put]] method of Result(2), passing GetPropertyName(V) for the property name and W for the value. Return. Call the [[Put]] method for the global object, passing GetPropertyName(V) for the property name and W for thevalue. Return. PutValue操作只对引用生效，在ECMAScript的描述中，修改对象的属性都是通过Refrence + PutValue进行的（ECMAScript是为了便于表达而引入Reference这个类型，实际上JS语言中并无此类型。The internal Reference type is not a language data type. It is defined by this specification purely for expositorypurposes.） 2、成员表达式(MemberExpression)解释过程 引用The production MemberExpression : MemberExpression [ Expression ] is evaluated as follows: Evaluate MemberExpression.53 Call GetValue(Result(1)). Evaluate Expression. Call GetValue(Result(3)). Call ToObject(Result(2)). Call ToString(Result(4)). Return a value of type Reference whose base object is Result(5) and whose property name is Result(6). 着重看第7步：a value of type Reference 3、赋值表达式解析 引用The production AssignmentExpression : LeftHandSideExpression = AssignmentExpression is evaluated as follows: Evaluate LeftHandSideExpression. Evaluate AssignmentExpression. Call GetValue(Result(2)). Call PutValue(Result(1), Result(3)). Return Result(3). 这里可以看到左侧得出的是引用，右侧调用GetValue取得的是确定值。 那么开始分析a.b = a = {n:2}这个表达式，先假设{n:1}这个对象为OBJ1，{n:2}为OBJ2，全局为GLOBAL。 它的解析如下：a.b = Expression1Expression1为另一个赋值表达式：a = {} 首先计算a.b = Expression1，按(3)中赋值表达式运行步骤step1先得到引用(OBJ1, “b”)step2解析Expression1{ Expression1解析 step1得到引用(GLOBAL, “a”) step2得到一个对象OBJ2 step3取值，仍是OBJ2 step4将引用(GLOBAL, “a”)赋值为step3结果 step5返回OBJ2} step3取值，结果同样为OBJ2step4将(OBJ1, “b”)赋值为OBJ2step5返回OBJ2 最终结果：OBJ1: {n:1, b:OBJ2}OBJ2: {n:2}a : OBJ2 PS:我们常说赋值运算是从右至左，是指右边先结合所以a.b = a = {n:2}解析为了a.b = ( a = {n:2})，而不会解析为(a.b = a) = {n:2}如果理解为右边先运算就会有误解了，虽然右边先赋值成功。]]></content>
      <categories>
        <category>jquery</category>
      </categories>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入jQuery中的data()]]></title>
    <url>%2F2019%2F02%2F07%2F%E6%B7%B1%E5%85%A5jQuery%E4%B8%AD%E7%9A%84data()%2F</url>
    <content type="text"><![CDATA[深入jQuery中的data()引入 data函数在jQuery中看起来很不起眼, 就像沙滩上一颗平凡的沙子, 但仔细一瞅, 却惊讶的发现data是jQuery中无比重要的一环, 甚至jQuery中各种事件都基于此。 data有什么作用？ 在我们平时js编码过程中，我们经常会向DOM元素中添加各种自定义属性，这样有一个弊端。 1 假设我们在DOM元素中添加了一个属性，这个属性指向了某个js对象。 dom1.ele = jsObj 2 当这个js对象发挥完作用后，我们已经用不到他了。这时候按理说应该把这个js变量清空，释放内存。大家都知道，如果一个js对象不存在任何外在引用的话，解释器会自动将其在内存中删除，这也是javascript相对于c++等手动管理内存的程序的优点。 3 但是这时候问题来了，因为DOM元素引用了这个js对象，尽管这个js对象已经没有存在的意义了，但是解释器是不会把他删除的。如果想要把其删除，我们可能需要将DOM元素的这个属性设置为null。 4 我们编写了这么多的代码，哪里能把 每个js对象是不是被DOM元素引用了都记住啊？ 5 而且，假如DOM元素与js对象之间相互循环引用，根本就无法删除！ 这就是内存泄漏 6 所以，为了避免这种情况的发生，我们要尽量避免 引用数据(这里的引用数据可以说是javascript对象) 直接依附在DOM对象上。 7 data就是用来搞定以上问题的方法。 data是如何搞定以上问题的？首先来说一说jQuery中Data实现的大体思路： 1 首先我们创建一个数据缓存池，这个缓存池专门用来存储 向 DOM对象或者jQuery对象附加的额外数据。 2 当我们要向DOM对象或者jQuery对象附加额外数据的时候，我们附加的数据其实是保存于这个缓存池中 3 DOM对象或者jQuery对象生成一个额外属性，这个属性保存了 附加数据在缓存池中的‘门牌号’(位置或者索引) 4 当我们访问DOM对象或者jQuery对象的附加数据时，实际上是先取得其附加数据的门牌号，然后找到缓存池中对应门牌号的数据，进行操作。 大体思路讲完，那么来分析一下具体思路：在jQuery中，有一个Data构造函数，每当运行这个构造函数时，就会生成一个实例。jQuery默认会自动生成两个Data实例： var dataPriv = new Data() jQuery私有的，我们尽量不要对这个实例进行操作。 var dataUser = new Data() 这个就是服务于用户了，我们使用data()方法都是对这个实例进行操作。 所有的Data实例都有以下属性： expando： 值为字符串类型，每个Data实例的expando属性的值都不相同，用来区分不同的Data实例，类似于id的作用，expando的值就是上文中的额外属性。 uid: 这就是上文中的门牌号，初始为1，随着不同对象的附加数据的加入，自增长。 cache ： 一个对象 {} ，这就是缓存池了。 来个实例： $(document.body).data(‘aaa’, ‘value-aaa’)console.dir(document.body) body对象有一个名为jquer210023……的额外属性， 这个属性的名称就是dataUser的expando的值 这个属性的值就是门牌号。 总结： data实际上就是对js对象或者DOM对象的额外属性做了一个集中的管理。对于那些不会产生内存泄漏的额外数据，我们也可以直接向js对象或者DOM对象附加。 好，理清楚上面的关系后，我们再来看一下源码： Data构造函数源码解析 可能会有同学问道：如果我想对dataPriv进行操作该如何？ 请看源码： jQuery.extend({ hasData: function( elem ) { return dataUser.hasData( elem ) || dataPriv.hasData( elem ); }, data: function( elem, name, data ) { return dataUser.access( elem, name, data ); }, removeData: function( elem, name ) { dataUser.remove( elem, name ); }, // TODO: Now that all calls to _data and _removeData have been replaced // with direct calls to dataPriv methods, these can be deprecated. _data: function( elem, name, data ) { return dataPriv.access( elem, name, data ); }, _removeData: function( elem, name ) { dataPriv.remove( elem, name ); }}); 通过源码，我们可以看出： jQuery.data() jQuery.remove()都是对dataUser进行操作，而jQuery._data() jQuery._remove()都是对dataPriv进行操作。 理解jQuery.data(ele,name,data) 与 jQuery().data(key,value)的不同。 通过上面的源码，我们可以看到jQuery.data(ele,name,data)是对ele元素附加数据。 而jQuery().data(key,value)则会为jQuery对象中的所有DOM对象分别附加数据 来看源码(删减了部分)： jQuery.fn.extend({ data: function( key, value ) { var i, name, data, elem = this[ 0 ], attrs = elem &amp;&amp; elem.attributes;return access( this, function( value ) { var data, camelKey = jQuery.camelCase( key );// 从这里可以看出，为jQuery对象中的每个DOM元素分别附加数据 this.each(function() { // First, attempt to store a copy or reference of any // data that might’ve been store with a camelCased key. var data = dataUser.get( this, camelKey ); // For HTML5 data-* attribute interop, we have to // store property names with dashes in a camelCase form. // This might not apply to all properties...* dataUser.set( this, camelKey, value ); // *... In the case of properties that might _actually_ // have dashes, we need to also store a copy of that // unchanged property. if ( key.indexOf(&quot;-&quot;) !== -1 &amp;&amp; data !== undefined ) { dataUser.set( this, key, value ); } }); }, null, value, arguments.length &gt; 1, null, true ); }, removeData: function( key ) { return this.each(function() { dataUser.remove( this, key ); }); }});]]></content>
      <categories>
        <category>jquery</category>
      </categories>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Node.js module.exports与exports]]></title>
    <url>%2F2019%2F01%2F17%2F(%E8%BD%AC)Node.js%20module.exports%E4%B8%8Eexports%2F</url>
    <content type="text"><![CDATA[(转)Node.js module.exports与exports折腾Node.js有些日子了，下面将陆陆续续记录下使用Node.js的一些细节。 熟悉Node.js的童鞋都知道，Node.js作为服务器端的javascript运行环境，它使用npm作为通用的包管理工具，npm遵循CommonJS规范定义了一套用于Node.js模块的约定，关于npm实现Node.js模块的更多细节请细读深入Node.js的模块机制，这里简单讲下书写Node.js代码时module.exports与exorts的区别。 在浏览器端js里面，为了解决各模块变量冲突等问题，往往借助于js的闭包把所有模块相关的代码都包装在一个匿名函数里。而Node.js编写模块相当的自由，开发者只需要关注require，exports，module等几个变量就足够，而为了保持模块的可读性，很推荐把不同功能的代码块都写成独立模块，减少各模块耦合。开发者可以在“全局”环境下任意使用var申明变量（不用写到闭包里了），通过exports暴露接口给调用者。 我们经常看到类似export.xxx = yyy或者module.exports = xx这样的代码，可实际在通过require函数引入模块时会出现报错的情况，这是什么原因导致的呢？ Node.js在模块编译的过程中会对模块进行包装，最终会返回类似下面的代码： (function (exports, require, module, filename, __dirname) { // module code…});其中，module就是这个模块本身，require是对Node.js实现查找模块的模块Module._load实例的引用，filename和__dirname是Node.js在查找该模块后找到的模块名称和模块绝对路径，这就是官方API里头这两个全局变量的来历。 关于module.exports与exorts的区别，了解了下面几点之后应该就完全明白： 模块内部大概是这样： exports = module.exports = {};exports是module.exports的一个引用require引用模块后，返回给调用者的是module.exports而不是exportsexports.xxx，相当于在导出对象上挂属性，该属性对调用模块直接可见exports =相当于给exports对象重新赋值，调用模块不能访问exports对象及其属性如果此模块是一个类，就应该直接赋值module.exports，这样调用者就是一个类构造器，可以直接new实例客官如果看明白咋回事儿了下面的内容可以忽略：）假如有模块a.js代码如下： exports.str = ‘a’;exports.fn = function() {};对a模块的调用： var a = require(‘./a’);console.log(a.str);console.log(a.fn());这样用是对的，如果改造a如下： exports.str = ‘a’;exports = function fn() {};在调用a模块时自然没用fn属性了。 再改造下a模块： exports.str = ‘a’;module.exports = function fn() {};这时a模块其实就是fn函数的引用，也就是说可以require(‘./a’)()这样使用，而同时不再有str属性了。 下面直接导出一个类： module.exports = function A() {};调用： var A = require(‘./a’);var a = new A();总结下，有两点： 对于要导出的属性，可以简单直接挂到exports对象上对于类，为了直接使导出的内容作为类的构造器可以让调用者使用new操作符创建实例对象，应该把构造函数挂到module.exports对象上，不要和导出属性值混在一起最后，不用再纠结module.exports与exorts什么时候该用哪个了吧～]]></content>
      <categories>
        <category>jquery</category>
      </categories>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础之模块之sys模块]]></title>
    <url>%2F2018%2F12%2F27%2Fpython%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%A8%A1%E5%9D%97%E4%B9%8Bsys%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[python基础之模块之sys模块sys模块的功能 sys是python中较为常用的一个模块，他提供了对python脚本运行时的环境的操作。 sys功能： 1 sys.argv #将python脚本运行时的脚本名以及参数作为一个list，并输出。 test_py.py文件#/usr/bin/python3import sys print(‘the script name is:’,sys.argv[0])if len(sys.argv) &gt; 1: print(“there are”, len(sys.argv)-1, “arguments:”) # 使用len(sys.argv)-1采集参数个数-1为减去[0]脚本名称 for arg in sys.argv[1:]: #输出除了[0]外所有参数 print(arg)else: print(“there are no arguments!”) [root@slyoyo ~]# python3 test_py.py hahathe script name is: test_py.pythere are 1 arguments:haha 2 sys.path #返回一个list，该list为当前脚本的path环境变量（PYTHONPATH） sys.path[‘’, ‘/usr/local/python/lib/python34.zip’, ‘/usr/local/python/lib/python3.4’, ‘/usr/local/python/lib/python3.4/plat-linux’, ‘/usr/local/python/lib/python3.4/lib-dynload’, ‘/usr/local/python/lib/python3.4/site-packages’] 如果我们要增加或者删除pythonpath的话，使用list的insert或者pop功能即可。 import os,sysprint(sys.path)new_path=os.path.dirname(os.path.dirname(os.path.abspath(file)))print(new_path)sys.path.insert(0,new_path)print(sys.path) #结果：[‘C:\Users\Capital-D\PycharmProjects\untitled\test\atm\fun’, ‘C:\Users\Capital-D\PycharmProjects\untitled’, ‘C:\windows\SYSTEM32\python34.zip’, ‘C:\Python34\DLLs’, ‘C:\Python34\lib’, ‘C:\Python34’, ‘C:\Python34\lib\site-packages’]C:\Users\Capital-D\PycharmProjects\untitled\test\atm[‘C:\Users\Capital-D\PycharmProjects\untitled\test\atm’, ‘C:\Users\Capital-D\PycharmProjects\untitled\test\atm\fun’, ‘C:\Users\Capital-D\PycharmProjects\untitled’, ‘C:\windows\SYSTEM32\python34.zip’, ‘C:\Python34\DLLs’, ‘C:\Python34\lib’, ‘C:\Python34’, ‘C:\Python34\lib\site-packages’] 3 sys.platform #返回当前平台 import sys,osprint(sys.platform) #结果：win324 sys.stdout sys.stdin #标准输出/输入 print ‘Dive in’ # 标准输出saveout = sys.stdout # 终在重定向前保存stdout，这样的话之后你还可以将其设回正常fsock = open(‘out.log’, ‘w’) # 打开一个新文件用于写入。如果文件不存在，将会被创建。如果文件存在，将被覆盖。sys.stdout = fsock # 所有后续的输出都会被重定向到刚才打开的新文件上。 print ‘This message will be logged instead of displayed’ # 这样只会将输出结果“打印”到日志文件中；屏幕上不会看到输出 sys.stdout = saveout # 在我们将 stdout 搞乱之前，让我们把它设回原来的方式。 fsock.close() # 关闭日志文件。 5 sys.stderr #错误输出 使用方法同上]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django中models field详解]]></title>
    <url>%2F2018%2F12%2F27%2Fdjango%E4%B8%ADmodels%20field%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[django中models field详解一 field options(所有字段共用) 1 null 默认为False，True则表示可以为null。（空字符串在数据库中可能被存储为’’） 2 blank 默认为False，True表示可以为空。 3 choice 可选的，限制了该选项的字段值必须是所指定的choice中的一个。 from django.db import models class Student(models.Model): FRESHMAN = ‘FR’ SOPHOMORE = ‘SO’ JUNIOR = ‘JR’ SENIOR = ‘SR’ YEAR_IN_SCHOOL_CHOICES = ( (FRESHMAN, ‘Freshman’), #第一个参数是真正的model参数，#第二个参数则是方便人们理解阅读 (SOPHOMORE, ‘Sophomore’), (JUNIOR, ‘Junior’), (SENIOR, ‘Senior’), ) year_in_school = models.CharField( max_length=2, choices=YEAR_IN_SCHOOL_CHOICES, default=FRESHMAN, ) def is_upperclass(self): return self.year_in_school in (self.JUNIOR, self.SENIOR) 4 db_column 数据库column名称。默认为本字段的名称。 5 db_index 如果为True的话，该字段的数据库索引将会被创建 6 default 设置该字段的默认值，可以是值也可以是对象。 7 editable 默认为True，若为False，则不会在admin/界面显示 8 primary_key 若设置为True，则表示将该字段设置为主键。一般情况下django默认会设置一个自增长的id主键。 9 unique 若设置为True，该字段值不可重复 二 field type(字段类型,细分的话可以分为普通字段以及关系字段) 1 普通字段 1 AutoField() 根据已有id自增长的整形唯一字段，一般每个model类不需设置该字段，因为django会为每个model自动设置。 django默认会为每个model类添加如下语句：id = models.AutoField(primary_key=True) 当其他字段添加了primary_key属性，则不会创建id字段了 每个model类仅能有一个主键 2 BooleanField() 布尔型字段，默认的表单窗口部件是CheckBoxInput 3 CharField() 字符型字段，默认的表单窗口部件是TextInput。该字段类型有一个必需参数：max_length 在数据库水平限定了字符串最大长度 4 DateField() 日期字段，字段的值是python中datetime.date的实例，默认的表单窗口是TextInput有几个可选的参数： auto_now=True/False:当设置为True时，每当该对象使用save()时，该字段的值就会被更新。 auto_now_add=True/False: 当设置为True时，该字段的值为该对象被创建时的日期 5 DateTimeField() 日期和时间字段，值为datetime.datetime实例。默认的表单窗口以及可选参数同上。 6 DecimalField() 混合精度的小数型数字字段。有两个必需的参数： max_digits=ingt_number:限定数字的最大位数（包含小数位） decimal_places=int_number:存储数字的小数位 #to store numbers up to 999 with a resolution of 2 decimal places, you’d usemodels.DecimalField(…, max_digits=5, decimal_places=2) 7 EmailField(max_length=254, **options) 邮件字段，使用EmailValidator进行验证 8 FileField(upload_to=None, max_length=100, **options) 文件上传字段。 这个字段不能设置primary_key和unique选项.在数据库中存储类型是varchar，默认最大长度为100. 有两个可选参数： upload_to 如果使用默认的FileSystomStorage，文件将会存储到settings文件中配置的MEDIA_ROOT路径中。 upload_to的值也可以为可调用对象,通过调用这个对象可以获得上传路径。 instance=: 定义了FileField的模型实例 filename=’’: 文件名称。 class MyModel01(models.Model): # file will be uploaded to MEDIA_ROOT/uploads upload = models.FileField(upload_to=&apos;uploads/&apos;) # or... # file will be saved to MEDIA_ROOT/uploads/2015/01/30 upload = models.FileField(upload_to=&apos;uploads/%Y/%m/%d/&apos;)#upload_to=可调用对象def user_directory_path(instance, filename): # file will be uploaded to MEDIA_ROOT/user_&lt;id&gt;/&lt;filename&gt; return &apos;user_{0}/{1}&apos;.format(instance.user.id, filename)class MyModel02(models.Model): upload = models.FileField(upload_to=user_directory_path) storage 用来设定文件存储仓库 9 FilePathField(path=None, match=None, recursive=False, max_length=100, **options) 这个字段的值被限制在系统上某个目录中的所有文件名集合中。有三个参数 path=’’: 该参数必需。上行所说的‘某个目录’的绝对路径。Example: “/home/images”. match=’pattern’: 可选参数。格式是正则表达式。用来拣选符合匹配正则表达式的文件 recursive=True/False: 可选参数，默认为False。设定是否递归该目录下所有子目录的所有文件。 FilePathField(path=”/home/images”, match=”foo.*”, recursive=True) 10 FloatField() 浮点字段，默认的表单窗口部件是NumberInput。和DecimalField经常混淆不清， FloatField在内部使用Python中的float对象，而DecimalField在内部使用Python中的decimal对象。 11 ImageField(upload_to=None, height_field=None, width_field=None, max_length=100, **options) 图像字段。继承了FileField的所有属性和方法。而且还能自动验证上传的对象是否为合法的图像。 12 IntegerField 整形字段。 13 GenericIPAddressField(protocol=’both’, unpack_ipv4=False, **options) ip地址字段 protocol=’both/ipv4/ipv6’ 默认为both unpack_ipv4 用处不大。 14 NullBooleanField 类似于BooleanField，不同的是其允许值为null 15 TextField() 与CharField类似，但一般用来存储体积较大的文本。 16 TimeField(auto_now=False, auto_now_add=False, **options) 时间字段，其值为datetime.time实例 17 URLField(max_length=200, **options) URL字段 类似于CharField的子类，默认最大长度为200. 18 UUIDField(**options) 通用唯一标识字段，当不想用django默认设置的AutoField字段时，可以用该字段代替。 2 关系字段 关系字段：一对一，多对一，多对多 一对一： 现在有很多一对一辅导班，也就是上课时，一个老师对应一个学生，一个学生对应一个老师 多对一： 很多偏远山区的学校可能整个学校只有一个老师，这一个老师对应多个学生，所有的学生对应这一个老师 多对多： 而我们则很幸福，学校里有许多老师，一个老师教习一科，学生有多个老师，老师有多个学生。 1 ForeignKey(othermodel, on_delete, **options) 多对一或者一对多或者外键字段。 othermodel： 所关联的模型，’多’ model使用外键关联 ‘一’model。 当所关联的模型为他自己时，使用’self’ 当引用的模型为其他app中的模型时，要加上app名称标签: ‘app_name.model_name’ 数据库会自动在外键字段上创建索引，可以使用de_index=False关闭该功能。 on_delete： 当删除 “一” 模型对象时，django会根据该参数的值对与该对象相关联的其他对象(也就是 ‘多’)进行操作。 在django1.9以及之前的版本中，on_delete作为一个关键字参数。而在1.10则可以作为第二个参数 models.CASCADE： 默认为models.CASCADE 级联删除。当删除’一’时，‘多’会被删除。比如： mysite项目下名为polls的app中的models.pyclass follower(models.Model): name = models.CharField(max_length=200) menpai = models.ForeignKey(‘menpai’, on_delete=models.CASCADE) #定义了models.CASCADE属性 def __str__(self): return self.nameclass menpai(models.Model): name = models.CharField(max_length=200) def __str__(self): return self.name#运行 python3 manager.py shell进入交互页面 from polls.models import follower,menpaim1=menpai(name=’huashanpai’)m1.save()m2=menpai(name=’riyuejiao’)m2.save()f1=follower(name=’linghuchong’,menpai=m1)f1.save()f2=follower(name=’renwoxing’,menpai=m2)f2.save()f1.menpai&lt;menpai: huashanpai&gt;m1.delete()(2, {‘polls.menpai’: 1, ‘polls.follower’: 1}) # 删除华山派时，将令狐冲也删除了 modles.PROTECT : 当删除一个具有外键关系的对象时，会引发一个异常，阻止删除该对象 models.SET_NULL: 设置删除对象所关联的外键字段为null。但字段的null属性必需为True models.SET_DEFAULT : 设置删除对象所关联的外键字段为默认的值。 models.SET(value) ：设置删除对象所关联的对象的外键字段为value,value也可以是一个可调用函数。 from django.conf import settingsfrom django.contrib.auth import get_user_modelfrom django.db import models def get_sentinel_user(): return get_user_model().objects.get_or_create(username=’deleted’)[0] class MyModel(models.Model): user = models.ForeignKey( settings.AUTH_USER_MODEL, on_delete=models.SET(get_sentinel_user), ) models.DO_NOTHING : 不做任何操作 limit_choices_to 限制该字段为选项形。格式：limit_choices_to={‘is_staff’: True}。值也可以为可调用函数。 def limit_pub_date_choices(): return {‘pub_date__lte’: datetime.date.utcnow()} limit_choices_to = limit_pub_date_choices related_name 设置从关联对象到自身的关系的名称，若值为’+’ 则关联对象与自身无逆向关系。详解请看官方文档。 to_field 设置所关联对象的关联字段。默认为关联对象的主键字段。 2 ManyToManyField(othermodel, **options) 多对多字段。 othermodel: 所关联的model名称 db_table: 多对多关系会在两个模型所对应的表中间创建一个‘中间表’ ，将多对多转换为两个多对一，该选项为这个中间表设置名称。一般来说django会默认为中间表创建名称，但人们读起来可能搞不清楚究竟中间表关联到了哪里。 related_name: 同多对一字段中的related_name limite_choices_to: 同…. symmetrical: 当多对多关联对象为自身时可能会用到的参数。默认为True。a,b同属于person模型，person中的friends字段与自身设置了多对多关系，当该值设置为True时，django假定关系为对称，即：a是b的朋友，那么b也是a的朋友。设置为False时，django会强制为逆向关系创建描述信息。 though: 不想让django自动创建中间表，手动创建中间表所对应的model，通过though指定对应的model名称。 though_field: 当though参数被使用时，该参数才会有效。指定使用哪些中间模型字段来确立两个模型的多对多关系。 3 OneToOneField(othermodel, on_delete, parent_link=False, **options) 一对一字段。 othermodel: ……. on_delete:…….. related_name:………..]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django中的request对象详解]]></title>
    <url>%2F2018%2F12%2F08%2Fdjango%E4%B8%AD%E7%9A%84request%E5%AF%B9%E8%B1%A1%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[django中的request对象详解Request 我们知道当URLconf文件匹配到用户输入的路径后，会调用对应的view函数，并将 HttpRequest对象 作为第一个参数传入该函数。 我们来看一看这个HttpRequest对象有哪些属性或者方法： 属性： 1 HttpRequest.scheme 请求的协议，一般为http或者https，字符串格式(以下属性中若无特殊指明，均为字符串格式) 2 HttpRequest.body http请求的主体，二进制格式。 3 HttpRequest.path 所请求页面的完整路径(但不包括协议以及域名)，也就是相对于网站根目录的路径。 4 HttpRequest.path_info 获取具有 URL 扩展名的资源的附加路径信息。相对于HttpRequest.path，使用该方法便于移植。 if the WSGIScriptAlias for your application is set to “/minfo”, then path might be “/minfo/music/bands/the_beatles/“ and path_info would be “/music/bands/the_beatles/“.5 HttpRequest.method 获取该请求的方法，比如： GET POST ……… 6 HttpRequest.encoding 获取请求中表单提交数据的编码。 7 HttpRequest.content_type 获取请求的MIME类型(从CONTENT_TYPE头部中获取)，django1.10的新特性。 8 HttpRequest.content_params 获取CONTENT_TYPE中的键值对参数，并以字典的方式表示，django1.10的新特性。 9 HttpRequest.GET 返回一个 querydict 对象(类似于字典，本文最后有querydict的介绍)，该对象包含了所有的HTTP GET参数 10 HttpRequest.POST 返回一个 querydict ，该对象包含了所有的HTTP POST参数，通过表单上传的所有 字符 都会保存在该属性中。 11 HttpRequest.COOKIES 返回一个包含了所有cookies的字典。 12 HttpRequest.FILES 返回一个包含了所有的上传文件的 querydict 对象。通过表单所上传的所有 文件 都会保存在该属性中。 key的值是input标签中name属性的值，value的值是一个UploadedFile对象 13 HttpRequest.META 返回一个包含了所有http头部信息的字典 View Code14 HttpRequest.session 中间件属性 15 HttpRequest.site 中间件属性 16 HttpRequest.user 中间件属性，表示当前登录的用户。 HttpRequest.user实际上是由一个定义在django.contrib.auth.models 中的 user model 类 所创建的对象。 该类有许多字段，属性和方法。列举几个常用的： 获取更详细信息–&gt;官方文档。 1 字段： username 用户名 first_name last_name email password groups user_permissions, is_staff 布尔值，标明用户是否可以访问admin页面 is_superuser last_login 上一次登陆时间 date_joined 用户创建时间 2 属性 is_authenticated 布尔值，标志着用户是否已认证。在django1.10之前，没有该属性，但有与该属性同名的方法。 3 方法 1 HttpRequest.user.get_username() 注意：方法的圆括号在templates标签中必需省略！！ 获取username。尽量使用该方法来代替使用username字段 2 HttpRequest.user.get_full_name() 注意：方法的圆括号在templates标签中必需省略！！ 获取first_name和last_name 3 HttpRequest.user.short_name() 注意：方法的圆括号在templates标签中必需省略！！ 获取first_name 4 HttpRequest.user.set_password(raw_password) 注意：该方法无法在template标签中使用！！ 设置密码 5 HttpRequest.user.check_password(raw_password) 注意：该方法无法在template标签中使用！！ 如果raw_password与用户密码相等，则返回True 方法： 1 HttpRequest.get_host() 返回请求的源主机。example: 127.0.0.1:8000 2 HttpRequest.get_port() django1.9的新特性。 3 HttpRequest.get_full_path() 返回完整路径，并包括附加的查询信息。example: “/music/bands/the_beatles/?print=true” 4 HttpRequest.bulid_absolute_uri(location) 返回location的绝对uri，location默认为request.get_full_path()。 Example: “https://example.com/music/bands/the_beatles/?print=true&quot; QueryDict 是一个类似于Python中字典的一种对象，他是Python中字典的子类，所以继承了字典的所有方法， 当然QueryDict对字典的某些方法进行了加工，并补充了一些独特的方法。这里列出部分方法。详情请看： 官方文档 。 1 QueryDict.get(key,default=None) 返回key所对应的value，若key不存在，则返回default的值 2 QueryDict.update(other_dict) 更新 3 QueryDict.values() 列出所有的值 4 QueryDict.items() 列出所有的键值对,若一个key有多个值，只显示最后一个值。 5 QueryDict.pop(key) 删除某个键值对 6 QueryDict.getlist(key) 根据输入的key返回一个Python中的list 7 QueryDict.dict() 返回QueryDict的字典的表现形式]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django中的认证与登录]]></title>
    <url>%2F2018%2F11%2F20%2Fdjango%E4%B8%AD%E7%9A%84%E8%AE%A4%E8%AF%81%E4%B8%8E%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[django中的认证与登录认证登录 django.contrib.auth中提供了许多方法，这里主要介绍其中的三个： 1 authenticate(**credentials) 提供了用户认证，即验证用户名以及密码是否正确 一般需要username password两个关键字参数 如果认证信息有效，会返回一个 User 对象。authenticate()会在User 对象上设置一个属性标识那种认证后端认证了该用户，且该信息在后面的登录过程中是需要的。当我们试图登陆一个从数据库中直接取出来不经过authenticate()的User对象会报错的！！ user = authentica(username=’someone’,password=’somepassword’) 2 login(HttpRequest, user, backend=None) 该函数接受一个HttpRequest对象，以及一个认证了的User对象 此函数使用django的session框架给某个已认证的用户附加上session id等信息。 from django.contrib.auth import authenticate, login def my_view(request): username = request.POST[‘username’] password = request.POST[‘password’] user = authenticate(username=username, password=password) if user is not None: login(request, user) # Redirect to a success page. ... else: # Return an &apos;invalid login&apos; error message. ... 3 logout(request) 注销用户 from django.contrib.auth import logout def logout_view(request): logout(request) # Redirect to a success page. 该函数接受一个HttpRequest对象，无返回值。 当调用该函数时，当前请求的session信息会全部清除 该用户即使没有登录，使用该函数也不会报错 只允许登录的用户访问 如果希望： 1 用户登陆后才能访问某些页面， 2 如果用户没有登录就访问该页面的话直接跳到登录页面 3 用户在跳转的登陆界面中完成登陆后，自动访问跳转到之前访问的地址 我们有几个方法来实现： 1 糙活 检测request.user.is_authenticated() from django.conf import settingsfrom django.shortcuts import redirect def my_view(request): if not request.user.is_authenticated(): return redirect(‘%s?next=%s’ % (settings.LOGIN_URL, request.path)) 2 django中的login_required函数 django已经为我们设计好了一个用于此种情况的装饰器：login_requierd() from django.contrib.auth.decorators import login_required @login_requireddef my_view(request): … 在运行my_view函数前需要进行登录验证。 1 若用户没有登录，则会跳转到django默认的 登录URL ‘/accounts/login/ ‘ (这个值可以在settings文件中通过LOGIN_URL进行修改)。并传递 当前访问url的绝对路径 (登陆成功后，会重定向到该路径)。 可以使用 login_url 参数来配置 登录url 。 可以使用redirect_field_name参数配置 当前访问url的绝对路径 。 如果要使用django的默认登陆界面，则可以通过在urls.py中如此配置，这样的话，如果未登录，程序会默认跳转到 “templates\registration\login.html”这个模板。 #urls.py…(r’^accounts/login/$’, ‘django.contrib.auth.views.login’), 2 如果用户登陆了，则会进入正常的页面]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)GIL 与 Python 线程的纠葛]]></title>
    <url>%2F2018%2F11%2F13%2F(%E8%BD%AC)GIL%20%E4%B8%8E%20Python%20%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%BA%A0%E8%91%9B%2F</url>
    <content type="text"><![CDATA[(转)GIL 与 Python 线程的纠葛原文地址：http://zhuoqiang.me/python-thread-gil-and-ctypes.html 作者：Qiang GIL 与 Python 线程的纠葛GIL 是什么？它对 python 程序会产生怎样的影响？我们先来看一个问题。运行下面这段 python 代码，CPU 占用率是多少？ 请勿在工作中模仿，危险:)def dead_loop(): while True: pass dead_loop()答案是什么呢，占用 100％ CPU？那是单核！还得是没有超线程的古董 CPU。在我的双核 CPU 上，这个死循环只会吃掉我一个核的工作负荷，也就是只占用 50％ CPU。那如何能让它在双核机器上占用 100％ 的 CPU 呢？答案很容易想到，用两个线程就行了，线程不正是并发分享 CPU 运算资源的吗。可惜答案虽然对了，但做起来可没那么简单。下面的程序在主线程之外又起了一个死循环的线程 import threading def dead_loop(): while True: pass 新起一个死循环线程t = threading.Thread(target=dead_loop)t.start() 主线程也进入死循环dead_loop() t.join()按道理它应该能做到占用两个核的 CPU 资源，可是实际运行情况却是没有什么改变，还是只占了 50％ CPU 不到。这又是为什么呢？难道 python 线程不是操作系统的原生线程？打开 system monitor 一探究竟，这个占了 50% 的 python 进程确实是有两个线程在跑。那这两个死循环的线程为何不能占满双核 CPU 资源呢？其实幕后的黑手就是 GIL。 GIL 的迷思：痛并快乐着GIL 的全程为 Global Interpreter Lock ，意即全局解释器锁。在 Python 语言的主流实现 CPython 中，GIL 是一个货真价实的全局线程锁，在解释器解释执行任何 Python 代码时，都需要先获得这把锁才行，在遇到 I/O 操作时会释放这把锁。如果是纯计算的程序，没有 I/O 操作，解释器会每隔 100 次操作就释放这把锁，让别的线程有机会执行（这个次数可以通过 sys.setcheckinterval 来调整）。所以虽然 CPython 的线程库直接封装操作系统的原生线程，但 CPython 进程做为一个整体，同一时间只会有一个获得了 GIL 的线程在跑，其它的线程都处于等待状态等着 GIL 的释放。这也就解释了我们上面的实验结果：虽然有两个死循环的线程，而且有两个物理 CPU 内核，但因为 GIL 的限制，两个线程只是做着分时切换，总的 CPU 占用率还略低于 50％。 看起来 python 很不给力啊。GIL 直接导致 CPython 不能利用物理多核的性能加速运算。那为什么会有这样的设计呢？我猜想应该还是历史遗留问题。多核 CPU 在 1990 年代还属于类科幻，Guido van Rossum 在创造 python 的时候，也想不到他的语言有一天会被用到很可能 1000＋ 个核的 CPU 上面，一个全局锁搞定多线程安全在那个时代应该是最简单经济的设计了。简单而又能满足需求，那就是合适的设计（对设计来说，应该只有合适与否，而没有好与不好）。怪只怪硬件的发展实在太快了，摩尔定律给软件业的红利这么快就要到头了。短短 20 年不到，代码工人就不能指望仅仅靠升级 CPU 就能让老软件跑的更快了。在多核时代，编程的免费午餐没有了。如果程序不能用并发挤干每个核的运算性能，那就意谓着会被淘汰。对软件如此，对语言也是一样。那 Python 的对策呢？ Python 的应对很简单，以不变应万变。在最新的 python 3 中依然有 GIL。之所以不去掉，原因嘛，不外以下几点： 欲练神功，挥刀自宫： CPython 的 GIL 本意是用来保护所有全局的解释器和环境状态变量的。如果去掉 GIL，就需要多个更细粒度的锁对解释器的众多全局状态进行保护。或者采用 Lock-Free 算法。无论哪一种，要做到多线程安全都会比单使用 GIL 一个锁要难的多。而且改动的对象还是有 20 年历史的 CPython 代码树，更不论有这么多第三方的扩展也在依赖 GIL。对 Python 社区来说，这不异于挥刀自宫，重新来过。 就算自宫，也未必成功： 有位牛人曾经做了一个验证用的 CPython，将 GIL 去掉，加入了更多的细粒度锁。但是经过实际的测试，对单线程程序来说，这个版本有很大的性能下降，只有在利用的物理 CPU 超过一定数目后，才会比 GIL 版本的性能好。这也难怪。单线程本来就不需要什么锁。单就锁管理本身来说，锁 GIL 这个粗粒度的锁肯定比管理众多细粒度的锁要快的多。而现在绝大部分的 python 程序都是单线程的。再者，从需求来说，使用 python 绝不是因为看中它的运算性能。就算能利用多核，它的性能也不可能和 C/C++ 比肩。费了大力气把 GIL 拿掉，反而让大部分的程序都变慢了，这不是南辕北辙吗。 难道 Python 这么优秀的语言真的仅仅因为改动困难和意义不大就放弃多核时代了吗？其实，不做改动最最重要的原因还在于：不用自宫，也一样能成功！ 其它神功那除了切掉 GIL 外，果然还有方法让 Python 在多核时代活的滋润？让我们回到本文最初的那个问题：如何能让这个死循环的 Python 脚本在双核机器上占用 100％ 的 CPU？其实最简单的答案应该是：运行两个 python 死循环的程序！也就是说，用两个分别占满一个 CPU 内核的 python 进程来做到。确实，多进程也是利用多个 CPU 的好方法。只是进程间内存地址空间独立，互相协同通信要比多线程麻烦很多。有感于此，Python 在 2.6 里新引入了 multiprocessing 这个多进程标准库，让多进程的 python 程序编写简化到类似多线程的程度，大大减轻了 GIL 带来的不能利用多核的尴尬。 这还只是一个方法，如果不想用多进程这样重量级的解决方案，还有个更彻底的方案，放弃 Python，改用 C/C++。当然，你也不用做的这么绝，只需要把关键部分用 C/C++ 写成 Python 扩展，其它部分还是用 Python 来写，让 Python 的归 Python，C 的归 C。一般计算密集性的程序都会用 C 代码编写并通过扩展的方式集成到 Python 脚本里（如 NumPy 模块）。在扩展里就完全可以用 C 创建原生线程，而且不用锁 GIL，充分利用 CPU 的计算资源了。不过，写 Python 扩展总是让人觉得很复杂。好在 Python 还有另一种与 C 模块进行互通的机制 : ctypes 利用 ctypes 绕过 GILctypes 与 Python 扩展不同，它可以让 Python 直接调用任意的 C 动态库的导出函数。你所要做的只是用 ctypes 写些 python 代码即可。最酷的是，ctypes 会在调用 C 函数前释放 GIL。所以，我们可以通过 ctypes 和 C 动态库来让 python 充分利用物理内核的计算能力。让我们来实际验证一下，这次我们用 C 写一个死循环函数 extern”C”{ void DeadLoop() { while (true); }} 用上面的 C 代码编译生成动态库 libdead_loop.so （Windows 上是 dead_loop.dll） ，接着就要利用 ctypes 来在 python 里 load 这个动态库，分别在主线程和新建线程里调用其中的 DeadLoop from ctypes import * from threading import Thread lib = cdll.LoadLibrary(“libdead_loop.so”)t = Thread(target=lib.DeadLoop)t.start() lib.DeadLoop()这回再看看 system monitor，Python 解释器进程有两个线程在跑，而且双核 CPU 全被占满了，ctypes 确实很给力！需要提醒的是，GIL 是被 ctypes 在调用 C 函数前释放的。但是 Python 解释器还是会在执行任意一段 Python 代码时锁 GIL 的。如果你使用 Python 的代码做为 C 函数的 callback，那么只要 Python 的 callback 方法被执行时，GIL 还是会跳出来的。比如下面的例子： extern”C”{ typedef void Callback(); void Call(Callback* callback) { callback(); }} from ctypes import * from threading import Thread def dead_loop(): while True: pass lib = cdll.LoadLibrary(“libcall.so”)Callback = CFUNCTYPE(None)callback = Callback(dead_loop) t = Thread(target=lib.Call, args=(callback,))t.start() lib.Call(callback)注意这里与上个例子的不同之处，这次的死循环是发生在 Python 代码里 (DeadLoop 函数) 而 C 代码只是负责去调用这个 callback 而已。运行这个例子，你会发现 CPU 占用率还是只有 50％ 不到。GIL 又起作用了。 其实，从上面的例子，我们还能看出 ctypes 的一个应用，那就是用 Python 写自动化测试用例，通过 ctypes 直接调用 C 模块的接口来对这个模块进行黑盒测试，哪怕是有关该模块 C 接口的多线程安全方面的测试，ctypes 也一样能做到。 结语虽然 CPython 的线程库封装了操作系统的原生线程，但却因为 GIL 的存在导致多线程不能利用多个 CPU 内核的计算能力。好在现在 Python 有了易筋经（multiprocessing）, 吸星大法（C 语言扩展机制）和独孤九剑（ctypes），足以应付多核时代的挑战，GIL 切还是不切已经不重要了，不是吗。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个案例深入Python中的__new__和__init__]]></title>
    <url>%2F2018%2F10%2F27%2F%E4%B8%80%E4%B8%AA%E6%A1%88%E4%BE%8B%E6%B7%B1%E5%85%A5Python%E4%B8%AD%E7%9A%84__new__%E5%92%8C__init__%2F</url>
    <content type="text"><![CDATA[一个案例深入Python中的new和init准备在Python中，一切皆对象。 既然一切皆对象，那么类也是对象，我们暂且称之为 类对象。来个简单例子(本篇文章的所有案例都是运行在Python3.4中)： class foo(): pass print(id(foo))print(type(foo)) 结果：46627056&lt;class ‘type’&gt;如果想深入了解一下，可以看：深刻理解Python中的元类(metaclass) 引入最近在阅读tornado源码，发现在其源码中有很多类是这样的： class HTTPServer(TCPServer, Configurable, httputil.HTTPServerConnectionDelegate): def __init__(self, *args, **kwargs): # Ignore args to __init__; real initialization belongs in # initialize since we&apos;re Configurable. 就是说默认的__init__初始化方法不在起作用了，改为了initialize方法进行初始化 pass或者是干脆没有init ，只写了个initialize方法来替代。 所以心生疑惑，tornado是如何做到这一点的？ 正题接下来我们来了解一下，Python解释器是如何创建对象的。 大家可能对Python中的init方法很熟悉，认为他是实例化类时调用的第一个方法。但其实他并不是。实例化时调用的第一个方法其实是new方法。 好了，接下来是重点： 1 当我们实例化A类对象时，Python中首先调用的是该A类对象的new方法，如果该A类对象没有定义new方法，则去父类中依次查找，直到object类 2 object类有一个new方法，该方法接收一个参数(一般为类对象)，将该参数进行实例化并返回一个对象 3 Python解释器会将调用new方法并将A类对象作为第一个参数传入，最后会返回一个对象(这个对象就是A类的实例对象，我们称之为a1) 4 Python解释器默认会调用a1对象的init方法，并将参数传入。 来一个例子验证一下： class asd(object): def new(cls, args, *kwargs): print(‘asd.new() is running. cls id is %s’%id(cls)) r = super(asd,cls).new(cls) print(‘r_id is %s’%id(r)) return r class bnm(asd): def __init__(self,name): print(&apos;bnm.__init__() is running, self id is %s&apos;%id(self)) self.name = name print(&apos;bnm.name is %s&apos;%(self.name))print(‘asd_id is %s’%id(asd))print(‘bnm_id is %s’%id(bnm))o1 = bnm(‘ni’)print(‘o1_id is’,id(o1)) asd_id is 49838320bnm_id is 49838768asd.new() is running. cls id is 49838768r_id is 49848400bnm.init() is running, self id is 49848400bnm.name is nio1_id is 49848400注意 ： bnm 和 cls 是同一个对象！ r 和 o1 也是同一个对象 ！ 应用仿tornado实现自定义类的初始化方法： class asd(object): def new(cls, args, **kwargs): r = super(asd,cls).new(cls) r.initialize(args) return r class bnm(asd): def initialize(self): print(&apos;bnm_initialize is running&apos;)class foo(asd): def initialize(self,name): self.name = name print(&apos;foo_initialize is running, my name is %s&apos; %(self.name))r = bnm()r1 = foo(‘linghuchong’) bnm_initialize is runningfoo_initialize is running, my name is linghuchong定义类时，只要继承了asd类，就会将initialize方法作为初始化方法，是不是感觉很(wu)酷(lun)炫(yong)？]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的select模块]]></title>
    <url>%2F2018%2F10%2F16%2Fpython%E4%B8%AD%E7%9A%84select%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[python中的select模块介绍：Python中的select模块专注于I/O多路复用，提供了select poll epoll三个方法(其中后两个在Linux中可用，windows仅支持select)，另外也提供了kqueue方法(freeBSD系统) select方法：进程指定内核监听哪些文件描述符(最多监听1024个fd)的哪些事件，当没有文件描述符事件发生时，进程被阻塞；当一个或者多个文件描述符事件发生时，进程被唤醒。 当我们调用select()时： 1 上下文切换转换为内核态 2 将fd从用户空间复制到内核空间 3 内核遍历所有fd，查看其对应事件是否发生 4 如果没发生，将进程阻塞，当设备驱动产生中断或者timeout时间后，将进程唤醒，再次进行遍历 5 返回遍历后的fd 6 将fd从内核空间复制到用户空间 fd:file descriptor 文件描述符 fd_r_list, fd_w_list, fd_e_list = select.select(rlist, wlist, xlist, [timeout]) 参数： 可接受四个参数（前三个必须）rlist: wait until ready for readingwlist: wait until ready for writingxlist: wait for an “exceptional condition”timeout: 超时时间 返回值：三个列表 select方法用来监视文件描述符(当文件描述符条件不满足时，select会阻塞)，当某个文件描述符状态改变后，会返回三个列表1、当参数1 序列中的fd满足“可读”条件时，则获取发生变化的fd并添加到fd_r_list中2、当参数2 序列中含有fd时，则将该序列中所有的fd添加到 fd_w_list中3、当参数3 序列中的fd发生错误时，则将该发生错误的fd添加到 fd_e_list中4、当超时时间为空，则select会一直阻塞，直到监听的句柄发生变化 当超时时间 ＝ n(正整数)时，那么如果监听的句柄均无任何变化，则select会阻塞n秒，之后返回三个空列表，如果监听的句柄有变化，则直接执行。 实例：利用select实现一个可并发的服务端 import socketimport select s = socket.socket()s.bind((‘127.0.0.1’,8888))s.listen(5)r_list = [s,]num = 0while True: rl, wl, error = select.select(r_list,[],[],10) num+=1 print(‘counts is %s’%num) print(“rl’s length is %s”%len(rl)) for fd in rl: if fd == s: conn, addr = fd.accept() r_list.append(conn) msg = conn.recv(200) conn.sendall((‘first—-%s’%conn.fileno()).encode()) else: try: msg = fd.recv(200) fd.sendall(‘second’.encode()) except ConnectionAbortedError: r_list.remove(fd) s.close() import socket flag = 1s = socket.socket()s.connect((‘127.0.0.1’,8888))while flag: input_msg = input(‘input&gt;&gt;&gt;’) if input_msg == ‘0’: break s.sendall(input_msg.encode()) msg = s.recv(1024) print(msg.decode()) s.close() 在服务端我们可以看到，我们需要不停的调用select， 这就意味着： 1 当文件描述符过多时，文件描述符在用户空间与内核空间进行copy会很费时 2 当文件描述符过多时，内核对文件描述符的遍历也很浪费时间 3 select最大仅仅支持1024个文件描述符 poll与select相差不大，本文不作介绍 epoll方法：epoll很好的改进了select： 1 epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时，会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。 2 epoll会在epoll_ctl时把指定的fd遍历一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd 3 epoll对文件描述符没有额外限制 select.epoll(sizehint=-1, flags=0) 创建epoll对象 epoll.close()Close the control file descriptor of the epoll object.关闭epoll对象的文件描述符 epoll.closedTrue if the epoll object is closed.检测epoll对象是否关闭 epoll.fileno()Return the file descriptor number of the control fd.返回epoll对象的文件描述符 epoll.fromfd(fd)Create an epoll object from a given file descriptor.根据指定的fd创建epoll对象 epoll.register(fd[, eventmask])Register a fd descriptor with the epoll object.向epoll对象中注册fd和对应的事件 epoll.modify(fd, eventmask)Modify a registered file descriptor.修改fd的事件 epoll.unregister(fd)Remove a registered file descriptor from the epoll object.取消注册 epoll.poll(timeout=-1, maxevents=-1)Wait for events. timeout in seconds (float)阻塞，直到注册的fd事件发生,会返回一个dict，格式为：{(fd1,event1),(fd2,event2),……(fdn,eventn)} 事件： EPOLLIN Available for read 可读 状态符为1EPOLLOUT Available for write 可写 状态符为4EPOLLPRI Urgent data for readEPOLLERR Error condition happened on the assoc. fd 发生错误 状态符为8EPOLLHUP Hang up happened on the assoc. fd 挂起状态EPOLLET Set Edge Trigger behavior, the default is Level Trigger behavior 默认为水平触发，设置该事件后则边缘触发EPOLLONESHOT Set one-shot behavior. After one event is pulled out, the fd is internally disabledEPOLLRDNORM Equivalent to EPOLLINEPOLLRDBAND Priority data band can be read.EPOLLWRNORM Equivalent to EPOLLOUTEPOLLWRBAND Priority data may be written.EPOLLMSG Ignored. 水平触发和边缘触发： Level_triggered(水平触发，有时也称条件触发)：当被监控的文件描述符上有可读写事件发生时，epoll.poll()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll.poll()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你！！！如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率！！！ 优点很明显：稳定可靠 Edge_triggered(边缘触发，有时也称状态触发)：当被监控的文件描述符上有可读写事件发生时，epoll.poll()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll.poll()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符！！！缺点：某些条件下不可靠 epoll实例： import socketimport select s = socket.socket()s.bind((‘127.0.0.1’,8888))s.listen(5)epoll_obj = select.epoll()epoll_obj.register(s,select.EPOLLIN)connections = {}while True: events = epoll_obj.poll() for fd, event in events: print(fd,event) if fd == s.fileno(): conn, addr = s.accept() connections[conn.fileno()] = conn epoll_obj.register(conn,select.EPOLLIN) msg = conn.recv(200) conn.sendall(‘ok’.encode()) else: try: fd_obj = connections[fd] msg = fd_obj.recv(200) fd_obj.sendall(‘ok’.encode()) except BrokenPipeError: epoll_obj.unregister(fd) connections[fd].close() del connections[fd] s.close()epoll_obj.close() import socket flag = 1s = socket.socket()s.connect((‘127.0.0.1’,8888))while flag: input_msg = input(‘input&gt;&gt;&gt;’) if input_msg == ‘0’: break s.sendall(input_msg.encode()) msg = s.recv(1024) print(msg.decode()) s.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python：generator的send()方法流程分析]]></title>
    <url>%2F2018%2F10%2F04%2FPython%EF%BC%9Agenerator%E7%9A%84send()%E6%96%B9%E6%B3%95%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Python：generator的send()方法流程分析先来一个简单地例子： def foo(): print(‘starting’) while True: r = yield 2 print(r) f = foo()print(f.send(None))print(f.send(1)) 结果如下： starting 讲解： 1 f = foo() 这句表示生成一个generator对象 f = foo()print(type(f)) #&lt;class ‘generator’&gt;2 f.send(None) 的作用与 next(f) 的作用相同：运行代码到 r = yield 2 处。 r = yield 2 主要分两步： 第一步： yield 2 ，也就是先返回2 第二步： r = (yield) 这里用括号把yield包起来是为了突出yield是一个表达式expression：可以用来表示某个值。 f.send(None) 或者说 next(f) 仅仅运行到了第一步，也就是返回了2，然后被print()函数打印到屏幕 3 f.send(1) 运行第二步，将1赋值给r ，然后运行print(r)，再一次运行到 r = yield 2 处时，也仅仅只运行第一步，也就是返回2，然后由print()函数打印到屏幕。 注意以下情况不会报错： def foo(): yield 1 yield 2 yield 3 f = foo()r = next(f)print(r)for i in range(2): r = f.send(‘hello world’) print(r) 结果也就是说send非空值时，即使yield语句前没有变量接收send的值，程序不会报错]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入python中的self]]></title>
    <url>%2F2018%2F09%2F21%2F%E6%B7%B1%E5%85%A5python%E4%B8%AD%E7%9A%84self%2F</url>
    <content type="text"><![CDATA[深入python中的self在Python中一切皆对象，不太清楚为什么请看：深刻理解Python中的元类(metaclass) 一句话概括：self表示类的实例对象，而非类对象。 class Foo(object): def init(self): print(‘Foo is %s, self is %s’%(Foo,self)) def prt(self): print(&apos;----------&apos;)r = Foo() 结果如下Foo is &lt;class ‘main.Foo’&gt;, self is &lt;main.Foo object at 0x02833C50&gt;当我们执行 r.prt() 时，Python解释器默认会调用Foo.prt()并将r作为第一个参数传入，也就是Foo.prt(r) 当然self不用非得写成self，this或者其他都可以。 当调用某个方法时，可以通过self来查看其调用者，self是一个只读属性，不可修改。 class Bar(object): def prt(self): print(‘bar’) b = Bar()p = b.prtprint(p.self) &lt;main.Bar object at 0x017AF970&gt;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单递归算法题]]></title>
    <url>%2F2018%2F09%2F07%2F%E7%AE%80%E5%8D%95%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[简单递归算法题递归题解决思路： 1 找到出口 2 找到f(n)与f(n-1)之间的关系 最近看了一些递归题，都很简单。根据上面的思路很容易就做出来。下面来看Python的实现： #!/usr/bin/env python -- coding: utf-8 --@Time : 2017/4/28 16:41@Author : MnCu@Site :@File : fbnq.py@Software: PyCharm一对兔子从出生到可繁殖需两个月，然后每月都能繁殖一对兔子，问n月后共有多少兔子def fbnc_func(n): ‘’’ 1 出口： 当月份小于等于2月时，则返回1 2 f(n)与f(n-1)的关系： 当月的兔子数 = 上个月的兔子数 + 上个月可以繁殖的兔子数 ‘’’ if n &lt;= 2: return 1 else: return fbnc_func(n-1) + fbnc_func(n-2) num = fbnc_func(12)print(num)楼梯有20阶，每次可以走一阶或者走两阶，问共有多少种走法def louti(n): ‘’’ 1 出口： 当楼梯数为1时，返回1 当楼梯数位2时，返回2 2 关系： n层楼梯走法 = (n-1)层楼梯走法 + (n-2)层楼梯走法 ‘’’ assert n &gt; 0 if n == 1: return 1 elif n == 2: return 2 else: return louti(n-1) + louti(n-2) r = louti(20)print(r)一列数的规则如下: 1、12、123、1234、12345、123456……,求第n个数的递归算法（n&lt;=9）def tran(n): ‘’’ 1 出口： 如果n为1，则返回1 2 关系： 当前的数 = int(前一个数的字符串格式 + n的字符串格式) &apos;&apos;&apos; n = int(n) if n == 1: return 1 else: return int(str(tran(n-1)) + str(n))r = tran(6)print(r)将一整数逆序，如987654321变为123456789。def reverse(num): ‘’’ 1 出口： (首先讲num转换为字符串) num为空，返回’’ num长度为1,返回num 2 关系： 当前被反转后的数 = 当前未被反转的数的末尾(字符串格式) + 前面的数(字符串格式) &apos;&apos;&apos; num = str(num) if len(num) == 0: return &apos;&apos; elif len(num) == 1: return num else: return num[-1] + reverse(num[:-1])r = reverse(123456)print(r)请写一段代码来计算给定文本内字符“A”的个数。用递归方式。def count(chr): ‘’’ 1 出口： 当字符串长度为0返回0 当字符串长度为1且该字符是‘A’则返回1，否则返回0 2 关系： 当前字符串中‘A’的个数 = 上一个字符串(也就是当前字符串减去最后一项)中‘A’的个数 + 最后一个字符中‘A’的个数 &apos;&apos;&apos; if len(chr) == 0: return 0 elif len(chr) == 1: if chr == &apos;A&apos;: return 1 return 0 else: return count(chr[:-1]) + count(chr[-1])r = count(‘AALKSJDFK’)print(r)一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。def taijie(n): ‘’’ 1 出口; 台阶数为1，则有1种跳法 台阶数为2，有2种跳法 2 关系： 分析：用Fib(n)表示青蛙跳上n阶台阶的跳法数，青蛙一次性跳上n阶台阶的跳法数1(n阶跳)，设定Fib(0) = 1； 当n = 1 时， 只有一种跳法，即1阶跳：Fib(1) = 1; 当n = 2 时， 有两种跳的方式，一阶跳和二阶跳：Fib(2) = Fib(1) + Fib(0) = 2; 当n = 3 时，有三种跳的方式，第一次跳出一阶后，后面还有Fib(3-1)中跳法； 第一次跳出二阶后，后面还有Fib(3-2)中跳法；第一次跳出三阶后，后面还有Fib(3-3)中跳法 Fib(3) = Fib(2) + Fib(1)+Fib(0)=4; 当n = n 时，共有n种跳的方式，第一次跳出一阶后，后面还有Fib(n-1)中跳法； 第一次跳出二阶后，后面还有Fib(n-2)中跳法……………………..第一次跳出n阶后，后面还有 Fib(n-n)中跳法. Fib(n) = Fib(n-1)+Fib(n-2)+Fib(n-3)+……….+Fib(n-n)=Fib(0)+Fib(1)+Fib(2)+…….+Fib(n-1) 又因为Fib(n-1)=Fib(0)+Fib(1)+Fib(2)+…….+Fib(n-2) 两式相减得：Fib(n)-Fib(n-1)=Fib(n-1) =====》 Fib(n) = 2*Fib(n-1) n &gt;= 2 ‘’’ if n == 1: return 1 elif n == 2: return 2 else: return 2 * taijie(n-1)r = taijie(20)print(r)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用python自己实现的一个简单Orm框架]]></title>
    <url>%2F2018%2F08%2F21%2F%E4%BD%BF%E7%94%A8python%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95Orm%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[使用python自己实现的一个简单Orm框架一个小型ORM框架，基于pymysql实现，较为简单。 1 #!/usr/bin/env python 2 # -- coding: utf-8 -- 3 4 import pymysql 5 from utils import NotImplementedError 6 7 ‘’’ 8 本文件是基于mysql实现的一个ORM框架 9 ‘’’ 10 11 class MysqlConnector(object): 12 ‘’’Python与mysql的连接器’’’ 13 14 def init(self, host, port, username, password, db): 15 conn = pymysql.connect(host=host, port=port, user=username, 16 passwd=password, db=db, use_unicode=True, charset=”utf8”) 17 self.conn = conn 18 self.cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) 19 20 def execute(self, sql_msg): 21 ‘’’ 22 执行sql语句 23 :param sql_msg: sql语句，字符串格式 24 :return: 25 ‘’’ 26 ret = self.cursor.execute(sql_msg) 27 self.conn.commit() 28 return ret 29 30 def close(self): 31 ‘’’关闭连接器’’’ 32 self.cursor.close() 33 self.conn.close() 34 35 class BaseModel(object): 36 ‘’’ 37 实现将Python语句转换为sql语句，配合MysqlConnector实现表的创建以及数据的增删查改等操作。 38 创建表时： 支持主键PRIMARY KEY，索引INDEX，唯一索引UNIQUE，自增AUTO INCREMENT 外键语句 39 创建的表引擎指定为InnoDB，字符集为 utf-8 40 增删查改： 支持WHERE [LIKE] LIMIT语句 41 其子类必须设置initialize方法，并在该方法中创建字段对象 42 ‘’’ 43 def new(cls, args, **kwargs): 44 instance = super()._new(cls) 45 instance.initialize() 46 return instance 47 48 def __init(self, table_name, sql_connector): 49 ‘’’ 50 :param table_name: 要建立的表名 51 :param sql_connector: MysqlConnector实例对象 52 ‘’’ 53 self.table_name = table_name 54 self.fields = [] 55 self.primary_key_field = None 56 self.uniques_fields = [] 57 self.index_fields = [] 58 self.is_foreign_key_fields = [] 59 self.sql_connector = sql_connector 60 self.create_fields_list() 61 self.create_table() 62 63 def initialize(self): 64 ‘’’BaseModel的每个子类中必需包含该方法，且在该方法中定义字段’’’ 65 raise NotImplementedError(“Method or function hasn’t been implemented yet.”) 66 67 def create_fields_list(self): 68 ‘’’创建list用来存储表的字段对象’’’ 69 for k, v in self.__dict.items(): 70 if isinstance(v, BaseField): 71 self.fields.append(v) 72 v.full_column = ‘%s.%s’ % (self.table_name, v.db_column) 73 v.table_name = self.table_name 74 for field in self.fields: 75 if field.primary_key: 76 self.primary_key_field = field 77 if field.unique: 78 self.uniques_fields.append(field) 79 if field.db_index: 80 self.index_fields.append(field) 81 if field.is_foreign_key: 82 self.is_foreign_key_fields.append(field) 83 84 def _has_created(self): 85 ‘’’检测表有没有被创建’’’ 86 self.sql_connector.cursor.execute(‘SHOW TABLES;’) 87 ret = self.sql_connector.cursor.fetchall() 88 for table in ret: 89 for k, v in table.items(): 90 if v == self.table_name: 91 return True 92 93 def _create_table(self): 94 ret = ‘CREATE TABLE %s (‘ % self.table_name 95 for v in self.fields: 96 ret += v.generate_field_sql() 97 ret = ‘%s%s%s%s%s’ % (ret, self._generate_primary_key(), 98 self._generate_unique(), self._generate_index(), 99 self._generate_is_foreign_key())100 ret = ret[:-1] + ‘)ENGINE=InnoDB DEFAULT CHARSET=utf8;’101 return ret102103 def create_table(self):104 ‘’’创建表’’’105 if not self._has_created():106 print(‘创建表：%s’ % self.table_name)107 sql_msg = self._create_table()108 # print(sql_msg)109 self.sql_connector.execute(sql_msg)110111 def _generate_primary_key(self):112 ‘’’生成sql语句中的 primary key 语句’’’113 ret = ‘’114 if self.primary_key_field:115 ret = ‘PRIMARY KEY(%s),’ % self.primary_key_field.db_column116 return ret117118 def _generate_is_foreign_key(self):119 ret = ‘’120 if self.is_foreign_key_fields:121 for field in self.is_foreign_key_fields:122 ret += ‘FOREIGN KEY(%s) REFERENCES %s(%s) ON DELETE %s ON UPDATE %s,’ % (field.db_column,123 field.model_obj.table_name,124 field.model_obj.primary_key_field.db_column,125 field.on_delete,126 field.on_delete )127 return ret128129 def _generate_unique(self):130 ‘’’生成sql语句中的 unique 语句’’’131 ret = ‘’132 if self.uniques_fields:133 ret = ‘UNIQUE (‘134 for field in self.uniques_fields:135 ret += ‘%s,’ % field.db_column136 ret = ret[:-1]137 ret += ‘),’138 return ret139140 def _generate_index(self):141 index = ‘’142 if self.index_fields:143 index = ‘INDEX (‘144 for field in self.index_fields:145 index += ‘%s,’ % field.db_column146 index = index[:-1]147 index += ‘),’148 return index149150 def _generate_where(self, condition={}):151 ‘’’152 根据条件生成 where 语句153 :param condition: 一个dict，key是字段对象，value是条件(比如 ‘WHERE ID=3’,那么value就是’=3’)154 :return:155 ‘’’156 where = ‘’157 if condition:158 where = ‘ WHERE ‘159 for k, v in condition.items():160 v = v.strip()161 offset = 1162 if v.startswith(‘l’):163 offset = 4164 if not k.is_str:165 where += ‘%s %s and’ % (k.db_column, v)166 else:167 where += ‘%s %s “%s” and’ % (k.db_column, v[:offset], v[offset:].strip())168 where = where[:-3]169 return where170171 def select_items(self, counts=0, select_fields=[], condition={}, join_conditions=[]):172 ‘’’173 根据condition 对表进行select，并 LIMIT counts174 :param counts:175 :param condition:176 :return:177 ‘’’178 join_length = len(join_conditions)179 counts_sql = ‘’180 join_sql = ‘’181 select_fields_sql = ‘’182 where = self._generate_where(condition)183 if counts:184 counts_sql = ‘LIMIT %s’ % counts185 if not select_fields:186 select_fields_sql = ‘ ‘187 if join_conditions:188 tables_order = list(list(zip(join_conditions))[0])189 tables_order.insert(0, self)190 for i in select_fields:191 select_fields_sql += ‘%s,’ % i.full_column192 for n in range(join_length):193 one_join_condition = join_conditions[n]194 if n == 0:195 base_table = tables_order[0].table_name196 else:197 base_table = ‘’198 bracket_counts = join_length - n - 1199 join_sql += ‘%s %s LEFT JOIN %s on %s=%s%s’ % (200 bracket_counts‘(‘, base_table, tables_order[n+1].table_name,201 one_join_condition[1].full_column, one_join_condition[2].full_column,202 bracket_counts * ‘)’, )203 else:204 for i in select_fields:205 select_fields_sql += ‘%s,’ % i.db_column206 join_sql = self.table_name207 select_fields_sql = select_fields_sql[:-1]208 select = ‘SELECT %s FROM %s %s %s;’ % (select_fields_sql, join_sql, where, counts_sql)209 # print(‘—————-‘, select)210 self.sql_connector.execute(select)211 result = self.sql_connector.cursor.fetchall()212 return result213214 def insert_item(self, data={}):215 ‘’’216 向表中插入一行217 :param data: 一个dict，key是字段对象，value则是值218 :return:219 ‘’’220 insert = ‘INSERT INTO %s (‘ % self.table_name221 value = ‘(‘222 if data:223 for k, v in data.items():224 insert += ‘%s,’ % k.db_column225 if k.is_str:226 value += ‘“%s”,’ % v227 else:228 value += ‘%s,’ % v229 # print(‘value is ‘,value)230 insert = insert[:-1] + ‘) VALUES ‘231 value = value[:-1] + ‘);’232 insert += value233 # print(‘……’,insert)234 self.sql_connector.execute(insert)235236 def delete_item(self, condition={}):237 ‘’’删除符合condition的条目’’’238 delete = ‘DELETE FROM %s ‘ % self.table_name239 where = self.generate_where(condition)240 delete += where241 # print(delete)242 self.sql_connector.execute(delete)243244 def update_item(self, data={}, condition={}):245 ‘’’将符合condition的条目修改为data’’’246 update = ‘UPDATE %s’ % self.table_name247 data_statement = ‘’248 if data:249 data_statement = ‘ SET ‘250 for k, v in data.items():251 if not k.is_str:252 data_statement += ‘%s=%s,’ % (k.db_column, v)253 else:254 data_statement += ‘%s=”%s”,’ % (k.db_column, v)255 data_statement = data_statement[:-1]256 where = self.generate_where(condition)257 update += data_statement + where258 # print(‘———‘,update)259 self.sql_connector.execute(update)260261 def get_field_value(self, field, condition={}):262 ret = self.select_items(condition=condition)263 # print(ret)264 if len(ret) == 1:265 value = ret[0][field.db_column]266 elif len(ret) &gt; 1:267 value = []268 for i in ret:269 value.append(i[field.db_column])270 else:271 value = ‘’272 # print(‘value is ‘,value)273 return value274275 class BaseField(object):276 def __init(self, db_column, null=True, blank=None, choice={},277 db_index=False, default=None, primary_key=False,278 unique=False, max_length=0, auto_increment=False,279 ):280 ‘’’281282 :param db_column: 数据库中表的字段名283 :param null: 该字段是否可以为空284 :param blank: 如果该字段为空，存储什么值285 :param choice: 该字段的值只能是choice的一个286 :param db_index: 是否为该字段设置索引287 :param default: 该字段的默认值288 :param primary_key: 是否为该字段设置主键289 :param unique: 该字段值是否可以重复290 :param max_length： 该字段的最大长度291 :param auto_increment: 是否自增292 ‘’’293 self.db_column = db_column294 self.null = null295 self.blank = blank296 self.choice = choice297 self.db_index = db_index298 self.default = default299 self.primary_key = primary_key300 if self.primary_key:301 self.null = False302 self.unique = unique303 self.max_length = max_length304 self.auto_increment = auto_increment305 self.is_foreign_key = False306307 def generate_field_sql(self):308 pass309310 def generate_null(self):311 if not self.null:312 null = ‘NOT NULL’313 else:314 null = ‘NULL’315 return null316317 def generate_default(self):318 default = ‘’319 if self.default is not None:320 if self.is_str:321 default = ‘ DEFAULT “%s”‘ % self.default322 else:323 default = ‘ DEFAULT %s’ % self.default324 return default325326 def _generate_auto_increment(self):327 ret = ‘’328 if self.auto_increment:329 ret = ‘AUTO_INCREMENT’330 return ret331332 class CharField(BaseField):333 def __init(self, args, **kwargs):334 super().init(args, kwargs)335 kwargs[‘blank’] = ‘’336 if not self.max_length:337 self.max_length = 128338 if not self.default:339 self.default = self.blank340 self.is_str = True341 self.field_type = ‘CHAR’342343 def generate_field_sql(self):344 null = self.generate_null()345 default = self.generate_default()346 return ‘%s CHAR(%s) %s %s,’ % (self.db_column, self.max_length, null, default)347348 class IntField(BaseField):349 def __init(self, *args, *kwargs):350 super().init(args, **kwargs)351 self.is_str = False352 self.field_type = ‘INT’353354 def generate_field_sql(self):355 null = self.generate_null()356 default = self.generate_default()357 auto_increment = self._generate_auto_increment()358 return ‘%s INT %s %s %s,’ % (self.db_column, null, default, auto_increment)359360 class ForeignKeyField(BaseField):361 def __init(self, db_column, model_obj, null=True, default=None, on_delete=’CASCADE’):362 self.db_column = db_column363 self.model_obj = model_obj364 self.null = null365 self.default = default366 self.is_str = model_obj.primary_key_field.is_str367 self.reference = model_obj.primary_key_field368 self.on_delete = on_delete369 self.is_foreign_key = True370 self.primary_key = False371 self.unique = False372 self.db_index = False373374 def generate_field_sql(self):375 null = self.generate_null()376 default = self.generate_default()377 return ‘%s %s %s %s,’ % (self.db_column, self.model_obj.primary_key_field.field_type, null, default)378379380 Connector = MysqlConnector(‘127.0.0.1’, 3306, ‘root’, ‘’, ‘test1’)381382383 if __name == ‘main‘:384 class UserModel(BaseModel):385 def initialize(self):386 self.uid = IntField(‘uid’, primary_key=True, auto_increment=True)387 self.account = IntField(‘account’, unique=True, null=False)388 self.password = CharField(‘password’, null=False)389 self.name = CharField(‘name’, null=False)390 self.class_name = CharField(‘class_name’, null=False)391 self.profession = CharField(‘profession’, null=False)392 self.out_date_counts = IntField(‘out_date_counts’, default=0)393394 u = UserModel(‘user’, Connector)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的requests模块注意事项]]></title>
    <url>%2F2018%2F08%2F01%2FPython%E4%B8%AD%E7%9A%84requests%E6%A8%A1%E5%9D%97%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[Python中的requests模块注意事项主要是说requests.post()方法， 参数： url ： 这就不解释了 data： 如果传入的是字典类型，则字典在发出请求时会自动编码为表单形式，表单形式会将字典中的键和值进行一些操作： key1=value1&amp;key2=value2 如果传入的是字符串类型，则数据会被直接发送出去。 2.4.2版本开始提供了json参数，默认会执行json.dumps() headers 我们可以自定义请求头部。 content-type: application/json application/json用来告诉服务端消息主体是序列化后的 JSON 字符串 application/x-www-form-urlencoded 浏览器的原生 form 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。 他会按照原始的表单编码方式，也就是将键值对按照以下方式编码：title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3 multipart/form-data 浏览器如果要在表单上传文件必须指定该首部 text/xml xml格式，相对于json来说xml更重量级。]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python变量类型]]></title>
    <url>%2F2018%2F07%2F26%2FPython%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Python中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字）String（字符串）List（列表）Tuple（元组）Dictionary（字典）Python数字Python支持四种不同的数字类型： int（有符号整型）long（长整型[也可以代表八进制和十六进制]）float（浮点型）complex（复数）Python字符串Python的字符串列表有两种取值顺序： 正向：从左导右索引默认从0开始，最大范围是字符串长度减1反向：从右到左索引默认从-1开始，最大范围是字符串开头如果需要取得一段子串，可以使用 变量[头下标 : 尾下标]，就可以截取到相应的字符串。其中下标是从0开始算起的，可以是整数或负数，下标可以为空，表示取到头或尾。 str = ‘Hello World!’ print str # 输出完整字符串print str[0] # 输出字符串中的第一个字符print str[2:5] # 输出字符串中的第三个到第五个之间的字符串print str[2:] # 输出从第三个字符开始的字符串print str *2 # 输出字符串两次print str + “TEST” # 输出连接的字符串 以上例子输出的结果为：Hello World! Hllollo World! Hello World!Hello World! Hello World!TEST Python列表List（列表）是Python中使用最频繁的数据类型。列表用 “[]” 标识。可以完成大多数集合类数据结构实现。是Python最通用的复合数据类型。 列表中的值的分割也可以用到变量 [头下标 : 尾下标]，就可以截取相应的列表。 list = [‘jesse’, 786, 2.23, ‘john’, 70.2]tinylist = [123, ‘john’] print list # 输出完整列表print list[0] # 输出列表的第一个元素print list[1:3] # 输出第二个至第三个的元素print list[2:] # 输出从第三个开始至列表末尾的所有元素print tinylist *2 # 输出列表两次print list + tinylist # 打印组合的列表 以上实例输出结果：[‘jesse’, 786, 2.23, ‘jhon’, 70.2]jesse[786, 2.23][123, ‘jhon’, 70.2][123, ‘jhon’, 123, ‘jhon’][‘jesse’, 786, 2.23, ‘jhon’, 70.2, 123, ‘jhon’] Python元组元组类似于List（列表）。元组用 “()” 标识。内部元素用逗号隔开。元组不能二次赋值，相当于只读列表。 tuple = (‘jesse’, 786, 2.23, ‘jhon’, 70.2)tinytuple = (123, ‘jhon’) print tuple # 输出完整元组print tuple[0] # 输出元组的第一个元素print tuple[1:3] # 输出第二到第三个的元素print tuple[2:] # 输出从第三个开始至元组末尾的所有元素print tinytuple *2 # 输出元组两次print tuple + tinytuple # 打印组合的元组 以上例子输出结果：(‘jesse’, 786, 2.23, ‘john’, 70.2)jesse(786, 2.23)(2.23, ‘john’, 70.2)(123, ‘john’, 123, ‘john’)(‘jesse’, 786, 2.23, ‘john’, 70.2, 123, ‘john’) Python元字典字典（dictionary）是除列表以外Python中最灵活的内置数据结构类型。列表是有序的对象集合，字典是无序的对象集合。两者的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取的。字典由索引（key）和它对应的值（value）组成。用“{ }”标识。 dict = {}dict[‘one’] = “This is one”dict[2] = “This is two” tinydict = {‘name’:’jone’,’code’:1234,’dept’:’sales’} print dict[‘one’] #输出键为’one’的值print dict[2] #输出键为 2 的值print tinydict #输出完整的字典print tinydict.keys() #输出所有键print tinydict.values() #输出所有值 以上例子输出结果为：This is oneThis is two{‘dept’:’sales’,’code’:1234,’name’:’john’}[‘dept’,’code’,’name’][‘sales’,1234,’john’] Python数据类型转换数据类型转换，只需要将数据类型作为函数名即可。函数会返回一个新的对象，表示转换的值。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)flask的context机制]]></title>
    <url>%2F2018%2F07%2F21%2F(%E8%BD%AC)flask%E7%9A%84context%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[(转)flask的context机制本文转自：https://blog.tonyseek.com/post/the-context-mechanism-of-flask/ 作者：无知的 TonySeek 注意：本文仅仅作为个人mark，所以排版不如原文，另本文对于原文做了部分的修改，但由于个人技术能力尚浅，可能会误导读者，建议到原文查看。 用过 Flask 做 Web 开发的同学应该不会不记得 App Context 和 Request Context 这两个名字——这两个 Context 算是 Flask 中比较特色的设计。[1] 从一个 Flask App 读入配置并启动开始，就进入了 App Context，在其中我们可以访问配置文件、打开资源文件、通过路由规则反向构造 URL。[2] 当一个请求进入开始被处理时，就进入了 Request Context，在其中我们可以访问请求携带的信息，比如 HTTP Method、表单域等。[3] 所以，这两个 Context 也成了 Flask 框架复杂度比较集中的地方，对此有评价认为 Flask 的这种设计比 Django、Tornado 等框架的设计更为晦涩。[4] 我不认同这种评价。对于一个 Web 应用来说，“应用” 和 “请求” 的两级上下文在理念上是现实存在的，如果理解了它们，那么使用 Flask 并不会晦涩；即使是使用 Django、Tornado，理解了它们的 Context 也非常有利于做比官网例子更多的事情（例如编写 Middleware）。 我因为开发 Flask 扩展，对这两个 Context 的具体实现也研究了一番，同时还解决了一些自己之前“知道结论不知道过程”的疑惑，所以撰写本文记录下来。 Thread Local 的概念 从面向对象设计的角度看，对象是保存“状态”的地方。Python 也是如此，一个对象的状态都被保存在对象携带的一个特殊字典中，可以通过 vars 函数拿到它。 Thread Local 则是一种特殊的对象，它的“状态”对线程隔离 —— 也就是说每个线程对一个 Thread Local 对象的修改都不会影响其他线程。这种对象的实现原理也非常简单，只要以线程的 ID 来保存多份状态字典即可，就像按照门牌号隔开的一格一格的信箱。 在 Python 中获得一个这样的 Thread Local 最简单的方法是 threading.local()： Python Shell import threadingstorage = threading.local()storage.foo = 1print(storage.foo)1 class AnotherThread(threading.Thread):… def run(self):… storage.foo = 2… print(storage.foo) # 这这个线程里已经修改了 another = AnotherThread()another.start()2 print(storage.foo) # 但是在主线程里并没有修改1 这样来说，只要能构造出 Thread Local 对象，就能够让同一个对象在多个线程下做到状态隔离。这个“线程”不一定要是系统线程，也可以是用户代码中的其他调度单元，例如 Greenlet。[5] Werkzeug 实现的 Local Stack 和 Local Proxy Werkzeug 没有直接使用 threading.local，而是自己实现了 werkzeug.local.Local 类。后者和前者有一些区别： 后者会在 Greenlet 可用的情况下优先使用 Greenlet 的 ID 而不是线程 ID 以支持 Gevent 或 Eventlet 的调度，前者只支持多线程调度； 后者实现了 Werkzeug 定义的协议方法 release_local，可以被 Werkzeug 自己的 release_pool 函数释放（析构）掉当前线程下的状态，前者没有这个能力。 除 Local 外，Werkzeug 还实现了两种数据结构：LocalStack 和 LocalProxy。 LocalStack 是用 Local 实现的栈结构，可以将对象推入、弹出，也可以快速拿到栈顶对象。当然，所有的修改都只在本线程可见。和 Local 一样，LocalStack 也同样实现了支持 release_pool 的接口。 LocalProxy 则是一个典型的代理模式实现，它在构造时接受一个 callable 的参数（比如一个函数），这个参数被调用后的返回值本身应该是一个 Thread Local 对象。对一个 LocalProxy 对象的所有操作，包括属性访问、方法调用（当然方法调用就是属性访问）甚至是二元操作 [6] 都会转发到那个 callable 参数返回的 Thread Local 对象上。 LocalProxy 的一个使用场景是 LocalStack 的 call 方法。比如 my_local_stack 是一个 LocalStack 实例，那么 my_local_stack() 能返回一个 LocalProxy 对象，这个对象始终指向 my_local_stack 的栈顶元素。如果栈顶元素不存在，访问这个 LocalProxy 的时候会抛出 RuntimeError。 Flask 基于 Local Stack 的 Context Flask 是一个基于 Werkzeug 实现的框架，所以 Flask 的 App Context 和 Request Context 也理所当然地基于 Werkzeug 的 Local Stack 实现。 在概念上，App Context 代表了“应用级别的上下文”，比如配置文件中的数据库连接信息；Request Context 代表了“请求级别的上下文”，比如当前访问的 URL。 这两种上下文对象的类定义在 flask.ctx 中，它们的用法是推入 flask.globals 中创建的 _app_ctx_stack 和 _request_ctx_stack 这两个单例 Local Stack 中。因为 Local Stack 的状态是线程隔离的，而 Web 应用中每个线程（或 Greenlet）同时只处理一个请求，所以 App Context 对象和 Request Context 对象也是请求间隔离的。 当 app = Flask(name) 构造出一个 Flask App 时，App Context 并不会被自动推入 Stack 中。所以此时 Local Stack 的栈顶是空的，current_app 也是 unbound 状态。 Python Shell from flask import Flaskfrom flask.globals import _app_ctx_stack, _request_ctx_stack app = Flask(name) _app_ctx_stack.top_request_ctx_stack.top_app_ctx_stack() from flask import current_appcurrent_app 这也是一些 Flask 用户可能被坑的地方 —— 比如编写一个离线脚本时，如果直接在一个 Flask-SQLAlchemy 写成的 Model 上调用 User.query.get(user_id)，就会遇到 RuntimeError。因为此时 App Context 还没被推入栈中，而 Flask-SQLAlchemy 需要数据库连接信息时就会去取 current_app.config，current_app 指向的却是 _app_ctx_stack 为空的栈顶。 解决的办法是运行脚本正文之前，先将 App 的 App Context 推入栈中，栈顶不为空后 current_app 这个 Local Proxy 对象就自然能将“取 config 属性” 的动作转发到当前 App 上了： Python Shell ctx = app.app_context()ctx.push()app_ctx_stack.top&lt;flask.ctx.AppContext object at 0x102eac7d0&gt;app_ctx_stack.top is ctxTruecurrent_app&lt;Flask ‘__main‘&gt; ctx.pop()_app_ctx_stack.topcurrent_app 那么为什么在应用运行时不需要手动 app_context().push() 呢？因为 Flask App 在作为 WSGI Application 运行时，会在每个请求进入的时候将请求上下文推入 _request_ctx_stack 中，而请求上下文一定是 App 上下文之中，所以推入部分的逻辑有这样一条：如果发现 _app_ctx_stack 为空，则隐式地推入一个 App 上下文。 所以，请求中是不需要手动推上下文入栈的，但是离线脚本需要手动推入 App Context。如果没有什么特殊困难，我更建议用 Flask-Script 来写离线任务。[7] 两个疑问 到此为止，就出现两个疑问： 为什么 App Context 要独立出来：既然在 Web 应用运行时里，App Context 和 Request Context 都是 Thread Local 的，那么为什么还要独立二者？ 为什么要放在“栈”里：在 Web 应用运行时中，一个线程同时只处理一个请求，那么 _req_ctx_stack 和 _app_ctx_stack 肯定都是只有一个栈顶元素的。那么为什么还要用“栈”这种结构？我最初也被这两个疑问困惑过。后来看了一些资料，就明白了 Flask 为何要设计成这样。这两个做法给予我们 多个 Flask App 共存 和 非 Web Runtime 中灵活控制 Context 的可能性。 我们知道对一个 Flask App 调用 app.run() 之后，进程就进入阻塞模式并开始监听请求。此时是不可能再让另一个 Flask App 在主线程运行起来的。那么还有哪些场景需要多个 Flask App 共存呢？前面提到了，一个 Flask App 实例就是一个 WSGI Application，那么 WSGI Middleware 是允许使用组合模式的，比如： from werkzeug.wsgi import DispatcherMiddlewarefrom biubiu.app import create_appfrom biubiu.admin.app import create_app as create_admin_app application = DispatcherMiddleware(create_app(), {‘/admin’: create_admin_app()}) 这个例子就利用 Werkzeug 内置的 Middleware 将两个 Flask App 组合成一个一个 WSGI Application。这种情况下两个 App 都同时在运行，只是根据 URL 的不同而将请求分发到不同的 App 上处理。 Note 需要注意的是，这种用法和 Flask 的 Blueprint 是有区别的。Blueprint 虽然和这种用法很类似，但前者自己没有 App Context，只是同一个 Flask App 内部整理资源的一种方式，所以多个 Blueprint 可能共享了同一个 Flask App；后者面向的是所有 WSGI Application，而不仅仅是 Flask App，即使是把一个 Django App 和一个 Flask App 用这种用法整合起来也是可行的。 如果仅仅在 Web Runtime 中，多个 Flask App 同时工作倒不是问题。毕竟每个请求被处理的时候是身处不同的 Thread Local 中的。但是 Flask App 不一定仅仅在 Web Runtime 中被使用 —— 有两个典型的场景是在非 Web 环境需要访问上下文代码的，一个是离线脚本（前面提到过），另一个是测试。这两个场景即所谓的“Running code outside of a request”。在非 Web 环境运行 Flask 关联的代码，离线脚本或者测试这类非 Web 环境和和 Web 环境不同 —— 前者一般只在主线程运行。 设想，一个离线脚本需要操作两个 Flask App 关联的上下文，应该怎么办呢？这时候栈结构的 App Context 优势就发挥出来了。 offline_script.pyfrom biubiu.app import create_appfrom biubiu.admin.app import create_app as create_admin_app app = create_app()admin_app = create_admin_app() def copy_data(): with app.app_context(): data = read_data() # fake function for demo with admin_app.app_context(): write_data(data) # fake function for demo mark_data_copied() # fake function for demo 无论有多少个 App，只要主动去 Push 它的 App Context，Context Stack 中就会累积起来。这样，栈顶永远是当前操作的 App Context。当一个 App Context 结束的时候，相应的栈顶元素也随之出栈。如果在执行过程中抛出了异常，对应的 App Context 中注册的 teardown 函数被传入带有异常信息的参数。 这么一来就解释了两个疑问 —— 在这种单线程运行环境中，只有栈结构才能保存多个 Context 并在其中定位出哪个才是“当前”。而离线脚本只需要 App 关联的上下文，不需要构造出请求，所以 App Context 也应该和 Request Context 分离。 另一个手动推入 Context 的场景是测试。测试中我们可能会需要构造一个请求，并验证相关的状态是否符合预期。例如： tests.pydef test_app(): app = create_app() client = app.test_client() resp = client.get(‘/‘) assert ‘Home’ in resp.data这里调用 client.get 时，Request Context 就被推入了。其特点和 App Context 非常类似，这里不再赘述。 为何建议使用 App Factory 模式从官方文档来看，Flask 有 Singleton 和 App Factory 两种用法。前一种用法和其他的一些 Web 框架（如 Bottle、Sinatra）的门面广告很相似，因为代码精简，所以显得非常的“帅”： app.pyfrom flask import Flask, render_templatefrom flask.ext.sqlalchemy import SQLAlchemyfrom flask.ext.login import LoginManager app = Flask(name) db = SQLAlchemy(app)login_manager = LoginManager() @app.route(‘/‘)def home(): return render_template(‘home.html’) 但是这种“帅”是有代价的。一个最麻烦的问题就是编写测试的时候： test_app.pyclass TestApp(unittest.TestCase): DEBUG = FalseTESTING = TrueSQLALCHEMY_DATABASE_URI = None def setUp(self):self.app = create_app()self.app.config.from_object(self)self.client = self.app.test_client() def test_app(self):@self.app.route(‘/test/int:id_‘)def my_view(id_):return ‘#%d’ % id_resp = self.client.get(‘/test/42’)self.assertEqual(resp.data, ‘#42’) def test_home(self):resp = self.client.get(‘/‘)self.assertIn(‘Welcome’, resp.data)在上面的例子中，我为了测试给 App 新挂载了一个 View 函数。这是很常见的一个测试需求。但是如果 Flask App 实例是单例的，这种做法就会“弄脏”下一个测试的运行。更加麻烦的是，上述例子中如果 test_home 在 test_app 之前运行了，Flask 的开发者防御机制会认为这是一个“已经开始处理 Web 请求了，又挂载了视图” [8] 的失误，从而抛出 RuntimeError。 所以除非是应用简单到不需要 Web 层测试，否则还是尽量使用 App Factory 模式比较好。况且配合 Blueprint 的情况下，App Factory 还能帮助我们良好地组织应用结构： happytree/app.pyfrom flask import Flaskfrom werkzeug.utils import import_string extensions = [‘happytree.ext:db’,‘happytree.ext:login_manager’,] blueprints = [‘happytree.views:bp’,] def create_app():app = Flask(name) for ext_name in extensions:ext = import_string(ext_name)ext.init_app(app)for bp_name in blueprints:bp = import_string(bp_name)app.register_blueprint(bp)return app这样就能彻底摆脱 app.py 和 View 模块“互相 Import”的纠结了。 好吧其实这一节和 Context 没啥关系…… 拖延不好这篇文章动笔开始写是 6 月 21 日，到今天发布出来，已经过去了整整一个月。而事实上我开始列提纲准备写这篇文章已经是三四月份的事情了。 (:з」∠) [1] Flask 文档对 Application Context 和 Request Context 作出了详尽的解释；[2] 通过访问 flask.current_app；[3] 通过访问 flask.request；[4] Flask(Werkzeug) 的 Context 基于 Thread Local 和代理模式实现，只要身处 Context 中就能用近似访问全局变量的的方式访问到上下文信息，例如 flask.current_app 和 flask.request；Django 和 Tornado 则将上下文封装在对象中，只有明确获取了相关上下文对象才能访问其中的信息，例如在视图函数中或按照规定模板实现的 Middleware 中；[5] 基于 Flask 的 Web 应用可以在 Gevent 或 Eventlet 异步网络库 patch 过的 Python 环境中正常工作。这二者都使用 Greenlet 而不是系统线程作为调度单元，而 Werkzeug 考虑到了这点，在 Greenlet 可用时用 Greenlet ID 代替线程 ID。[6] Python 的对象方法是 Descriptior 实现的，所以方法就是一种属性；而 Python 的二元操作可以用双下划线开头和结尾的一系列协议，所以 foo + bar 等同于 foo.add(bar)，本质还是属性访问。[7] Flask-Script 是一个用来写 manage.py 管理脚本的 Flask 扩展，用它运行的任务会在开始前自动推入 App Context。将来这个“运行任务”的功能将被整合到 Flask 内部。[8] 详见 Flask 源码中的 setup_method 装饰器。]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python2中的ascii code can't encode]]></title>
    <url>%2F2018%2F07%2F08%2Fpython2%E4%B8%AD%E7%9A%84ascii%20code%20can't%20encode%2F</url>
    <content type="text"><![CDATA[python2中的ascii code can’t encode准备知识： 1 字符集，字符集实际上就是字符与数字之间的相互映射， 比如unicode字符集 ASCII字符集 GB2312字符集等等， 2 字符编码，仅仅有字符集是不够的，因为实际工作中我们需要将某些字保存在计算机上，计算机上保存的数据只能是0以及1的排列，还是拿字符集unicode来说，如果我们要将字符保存在计算机上那么就必须有某种规则将unicode字符转换为0以及1，同时这种规则还能让计算机区分出相邻字符的分隔位置，utf32就是一种规则，简单粗暴的用4个字节表示一个字符，以为unicode中最大的数远远小于2的32次方，所以这个规则保证了转码的可靠性，但是对于那些只使用英文的国家来说一个字符就要4字节有点浪费空间啊有木有！于是出现了utf8，可变长的且适用于unicode的一种编码方式，这种方式有效的降低了空间浪费。 用Python 2.x会经常碰到一个错误: UnicodeEncodeError: ‘ascii’ codec can’t encode character … : ordinal not in range(128) 搞清这个问题之前要先理解三个知识点: UTF-8 vs UnicodeEncoding vs DecodingPython 2.7里的 str 和 unicode UTF-8 vs Unicode这一点已经在[之前的博文]里解释过了(http://cheng.logdown.com/posts/2015/01/14/utf-8-vs-unicode:)，这里我来总结一下 Unicode 只是一个文字与数字之间的映射。比如，’汉’ 这个字在Unicode里的代码是 ‘6C49’，想对应的数字就是 27721。地球上每种语言里的每一个文字都有这样一个相对应的数字标识。这个文字与数字的映射表就是 Unicode。当我们把这个映射后的数字存储在计算机上时，需要把它转换成 0 和 1. 我们可以简单的把 27721 转换成二进制代码 ’01101100 01001001‘ 来存储。但问题是计算机怎么能够知道这个两个字节是代表一个文字而不是两个文字? 这个时候就需要再有一种编码形式来告诉计算机将这个字节视为一个文字。这个编码就是UTF-8 (当然，UTF-8只是众多编码中的一种)可以用这个顺序来理解： 屏幕上看到的文字 Unicode代码 根据UTF-8规范存在计算机内存或者硬盘里的模样 汉 -&gt; 6C49 -&gt; 11100110 10110001 10001001 Encoding vs Decoding在Python2中把一个Unicode类转化为 0 和 1 的过程叫做Encoding。 把 0 和 1 反转为Unicode类的过程叫做Decoding。Python3已经没有unicode和str的区别了，取而代之的是byte和str 在Python 2.7版本里，ASCII是默认的Encoding和Decoding的方法。 Python 2.7里的 str 和 unicode当你把一个字符(不管该字符是英文字母还是ASCII里不能包含的字符)赋值给一个变量时: han = ‘汉’这个变量的类型都会是str: In [113]:han = ‘汉’ type(han)Out[113]:str但这里有很重要的一点需要理解： In [124]:han = ‘汉’ bin_han = ‘\xe6\xb1\x89’ han == bin_han # 虽然在界面里我们看到的是’汉’这个字，但其实它是一堆字符，并不是我们看到的文字 Out[124]:Truestr这个类的本质其实就是原始字符数据（raw byte data）。它并不是我们所看到的’汉’！ 那么Unicode类也是这样吗？ In [114]:uni_han= u’汉’ type(uni_han)Out[114]:unicode In [131]:uni_han= u’汉’ u_han = u’\u6c49’ uni_han == u_han # 在Unicode中存储的是u’\u6c49’而不是你所看到的u’汉’ Out[131]:True理解了以上三个知识点，我们就可以很容易的解释 ‘ascii’ codec can’t encode character 这个错误的缘由了。 用示例来解释 ‘ascii’ codec can’t encode characterIn [117]:han = ‘汉’ print type(han) print len(han) str(han) Out[117]:&lt;type ‘str’&gt; 3 &lt;- ‘汉’的长度是3，明明是一个字，为什么长度是3? ‘\xe6\xb1\x89’ &lt;- 答案在这里当’汉’这个字被存储在内存中时，它会被转为三个字符’\xe6\xb1\x89’。所以len()给出的长度是3，而不是1. 那么为了让’汉’变成一个真正的字，我们就需要对它进行Decoding。（参看2. 把 0 和 1 转换为Unicode的过程叫Decoding） In [125]:str.decode(han)UnicodeDecodeError Traceback (most recent call last) in ()—-&gt; 1 str.encode(han) UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe6 in position 0: ordinal not in range(128)这里Python抛出了异常。因为默认的ASCII编码无法Decode这个文字。因为这个文字的数值已经超过了0 - 127这个范围。所以我们需要使用UTF-8编码来Decode： In [127]:str.decode(han, ‘utf8’)Out[127]:u’\u6c49’这里han这个变量被成功Decode为 u’\u6c49 In [142]:uni_han = u’\u6c49’ len(uni_han)Out[142]:1 &lt;- 长度变为了正确的1再来个示例作为结尾猜猜这段代码的输出是什么： uni_han = u’\u6c49’print ‘&#39;{0}&#39;的长度是{1}’.format(uni_han, len(uni_han))结果是: UnicodeEncodeError Traceback (most recent call last) in () 1 uni_han = u’\u6c49’—-&gt; 2 print ‘&#39;{0}&#39;的长度是{1}’.format(uni_han, len(uni_han)) UnicodeEncodeError: ‘ascii’ codec can’t encode character u’\u6c49’ in position 0: ordinal not in range(128)好伤心啊，本以为再也不会碰到这个问题了。那么问题出在哪呢？这部分代码’&#39;{0}&#39;的长度是{1}’是str，也就是原始的字符数据。我们想把一个Unicode （uni_han）混到它们里一起打印。这时，Python信心满满的用了默认的ASCII编码来Encode uni_han。结果可想而知，又是再次超出0 - 127的范围，无法Encode。这时，我们就需要告诉Python放弃ASCII吧，请使用UTF-8： In [145]:uni_han = u’\u6c49’ print ‘&#39;{0}&#39;的长度是{1}’.format(unicode.encode(uni_han,’utf8’), len(uni_han)) ‘汉’的长度是1另一种方法是让前半部分的str变为Unicode： In [150]:uni_han = u’\u6c49’ print u’&#39;{0}&#39;的长度是{1}’.format(uni_han, len(uni_han)) ‘汉’的长度是1总结在Python 2.x里str就是原始的010101， Unicode是Unicode，这两个东西不能混着用。当一个文字被写到硬盘上时，或者打印到屏幕上时，需要使用正确的Encoding编码。Python默认使用ASCII，但其实应该用UTF-8。这个问题以后还会经常碰到。关键是要理解ASCII，UTF8，Unicode, Encoding和Decoding的定义和关系。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入flask中的request]]></title>
    <url>%2F2018%2F07%2F08%2F%E6%B7%B1%E5%85%A5flask%E4%B8%AD%E7%9A%84request%2F</url>
    <content type="text"><![CDATA[深入flask中的request缘起 在使用flask的时候一直比较纳闷request是什么原理，他是如何保证多线程情况下对不同请求参数的隔离的。 准备知识 在讲request之前首先需要先理解一下werkzeug.local中的几个类，因为request就是基于这几个类来搞事情的。 -- coding: utf-8 --import copyfrom werkzeug._compat import PY2, implements_bool since each thread has its own greenlet we can just use those as identifiersfor the context. If greenlets are not available we fall back to thecurrent thread ident depending on where it is.try: from greenlet import getcurrent as get_identexcept ImportError: try: from thread import get_ident except ImportError: from _thread import get_ident def release_local(local): “””Releases the contents of the local for the current context. This makes it possible to use locals without a manager. Example:: &gt;&gt;&gt; loc = Local() &gt;&gt;&gt; loc.foo = 42 &gt;&gt;&gt; release_local(loc) &gt;&gt;&gt; hasattr(loc, &apos;foo&apos;) False With this function one can release :class:`Local` objects as well as :class:`LocalStack` objects. However it is not possible to release data held by proxies that way, one always has to retain a reference to the underlying local object in order to be able to release it. .. versionadded:: 0.6.1 &quot;&quot;&quot; local.__release_local__()class Local(object): &quot;&quot;&quot; 用一个大字典实现局部上下文 不同的线程或者greenlet调用该local对象时获取的值都是本线程或者greenlet独享的 实际上就是为每一个线程或者协程在字典里单独开辟出了一个空间(实际上就是一个键值对，键就是线程或者greenlet的唯一标识)， 这空间用来存储单个线程(或者greenlet)的私有变量 &quot;&quot;&quot; __slots__ = (&apos;__storage__&apos;, &apos;__ident_func__&apos;) def __init__(self): object.__setattr__(self, &apos;__storage__&apos;, {}) object.__setattr__(self, &apos;__ident_func__&apos;, get_ident) def __iter__(self): return iter(self.__storage__.items()) def __call__(self, proxy): &quot;&quot;&quot;Create a proxy for a name.&quot;&quot;&quot; return LocalProxy(self, proxy) def __release_local__(self): self.__storage__.pop(self.__ident_func__(), None) def __getattr__(self, name): try: return self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) def __setattr__(self, name, value): ident = self.__ident_func__() storage = self.__storage__ try: storage[ident][name] = value except KeyError: storage[ident] = {name: value} def __delattr__(self, name): try: del self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name)class LocalStack(object): &quot;&quot;&quot; LocalStack也是一个栈相关的局部上下文，底层实现是基于Local类。 注意一下他的pop方法，如果当前栈的长度为1，pop时会清空当前线程(greenlet)在底层的Local中所对应的&quot;键值对&quot;的 &quot;&quot;&quot; def __init__(self): self._local = Local() def __release_local__(self): self._local.__release_local__() def _get__ident_func__(self): return self._local.__ident_func__ def _set__ident_func__(self, value): object.__setattr__(self._local, &apos;__ident_func__&apos;, value) __ident_func__ = property(_get__ident_func__, _set__ident_func__) del _get__ident_func__, _set__ident_func__ def __call__(self): def _lookup(): rv = self.top if rv is None: raise RuntimeError(&apos;object unbound&apos;) return rv return LocalProxy(_lookup) def push(self, obj): &quot;&quot;&quot;Pushes a new item to the stack&quot;&quot;&quot; rv = getattr(self._local, &apos;stack&apos;, None) if rv is None: self._local.stack = rv = [] rv.append(obj) return rv def pop(self): &quot;&quot;&quot;Removes the topmost item from the stack, will return the old value or `None` if the stack was already empty. &quot;&quot;&quot; stack = getattr(self._local, &apos;stack&apos;, None) if stack is None: return None elif len(stack) == 1: release_local(self._local) return stack[-1] else: return stack.pop() @property def top(self): &quot;&quot;&quot;The topmost item on the stack. If the stack is empty, `None` is returned. &quot;&quot;&quot; try: return self._local.stack[-1] except (AttributeError, IndexError): return None@implements_boolclass LocalProxy(object): “”” 代理模式: 给目标对象提供一个代理对象，并由代理对象控制对目标对象的引用 “”” &quot;&quot;&quot;Acts as a proxy for a werkzeug local. Forwards all operations to a proxied object. The only operations not supported for forwarding are right handed operands and any kind of assignment. Example usage:: from werkzeug.local import Local l = Local() # these are proxies request = l(&apos;request&apos;) user = l(&apos;user&apos;) from werkzeug.local import LocalStack _response_local = LocalStack() # this is a proxy response = _response_local() Whenever something is bound to l.user / l.request the proxy objects will forward all operations. If no object is bound a :exc:`RuntimeError` will be raised. To create proxies to :class:`Local` or :class:`LocalStack` objects, call the object as shown above. If you want to have a proxy to an object looked up by a function, you can (as of Werkzeug 0.6.1) pass a function to the :class:`LocalProxy` constructor:: session = LocalProxy(lambda: get_current_request().session) .. versionchanged:: 0.6.1 The class can be instantiated with a callable as well now. &quot;&quot;&quot; __slots__ = (&apos;__local&apos;, &apos;__dict__&apos;, &apos;__name__&apos;, &apos;__wrapped__&apos;) def __init__(self, local, name=None): # __local 会被重命名为 _LocalProxy__local object.__setattr__(self, &apos;_LocalProxy__local&apos;, local) object.__setattr__(self, &apos;__name__&apos;, name) if callable(local) and not hasattr(local, &apos;__release_local__&apos;): # &quot;local&quot; is a callable that is not an instance of Local or # LocalManager: mark it as a wrapped function. object.__setattr__(self, &apos;__wrapped__&apos;, local) def _get_current_object(self): &quot;&quot;&quot;Return the current object. This is useful if you want the real object behind the proxy at a time for performance reasons or because you want to pass the object into a different context. &quot;&quot;&quot; if not hasattr(self.__local, &apos;__release_local__&apos;): return self.__local() try: return getattr(self.__local, self.__name__) except AttributeError: raise RuntimeError(&apos;no object bound to %s&apos; % self.__name__) @property def __dict__(self): try: return self._get_current_object().__dict__ except RuntimeError: raise AttributeError(&apos;__dict__&apos;) def __repr__(self): try: obj = self._get_current_object() except RuntimeError: return &apos;&lt;%s unbound&gt;&apos; % self.__class__.__name__ return repr(obj) def __bool__(self): try: return bool(self._get_current_object()) except RuntimeError: return False def __unicode__(self): try: return unicode(self._get_current_object()) # noqa except RuntimeError: return repr(self) def __dir__(self): try: return dir(self._get_current_object()) except RuntimeError: return [] def __getattr__(self, name): if name == &apos;__members__&apos;: return dir(self._get_current_object()) return getattr(self._get_current_object(), name) def __setitem__(self, key, value): self._get_current_object()[key] = value def __delitem__(self, key): del self._get_current_object()[key] if PY2: __getslice__ = lambda x, i, j: x._get_current_object()[i:j] def __setslice__(self, i, j, seq): self._get_current_object()[i:j] = seq def __delslice__(self, i, j): del self._get_current_object()[i:j] __setattr__ = lambda x, n, v: setattr(x._get_current_object(), n, v) __delattr__ = lambda x, n: delattr(x._get_current_object(), n) __str__ = lambda x: str(x._get_current_object()) __lt__ = lambda x, o: x._get_current_object() &lt; o __le__ = lambda x, o: x._get_current_object() &lt;= o __eq__ = lambda x, o: x._get_current_object() == o __ne__ = lambda x, o: x._get_current_object() != o __gt__ = lambda x, o: x._get_current_object() &gt; o __ge__ = lambda x, o: x._get_current_object() &gt;= o __cmp__ = lambda x, o: cmp(x._get_current_object(), o) # noqa __hash__ = lambda x: hash(x._get_current_object()) __call__ = lambda x, *a, **kw: x._get_current_object()(*a, **kw) __len__ = lambda x: len(x._get_current_object()) __getitem__ = lambda x, i: x._get_current_object()[i] __iter__ = lambda x: iter(x._get_current_object()) __contains__ = lambda x, i: i in x._get_current_object() __add__ = lambda x, o: x._get_current_object() + o __sub__ = lambda x, o: x._get_current_object() - o __mul__ = lambda x, o: x._get_current_object() * o __floordiv__ = lambda x, o: x._get_current_object() // o __mod__ = lambda x, o: x._get_current_object() % o __divmod__ = lambda x, o: x._get_current_object().__divmod__(o) __pow__ = lambda x, o: x._get_current_object() ** o __lshift__ = lambda x, o: x._get_current_object() &lt;&lt; o __rshift__ = lambda x, o: x._get_current_object() &gt;&gt; o __and__ = lambda x, o: x._get_current_object() &amp; o __xor__ = lambda x, o: x._get_current_object() ^ o __or__ = lambda x, o: x._get_current_object() | o __div__ = lambda x, o: x._get_current_object().__div__(o) __truediv__ = lambda x, o: x._get_current_object().__truediv__(o) __neg__ = lambda x: -(x._get_current_object()) __pos__ = lambda x: +(x._get_current_object()) __abs__ = lambda x: abs(x._get_current_object()) __invert__ = lambda x: ~(x._get_current_object()) __complex__ = lambda x: complex(x._get_current_object()) __int__ = lambda x: int(x._get_current_object()) __long__ = lambda x: long(x._get_current_object()) # noqa __float__ = lambda x: float(x._get_current_object()) __oct__ = lambda x: oct(x._get_current_object()) __hex__ = lambda x: hex(x._get_current_object()) __index__ = lambda x: x._get_current_object().__index__() __coerce__ = lambda x, o: x._get_current_object().__coerce__(x, o) __enter__ = lambda x: x._get_current_object().__enter__() __exit__ = lambda x, *a, **kw: x._get_current_object().__exit__(*a, **kw) __radd__ = lambda x, o: o + x._get_current_object() __rsub__ = lambda x, o: o - x._get_current_object() __rmul__ = lambda x, o: o * x._get_current_object() __rdiv__ = lambda x, o: o / x._get_current_object() if PY2: __rtruediv__ = lambda x, o: x._get_current_object().__rtruediv__(o) else: __rtruediv__ = __rdiv__ __rfloordiv__ = lambda x, o: o // x._get_current_object() __rmod__ = lambda x, o: o % x._get_current_object() __rdivmod__ = lambda x, o: x._get_current_object().__rdivmod__(o) __copy__ = lambda x: copy.copy(x._get_current_object()) __deepcopy__ = lambda x, memo: copy.deepcopy(x._get_current_object(), memo)先来讲Local对象1 创建一个Local对象 local = Local()刚创建后, 这个local_context中负责存储局部上下文变量的storage是一个空字典 local.storage = {}我们用iden1, inde2 …. indeN 来表示n个同属于一个进程的线程(或者greenlet), 假如当前的线程(或者greenlet)的id为iden1, 我们来操作一下local local.name = “iden1_name”实际执行的代码是: local.storage.setdefault(“iden1”, {})[“name”] = “iden1_name”这个local中负责存储局部上下文变量的storage就变成了这样: local.storage = { “iden1”: { “name”: “iden1_name” }} 当我们在不同的线程(或者greenlet)中操作后，local就可能会变成这样 local.storage = { “iden1”: {…}, “iden2”: {…}, … “idenN”: {…}} local对象有一个release_local方法, 执行该方法会清理掉当前线程(或者greenlet)对应的存储空间, 假如当前的线程(或者greenlet)的id为iden1,当我们执行完release_local方法后, local的存储空间就会变成这样: 已经没有iden1了local.storage = { “iden2”: {…}, … “idenN”: {…}} local还定义了call方法, 当我们执行local()后会返回一个LocalStack对象 LocalStack对象LocalStack底层使用的是Local，然后在Local实例中实现了一个栈 创建一个LocalStack对象 local_stack = LocalStack()该对象的local属性就是一个Local实例 isinstance(local_stack.local, Local) is Truelocal_stack的栈存储在他的local属性中, 当我们调用local_stack.push(some_obj)的时候, 实际上是执行了 local_stack.local.stack.append(some_obj) if hasattr(local_stack.local, “stack”) else local_stack.local.stack = [some_obj]假如当前的线程(或者greenlet)的id为iden1, 我们push一个对象request_ctx_obj, 然后又push一个对象request_ctx_obj2, 那么local_stack.local就会是这样: local_stack.local.storage = { “iden1”: { “stack”: [request_ctx_obj, request_ctx_obj2] }} 假如当前的线程(或者greenlet)的id为iden1，我们在调用local_stack.top()方法时，实际上执行的是: return local_stack.local.stack[-1]需要注意的是: 如果我们当前所处的线程(或者greenlet)中之前并没有进行过push的话，那么我们调用local_stack.top()方法返回的结果是None 当我们执行local_stack.pop()时, 实际上执行的是 local_stack.local.stack.pop()需要注意两点: 1 如果当前线程(或者greenlet)中之前没有push过, 那么pop()方法会返回None 2 如果当前线程(或者greenlet)中的栈中只有一个对象, 那么本次pop()还会清理掉stack(实际上执行了local_stack.local.release_local方法), 假如当前的线程(或者greenlet)的id为iden2的话，没有pop()之前是这样的: local_stack.local.storage = { “iden1”: { “stack”: [request_ctx_obj, request_ctx_obj2] } “iden2”: { “stack”: [request_ctx_obj3] }} 执行pop()则会将当前线程(或者greenlet)的局部上下文存储空间清理掉, 变为这样: local_stack.local.storage = { “iden1”: { “stack”: [request_ctx_obj, request_ctx_obj2] }} LocalStack也提供了call方法, 执行该方法会生成一个LocalProxy对象 LocalProxyLocalProxy实现了代理模式, 给目标对象提供一个代理对象，并由代理对象控制对目标对象的引用。根据传入参数的类型以及数量的不同他会有两种表现形式.第一种，第一个参数传入一个Local实例, 然后第二个参数传入想要代理的对象的名称: 我们执行下面的语句: local_1 = Local()local_proxy1 = LocalProxy(local_1, “age”) local_proxy1所代理的实际上就是local_1实例中的age属性了。 假如当前的线程(或者greenlet)的id为iden1，那么我们执行local_proxy1 = 12, 实际执行的就是local_1.age = 12 第二种，只传入一个函数，通过该函数可以获取到想要代理的对象: 我们执行下面的语句: local_2 = Local() def _find_raw_obj(): return local_2.name local_proxy2 = LocalProxy(_find_raw_obj) local_proxy2所代理的实际上就是local_2实例中的name属性 flask源码剖析request源码 def _lookup_req_object(name): top = _request_ctx_stack.top if top is None: raise RuntimeError(_request_ctx_err_msg) return getattr(top, name) context locals_request_ctx_stack = LocalStack()request = LocalProxy(partial(_lookup_req_object, ‘request’)) 只要看懂了文章上半部分的local，这里实际上很简单， _request_ctx_stack是一个LocalStack实例，而我们每次调用request.some_attr 的时候实际上是执行_request_ctx_stack.top.some_attr 再来看一下当请求过来的时候，flask是如何处理的： flask.appclass Flask(_PackageBoundObject): request_class = Request def request_context(self, environ): return RequestContext(self, environ) def wsgi_app(self, environ, start_response): # self.request_context是一个RequestContext实例 ctx = self.request_context(environ) error = None try: try: # 2 执行了request_ctx_stack.push(ctx) ctx.push() # 3 处理请求得到响应 response = self.full_dispatch_request() except Exception as e: error = e response = self.handle_exception(e) except: error = sys.exc_info()[1] raise return response(environ, start_response) finally: if self.should_ignore_error(error): error = None # 4 request_ctx_stack.pop(ctx) ctx.auto_pop(error) # 1 当请求来的时候会执行app.__call__()方法 def __call__(self, environ, start_response): &quot;&quot;&quot;The WSGI server calls the Flask application object as the WSGI application. This calls :meth:`wsgi_app` which can be wrapped to applying middleware.&quot;&quot;&quot; return self.wsgi_app(environ, start_response)flask.ctxclass RequestContext(object): def __init__(self, app, environ, request=None): self.app = app if request is None: # request是一个Request对象 request = app.request_class(environ) self.request = request self._implicit_app_ctx_stack = [] def push(self): top = _request_ctx_stack.top if top is not None and top.preserved: top.pop(top._preserved_exc) # Before we push the request context we have to ensure that there # is an application context. app_ctx = _app_ctx_stack.top if app_ctx is None or app_ctx.app != self.app: app_ctx = self.app.app_context() app_ctx.push() self._implicit_app_ctx_stack.append(app_ctx) else: self._implicit_app_ctx_stack.append(None) if hasattr(sys, &apos;exc_clear&apos;): sys.exc_clear() # 2.1 这里是重点 _request_ctx_stack.push(self) def pop(self, exc=_sentinel): app_ctx = self._implicit_app_ctx_stack.pop() try:if not self._implicit_app_ctx_stack: self.preserved = False self._preserved_exc = None if exc is _sentinel: exc = sys.exc_info()[1] self.app.do_teardown_request(exc) request_close = getattr(self.request, ‘close’, None) if request_close is not None: request_close()finally: rv = _request_ctx_stack.pop()# Get rid of the app as well if necessary. if app_ctx is not None: app_ctx.pop(exc) assert rv is self, &apos;Popped wrong request context. &apos; \ &apos;(%r instead of %r)&apos; % (rv, self) 当请求过来时： 1 将请求封装为一个RequestContext实例 2 然后将请求的environ封装成Request对象 3 执行_request_ctx_stack.push(RequestContext实例) 4 处理请求得到响应 5 执行_request_ctx_stack.pop() 6 返回结果 看到这里，大体原理我们也就懂了。 来点深入的高级用法需求工作中使用到flask flask-restful，有这样的场景： 1 首先是遵循restful 2 我希望所有接口有统一的参数传递格式，类似于这样: timestamp: int # 以秒为单位 token: str # 这个就不用说了 data: str # base64.encode(json.dumps(原始参数数据)) signature: md5(data + string(timestamp) + key) # 签名,注：key为加密密钥假如是登陆接口，客户端需要给我传递手机号以及验证码，我希望格式是json，所以原始参数数据大概是这样: {“mobile”: “12377776666”, “code”: “1234”} 3 我希望所有接口有统一的响应格式，类似于这样： { “code”: 0, # 这成功。 1 -9999 错误 “message”: “”, “data”: { “mobile”: “sdfasdf”, “code”: “” }} 4 我希望请求参数数据经过统一的参数检测之后, request的args(如果请求方式为get) 或者form(如果请求方式为post) 或者values属性变为原始参数数据.这样就可以正常使用RequestParser() 实现以下是本人的实现源码(生产环境测试无问题，但有几处需要改进的地方): 统一参数解析处理 参考： https://blog.tonyseek.com/post/the-context-mechanism-of-flask/ flask源码 flask-restful源码]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jwt安全认证]]></title>
    <url>%2F2018%2F06%2F28%2Fjwt%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[安装：pip install djangorestframework-jwt 1.settings中配置INSTALLED_APPS = [ 1&apos;rest_framework.authtoken&apos;, ]2、配置jwt验证REST_FRAMEWORK = { #身份认证 &apos;DEFAULT_AUTHENTICATION_CLASSES&apos;:( &apos;rest_framework_jwt.authentication.JSONWebTokenAuthentication&apos;, &apos;rest_framework.authentication.SessionAuthentication&apos;, &apos;rest_framework.authentication.BasicAuthentication&apos;, ), } import datetime JWT_AUTH = { &apos;JWT_AUTH_HEADER_PREFIX&apos;: &apos;JWT&apos;, &apos;JWT_EXPIRATION_DELTA&apos;: datetime.timedelta(days=1), } AUTH_USER_MODEL=&apos;users.User&apos; #指定使用users APP中的 model User进行验证2.models.py添加认证User表from django.db import models from django.contrib.auth.models import AbstractUser class User(AbstractUser): username = models.CharField(max_length=64, unique=True) password = models.CharField(max_length=64) phone = models.CharField(max_length=64) token = models.CharField(max_length=255)3.serializers.py create创建tokenfrom rest_framework_jwt.settings import api_settings from rest_framework import serializers from users.models import User class UserSerializer(serializers.Serializer): username = serializers.CharField() password = serializers.CharField() phone = serializers.CharField() token = serializers.CharField(read_only=True) def create(self, data): user = User.objects.create(**data) user.set_password(data.get(&apos;password&apos;)) user.save() #补充生成记录登录状态的token jwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLER jwt_encode_handler = api_settings.JWT_ENCODE_HANDLER payload = jwt_payload_handler(user) token = jwt_encode_handler(payload) user.token = token return user4.urls.pyfrom django.contrib import admin from django.urls import path,re_path,include from users import views from rest_framework_jwt.views import obtain_jwt_token #验证密码后返回token urlpatterns = [ path(&apos;admin/&apos;, admin.site.urls), path(&apos;register/&apos;, views.RegisterView.as_view(), name=&apos;register&apos;), #注册用户 path(&apos;login/&apos;, obtain_jwt_token,name=&apos;login&apos;), 用户登录后返回token path(&apos;user/list/&apos;, views.UserList.as_view(), name=&apos;register&apos;), #测试需要携带token才能访问 ] urls.py5.viws.pyimport json from rest_framework.views import APIView from rest_framework.views import Response from rest_framework.permissions import IsAuthenticated from rest_framework_jwt.authentication import JSONWebTokenAuthentication from users.serializers import UserSerializer #用户注册 class RegisterView(APIView): def post(self, request, *args, **kwargs): serializer = UserSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=201) return Response(serializer.error, status=400) #测试必须携带token才能访问接口 class UserList(APIView): permission_classes = [IsAuthenticated] # 接口中加权限 authentication_classes = [JSONWebTokenAuthentication] def get(self,request, *args, **kwargs): print(request.META.get(&apos;HTTP_AUTHORIZATION&apos;, None)) return Response({&apos;name&apos;:&apos;zhangsan&apos;}) def post(self,request, *args, **kwargs): return Response({&apos;name&apos;:&apos;zhangsan&apos;}) users/views.py]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>rest_framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Django2.0跨域问题]]></title>
    <url>%2F2018%2F06%2F19%2F%E5%85%B3%E4%BA%8EDjango2.0%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[关于Django2.0跨域问题首页 - Python /2019-03-05 首先，明确一个问题，就是跨域需要服务端支持才行，而不是靠前端修改代码可以解决的，无论你用jquery axios fetch都是如此。 这里介绍一下，如果使用的是Django2.0本身作为web服务端，该如何处理跨域请求问题。 1 安装插件 pip install django-cors-headers 2 编辑配置文件(settings.py) INSTALLED_APPS = [ ‘’’’’’’’’’’’ ‘’’’’’’’’’’’ ‘’’’’’’’’’’’ ‘’’’’’’’’’’’ ‘’’’’’’’’’’’ ‘corsheaders’,] MIDDLEWARE = [ ‘django.middleware.security.SecurityMiddleware’, ‘django.contrib.sessions.middleware.SessionMiddleware’, &apos;corsheaders.middleware.CorsMiddleware&apos;, #这里是新增的中间件 &apos;django.middleware.common.CommonMiddleware&apos;, &apos;django.middleware.csrf.CsrfViewMiddleware&apos;, &apos;django.contrib.auth.middleware.AuthenticationMiddleware&apos;, &apos;django.contrib.messages.middleware.MessageMiddleware&apos;, &apos;django.middleware.clickjacking.XFrameOptionsMiddleware&apos;,] CORS_ALLOW_CREDENTIALS = TrueCORS_ORIGIN_ALLOW_ALL = True 至此，就可以解决跨域异步请求的问题了，服务端的不同配置也不一样，比如使用nginx服务就需要单独配置nginx才能让服务端支持，所以不能一概而论。]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[get请求中的url encode问题]]></title>
    <url>%2F2018%2F06%2F18%2Fget%E8%AF%B7%E6%B1%82%E4%B8%AD%E7%9A%84url%20encode%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[get请求中的url encode问题首先发表一下感慨，Python的requests模块确实太简便，省却了很多的转码等等等等的问题，但这也是缺点，对于我这种基础不好的同学来说让我少知道了许多本来应该知道的东西。 url encode: 对于get方法来说，都是把数据串联在请求的url后面作为参数，如：http://localhost:8080/servlet?msg=abc （很常见的一个乱码问题就要出现了，如果url中出现中文或其它特殊字符的话，如：http://localhost:8080 /servlet?msg=杭州，服务器端容易得到乱码），url拼接完成后，浏览器会对url进行URL encode，然后发送给服务器，URL encode的过程就是把部分url做为字符，按照某种编码方式（如：utf-8,gbk等）编码成二进制的字节码，然后每个字节用一个包含3个字符的字符串 “%xy” 表示，其中xy为该字节的两位十六进制表示形式。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入tornado中的Configurable]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5tornado%E4%B8%AD%E7%9A%84Configurable%2F</url>
    <content type="text"><![CDATA[深入tornado中的ConfigurableConfigurable十分重要！ 位于tornado.util文件中,它是一个工厂类。 我们暂且称这个类为 配置类 。 我们暂且约定：该类的子类称之为 直属配置子类 ， 该类的孙类、重孙类……称之为 从属配置子类。就像这样： class Configurable(object): # 配置类 pass class IOLoop(Configurable): # 直属配置子类 pass class PollIOLoop(IOLoop): # 从属配置子类 pass 这个配置类类似于java中的接口，他要求 直属配置子类必须有三个方法，分别是： configurable_base configurable_default ‘initialize’另外，在Configurable中定义了 new方法： class Configurable(object): # 配置类 ‘’’ 要求其直属配置子类必须有以下三个方法： configurable_base 一般返回该直属配置子类自身 configurable_default 返回该直属配置子类的执行类对象 ‘initialize’ 该直属配置子类的初始化方法 ‘’’ def new(cls, args, **kwargs) ‘’’ 解析出impl对象 1 cls是直属配置子类时，impl就是该直属配置子类的’执行类对象’ 2 cls是从属配置子类时，impl就是该从属配置子类自身 然后实例化一个impl实例对象 运行其initialize方法，并传入合并后的参数 返回该impl实例对象 ‘’’ base = cls.configurable_base() init_kwargs = {} if cls is base: impl = cls.configured_class() if base.impl_kwargs: init_kwargs.update(base.impl_kwargs) else: impl = cls init_kwargs.update(kwargs) instance = super(Configurable, cls).new(impl) instance.initialize(args, **init_kwargs) return instance 不太清楚new方法作用的同学请移步：Python中的init和new Configurable之所以这么重要，是因为他重写了new方法。 他的子类可以直接不用定义init方法，但必须使用initialize方法来替代init 另外，当实例化Configurable子类对象时，所产生的实例对象对应的类并不一定会是该子类，有可能是其他类。。。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入tornado中的IOStream]]></title>
    <url>%2F2018%2F05%2F19%2F%E6%B7%B1%E5%85%A5tornado%E4%B8%AD%E7%9A%84IOStream%2F</url>
    <content type="text"><![CDATA[深入tornado中的IOStreamIOStream对tornado的高效起了很大的作用，他封装了socket的非阻塞IO的读写操作。大体上可以这么说，当连接建立后，服务端与客户端的请求响应的读写都是基于IOStream的，也就是说：IOStream是用来处理对连接的读写，当然IOStream是异步的读写而且可以有很多花样的读写。 接下来说一下有关接收请求的大体流程： 当连接建立，服务器端会产生一个对应该连接的socket，同时将该socket封装至IOStream实例中(这代表着IOStream的初始化)。 我们知道tornado是基于IO多路复用的(就拿epoll来说)，此时将socket进行register，事件为READABLE，这一步与IOStream没有多大关系。 当该socket事件发生时，也就是意味着有数据从连接发送到了系统缓冲区中，这时就需要将chunk读入到我们在内存中为其开辟的_read_buffer中，在IOStream中使用deque作为buffer。_read_buffer表示读缓冲，当然也有_write_buffer，并且在读的过程中也会检测总尺寸是否大于我们设定的最大缓冲尺寸。不管是读缓冲还是写缓冲本质上就是tornado进程开辟的一段用来存储数据的内存。 而这些chunk一般都是客户端发送的请求了，但是我们还需要对这些chunk作进一步操作，比如这个chunk中可能包含了多个请求，如何把请求分离？(每个请求首部的结束符是b’\r\n\r\n’)，这里就用到read_until来分离请求并设置callback了。同时会将被分离的请求数据从_read_buffer中移除。 然后就是将callback以及他的参数(被分离的请求数据)添加至IOLoop._callbacks中，等待下一次IOLoop的执行，届时会迭代_callbacks并执行回调函数。 补充： tornado是水平触发，所以假如读完一次chunk后系统缓存区中依然还有数据，那么下一次的epoll.poll()依然会返回该socket。 在iostream中有一个类叫做：IOStream 有几个较为重要的属性： def init(): self.socket = socket # 封装socket self.socket.setblocking(False) # 设置socket为非阻塞 self.io_loop = io_loop or ioloop.IOLoop.current() self._read_buffer = deque() # 读缓冲 self._write_buffer = deque() # 写缓冲 self._read_callback = None # 读到指定字节数据时，或是指定标志字符串时，需要执行的回调函数 self._write_callback = None # 发送完_write_buffer的数据时，需要执行的回调函数 有几个较为重要的方法 class IOStream(object): def read_until(self, delimiter, callback): def read_bytes(self, num_bytes, callback, streaming_callback=None): def read_until_regex(self, regex, callback): def read_until_close(self, callback, streaming_callback=None): def write(self, data, callback=None):以上所有的方法都需要一个可选的callback参数，如果该参数为None则该方法会返回一个Future对象。 以上所有的读方法本质上都是读取该socket所发送来的数据，然后当读到指定分隔符或者标记或者条件触发的时候，停止读，然后将该分隔符以及其前面的数据作为callback(如果没有callback，则将数据设置为Future对象的result)的参数，然后将callback添加至IOLoop._callbacks中。当然其中所有的”读”操作是非阻塞的！ 像read_until read_until_regex 这两个方法相差不大，原理都是差不多的，都是在buffer中找指定的字符或者字符样式。 而read_bytes则是设置读取字节数，达到这些字节就会触发并运行回调函数(当然这些回调函数不是立刻运行，而是被送到ioloop中的_callbacks中)，该方法主要是用来读取包含content-length或者分块传输编码的具有主体信息的请求或者响应。 而read_until_close则是主要被用在非持久连接上，因为非持久连接响应的结束标志就是连接关闭。 read_bytes和read_until_close这两个方法都有streaming_callback这个参数，假如指定了该参数，那么只要read_buffer中有数据，则将数据作为参数调用该函数 就拿比较常见的read_until方法来说，下面是代码简化版： def read_until(self, delimiter, callback=None, max_bytes=None): future = self._set_read_callback(callback) # 可能是Future对象，也可能是None self._read_delimiter = delimiter # 设置分隔符 self._read_max_bytes = max_bytes # 设置最大读字节数 self._try_inline_read() return future其中_set_read_callback会根据callback是否存在返回None或者Future对象(存在返回None，否则返回一个Future实例对象) 如果我们再来看_try_inline_read方法的简化版： def _try_inline_read(self): “”” 尝试从_read_buffer中读取所需数据 “”” # 查看是否我们已经在之前的读操作中得到了数据 self._run_streaming_callback() # 检查字符流回调，如果调用read_bytes和read_until_close并指定了streaming_callback参数就会造成这个回调 pos = self._find_read_pos() # 尝试在_read_buffer中找到分隔符的位置。找到则返回分隔符末尾所处的位置，如果不能，则返回None。 if pos is not None: self._read_from_buffer(pos) return self._check_closed() # 检查当前IOStream是否关闭 pos = self._read_to_buffer_loop() # 从系统缓冲中读取一个chunk，检查是否含有分隔符，没有则继续读取一个chunk，合并两个chunk，再次检查是否函数分隔符…… 如果找到了分隔符，会返回分隔符末尾在_read_buffer中所处的位置 if pos is not None: # 如果找到了分隔符， self._read_from_buffer(pos) # 将所需的数据从_read_buffer中移除，并将其作为callback的参数，然后将callback封装后添加至IOLoop._callbacks中 return # 没找到分隔符，要么关闭IOStream，要么为该socket在IOLoop中注册事件 if self.closed(): self._maybe_run_close_callback() else: self._add_io_state(ioloop.IOLoop.READ)上面的代码被我用空行分为了三部分，每一部分顺序的对应下面每一句话 分析该方法： 1 首先在_read_buffer第一项中找分隔符，找到了就将分隔符以及其前的数据从_read_buffer中移除并将其作为参数传入回调函数，没找到就将第二项与第一项合并然后继续找……； 2 如果在_read_buffer所有项中都没找到的话就把系统缓存中的数据读取至_read_buffer，然后合并再次查找， 3 如果把系统缓存中的数据都取完了都还没找到，那么就等待下一次该socket发生READ事件后再找，这时的找则就是：将系统缓存中的数据读取到_read_buffer中然后找，也就是执行第2步。 来看一看这三部分分别调用了什么方法：第一部分中的_find_read_pos以及_read_from_buffer 前者主要是在_read_buffer中查找分隔符，并返回分隔符的位置，后者则是将分隔符以及分隔符前面的所有数据从_read_buffer中取出并将其作为callback的参数，然后将callback封装后添加至IOLoop._callbacks中 来看_find_read_pos方法的简化版： def _find_read_pos(self): # 尝试在_read_buffer中寻找分隔符。找到则返回分隔符末尾所处的位置，如果不能，则返回None。 if self._read_delimiter is not None: if self._read_buffer: # 查看_read_buffer中是否有之前未处理的数据 while True: loc = self._read_buffer[0].find(self._read_delimiter) # 查找分隔符所出现的首部位置 if loc != -1: # 在_read_buffer的首项中找到了 delimiter_len = len(self._read_delimiter) self._check_max_bytes(self._read_delimiter, loc + delimiter_len) return loc + delimiter_len # 分隔符末尾的位置 if len(self._read_buffer) == 1: break _double_prefix(self._read_buffer) self._check_max_bytes(self._read_delimiter, len(self._read_buffer[0])) return None def _read_from_buffer(self, pos): # 将所需的数据从_read_buffer中移除，并将其作为callback的参数，然后将callback封装后添加至IOLoop._callbacks中 self._read_bytes = self._read_delimiter = self._read_regex = None self._read_partial = False self._run_read_callback(pos, False) 来看_run_read_callback源码简化版： def _run_read_callback(self, size, streaming): if streaming: callback = self._streaming_callback else: callback = self._read_callback self._read_callback = self._streaming_callback = None if self._read_future is not None: # 这里将_read_future进行set_result assert callback is None future = self._read_future self._read_future = None future.set_result(self._consume(size)) if callback is not None: assert (self._read_future is None) or streaming self._run_callback(callback, self._consume(size)) # 将后者作为前者的参数，然后将前者进行封装后添加至IOLoop._callbacks中 来看_consume的源码： def _consume(self, loc): # 将self._read_buffer 的首项改为 原首项[loc:] ，然后返回 原首项[:loc] if loc == 0: return b”” _merge_prefix(self._read_buffer, loc) # 将双端队列（deque）的首项调整为指定大小。 self._read_buffer_size -= loc return self._read_buffer.popleft() 来看_run_callback源码简化版： def _run_callback(self, callback, args):# 将callback封装后添加至ioloop._callbacks中 def wrapper(): self._pending_callbacks -= 1 try: return callback(args) finally: self._maybe_add_error_listener() with stack_context.NullContext(): self._pending_callbacks += 1 self.io_loop.add_callback(wrapper) # 将callback添加至IOLoop._callbacks中 这里面还用到一个很有意思的函数：_merge_prefix ，这个函数的作用就是将deque的首项调整为指定大小 def _merge_prefix(deque, size): “””Replace the first entries in a deque of strings with a single string of up to size bytes. &gt;&gt;&gt; d = collections.deque([&apos;abc&apos;, &apos;de&apos;, &apos;fghi&apos;, &apos;j&apos;]) &gt;&gt;&gt; _merge_prefix(d, 5); print(d) deque([&apos;abcde&apos;, &apos;fghi&apos;, &apos;j&apos;]) Strings will be split as necessary to reach the desired size. &gt;&gt;&gt; _merge_prefix(d, 7); print(d) deque([&apos;abcdefg&apos;, &apos;hi&apos;, &apos;j&apos;]) &gt;&gt;&gt; _merge_prefix(d, 3); print(d) deque([&apos;abc&apos;, &apos;defg&apos;, &apos;hi&apos;, &apos;j&apos;]) &gt;&gt;&gt; _merge_prefix(d, 100); print(d) deque([&apos;abcdefghij&apos;]) &quot;&quot;&quot; if len(deque) == 1 and len(deque[0]) &lt;= size: return prefix = [] remaining = size while deque and remaining &gt; 0: chunk = deque.popleft() if len(chunk) &gt; remaining: deque.appendleft(chunk[remaining:]) chunk = chunk[:remaining] prefix.append(chunk) remaining -= len(chunk) if prefix: deque.appendleft(type(prefix[0])().join(prefix)) if not deque: deque.appendleft(b&quot;&quot;)第二部分的_read_to_buffer_loop 来看_read_to_buffer_loop简化版： 系统缓冲中的data可能十分长，为了查找指定的字符，我们应该先读一个chunk，检查其中是否有指定的字符，若有则返回分隔符末尾所处的位置 若没有则继续读第二个chunk，然后将这两个chunk合并(多字节分隔符（例如“\ r \ n”）可能跨读取缓冲区中的两个块)，重复查找过程 def _read_to_buffer_loop(self): try: next_find_pos = 0 self._pending_callbacks += 1 while not self.closed(): if self._read_to_buffer() == 0: # 从系统缓冲中读一个chunk并将其添加至_read_buffer中，然后返回chunk的大小，如果无数据则返回0 break self._run_streaming_callback() if self._read_buffer_size &gt;= next_find_pos: # _read_buffer_size 表示_read_buffer的大小 pos = self._find_read_pos() # 尝试在_read_buffer中寻找分隔符。找到则返回分隔符末尾所处的位置，如果不能，则返回None。 if pos is not None: return pos next_find_pos = self._read_buffer_size * 2 return self._find_read_pos() finally: self._pending_callbacks -= 1第三部分_add_io_state，该函数和ioloop异步相关 def _add_io_state(self, state): if self.closed(): # 连接已经关闭 return if self._state is None: self._state = ioloop.IOLoop.ERROR | state with stack_context.NullContext(): self.io_loop.add_handler(self.fileno(), self._handle_events, self._state) # 为对应socket的文件描述符添加事件及其处理函数, elif not self._state &amp; state: self._state = self._state | state self.io_loop.update_handler(self.fileno(), self._state) # self._handle_events 是根据events选择对应的处理函数，在这里我们假设处理函数是_handle_read def _handle_read(self): try: pos = self._read_to_buffer_loop() except UnsatisfiableReadError: raise except Exception as e: gen_log.warning(&quot;error on read: %s&quot; % e) self.close(exc_info=True) return if pos is not None: self._read_from_buffer(pos) return else: self._maybe_run_close_callback()参考： http://www.nowamagic.net/academy/detail/13321051]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入tornado中的协程]]></title>
    <url>%2F2018%2F05%2F16%2F%E6%B7%B1%E5%85%A5tornado%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[深入tornado中的协程tornado使用了单进程(当然也可以多进程) + 协程 + I/O多路复用的机制，解决了C10K中因为过多的线程(进程)的上下文切换 而导致的cpu资源的浪费。 tornado中的I/O多路复用前面已经讲过了。本文不做详细解释。 来看一下tornado中的协程模块：tornado.gen： tornado.gen是根据生成器(generator)实现的，用来更加简单的实现异步。 先来说一下tornado.gen.coroutine的实现思路： 我们知道generator中的yield语句可以使函数暂停执行，而send()方法则可以恢复函数的执行。 tornado将那些异步操作放置到yield语句后，当这些异步操作完成后，tornado会将结果send()至generator中恢复函数执行。 在tornado的官方文档中有这么一句话： Most asynchronous functions in Tornado return a Future; yielding this object returns its result. 就是说： 在tornado中大多数的异步操作返回一个Future对象 yield Future对象 会返回该异步操作的结果，这句话的意思就是说 假如 ret = yield some_future_obj 当some_future_obj所对应的异步操作完成后会自动的将该异步操作的结果赋值给 ret 那么，Future对象到底是什么？ 一 Future对象先来说说Future对象： Future对象可以概括为： 一个异步操作的占位符，当然这个占位符有些特殊，它特殊在： 1 这个占位符是一个对象 2 这个对象包含了很多属性，包括_result 以及 _callbacks，分别用来存储异步操作的结果以及回调函数 3 这个对象包含了很多方法，比如添加回调函数，设置异步操作结果等。 4 当这个对象对应的异步操作完成后，该对象会被set_done，然后遍历并运行_callbacks中的回调函数 来看一下Future的简化版： class Future(object): ‘’’ Future对象主要保存一个回调函数列表callbacks与一个执行结果result，当我们set_result时，就会执行_callbacks中的函数 如果set_result或者set_done，就会遍历_callbacks列表并执行callback(self)函数 ‘’’ def __init(self): self._result = None # 执行的结果 self._callbacks = [] # 用来保存该future对象的回调函数 def result(self, timeout=None): # 如果操作成功，返回结果。如果失败则抛出异常 self._clear_tb_log() if self._result is not None: return self._result if self._exc_info is not None: raise_exc_info(self._exc_info) self._check_done() return self._result def add_done_callback(self, fn): if self._done: fn(self) else: self._callbacks.append(fn) def set_result(self, result): self._result = result self._set_done() def _set_done(self): # 执行结束(成功)后的操作。 self._done = True for cb in self._callbacks: try: cb(self) except Exception: app_log.exception(&apos;Exception in callback %r for %r&apos;, cb, self) self._callbacks = None完整源码： Future源码 二 gen.coroutine装饰器tornado中的协程是通过tornado.gen中的coroutine装饰器实现的： def coroutine(func, replace_callback=True): return _make_coroutine_wrapper(func, replace_callback=True)_make_coroutine_wrapper ： def _make_coroutine_wrapper(func, replace_callback): @functools.wraps(func) def wrapper(args, **kwargs): ‘’’ 大体过程： future = TracebackFuture() result = func(args, **kwargs) if isinstance(result, GeneratorType): yielded = next(result) Runner(result, future, yielded) return future ‘’’ future = TracebackFuture() # TracebackFuture = Future if replace_callback and &apos;callback&apos; in kwargs: callback = kwargs.pop(&apos;callback&apos;) IOLoop.current().add_future(future, lambda future: callback(future.result())) try: result = func(*args, **kwargs) # 执行func，若func中包含yield，则返回一个generator对象 except (Return, StopIteration) as e: result = _value_from_stopiteration(e) except Exception: future.set_exc_info(sys.exc_info()) return future else: if isinstance(result, GeneratorType): # 判断其是否为generator对象 try: orig_stack_contexts = stack_context._state.contexts yielded = next(result) # 第一次执行 if stack_context._state.contexts is not orig_stack_contexts: yielded = TracebackFuture() yielded.set_exception( stack_context.StackContextInconsistentError( &apos;stack_context inconsistency (probably caused &apos; &apos;by yield within a &quot;with StackContext&quot; block)&apos;)) except (StopIteration, Return) as e: future.set_result(_value_from_stopiteration(e)) except Exception: future.set_exc_info(sys.exc_info()) else: Runner(result, future, yielded) # Runner(result, future, yield) try: return future finally: future = None future.set_result(result) return future return wrapper先来看一下大体过程： 1 首先生成一个Future对象 2 运行该被装饰函数并将结果赋值给result。 在这里因为tornado的’异步’实现是基于generator的，所以一般情况下 result是一个generator对象 3 yielded = next(result) 执行到被装饰函数的第一次yield，将结果赋值给yielded。一般情况下，yielded很大情况下是一个Future对象。 4 Runner(result, future, yielded) 5 return future 除了第4步以外其他都很好理解，所以来了解一下第四步Runner()干了些啥： 三 Runner()类1 为什么要有Runner()？或者说Runner()的作用是什么？ Runner()可以自动的将异步操作的结果send()至生成器中止的地方 tornado的协程或者说异步是基于generator实现的，generator较为常用的有两个方法：send() next() ，关于这两个方法的流程分析在这。 很多情况下会有generator的嵌套。比如说经常会yield 一个generator。当A生成器yield B生成器时，分两步： 1 我们首先中止A的执行转而执行B 2 当B执行完成后，我们需要将B的结果send()至A中止的地方，继续执行A Runner()主要就是来做这些的，也就是控制生成器的执行与中止，并在合适的情况下使用send()方法同时传入B生成器的结果唤醒A生成器。 来看一个简单例子： def run(): print(‘start running’) yield 2 # 跑步用时2小时 def eat(): print(‘start eating’) yield 1 # 吃饭用时1小时 def time(): run_time = yield run() eat_time = yield eat() print(run_time+eat_time) def Runner(gen): r = next(gen) return r t = time()try: action = t.send(Runner(next(t))) t.send(Runner(action))except StopIteration: pass 上例中的Runner()仅仅完成了第一步，我们还需要手动的执行第二步，而tornado的gen的Runner()则做了全套奥！ 2 剖析Runner() 在Runner()中主要有三个方法init handle_yield run： class Runner(object): def init(self, gen, result_future, first_yielded): self.gen = gen # 一个generator对象 self.result_future = result_future # 一个Future对象 self.future = _null_future # 一个刚初始化的Future对象 _null_future = Future(); _null_future.set_result(None) self.yield_point = None self.pending_callbacks = None self.results = None self.running = False self.finished = False self.had_exception = False self.io_loop = IOLoop.current() self.stack_context_deactivate = None if self.handle_yield(first_yielded): self.run() ………… 部分方法省略 def run(self): &quot;&quot;&quot;Starts or resumes the generator, running until it reaches a yield point that is not ready. &quot;&quot;&quot; if self.running or self.finished: return try: self.running = True while True: future = self.future if not future.done(): return self.future = None try: orig_stack_contexts = stack_context._state.contexts exc_info = None try: value = future.result() except Exception: self.had_exception = True exc_info = sys.exc_info() if exc_info is not None: yielded = self.gen.throw(*exc_info) exc_info = None else: yielded = self.gen.send(value) if stack_context._state.contexts is not orig_stack_contexts: self.gen.throw( stack_context.StackContextInconsistentError( &apos;stack_context inconsistency (probably caused &apos; &apos;by yield within a &quot;with StackContext&quot; block)&apos;)) except (StopIteration, Return) as e: self.finished = True self.future = _null_future if self.pending_callbacks and not self.had_exception: # If we ran cleanly without waiting on all callbacks # raise an error (really more of a warning). If we # had an exception then some callbacks may have been # orphaned, so skip the check in that case. raise LeakedCallbackError( &quot;finished without waiting for callbacks %r&quot; % self.pending_callbacks) self.result_future.set_result(_value_from_stopiteration(e)) self.result_future = None self._deactivate_stack_context() return except Exception: self.finished = True self.future = _null_future self.result_future.set_exc_info(sys.exc_info()) self.result_future = None self._deactivate_stack_context() return if not self.handle_yield(yielded): return finally: self.running = False def handle_yield(self, yielded): if _contains_yieldpoint(yielded): # 检查其中是否包含YieldPoint yielded = multi(yielded) if isinstance(yielded, YieldPoint): # Base class for objects that may be yielded from the generator self.future = TracebackFuture() # 一个刚刚初始化的Future对象 def start_yield_point(): try: yielded.start(self) if yielded.is_ready(): self.future.set_result(yielded.get_result()) else: self.yield_point = yielded except Exception: self.future = TracebackFuture() self.future.set_exc_info(sys.exc_info()) if self.stack_context_deactivate is None: with stack_context.ExceptionStackContext(self.handle_exception) as deactivate: self.stack_context_deactivate = deactivate def cb(): start_yield_point() self.run() self.io_loop.add_callback(cb) return False else: start_yield_point() else: try: self.future = convert_yielded(yielded) except BadYieldError: self.future = TracebackFuture() self.future.set_exc_info(sys.exc_info()) if not self.future.done() or self.future is moment: # moment = Future() self.io_loop.add_future(self.future, lambda f: self.run()) # 为该future添加callback return False return True2.1 init方法 init 里面执行了一些初始化的操作，最主要是最后两句： if self.handle_yield(first_yielded): # 运行 self.run()2.2 handle_yield方法 handle_yield(self, yielded) 函数，这个函数顾名思义，就是用来处理yield返回的对象的。 首先我们假设yielded是一个Future对象(因为这是最常用的情况)，这样的话代码就缩减了很多 def handle_yield(self, yielded): self.future = convert_yielded(yielded) # 如果yielded是Future对象则原样返回 if not self.future.done() or self.future is moment: # moment是tornado初始化时就建立的一个Future对象，且被set_result(None) self.io_loop.add_future(self.future, lambda f: self.run()) # 为该future添加callback return False return True也就是干了三步： 首先解析出self.future 然后判断self.future对象是否已经被done(完成)，如果没有的话为其添加回调函数，这个回调函数会执行self.run() 返回self.future对象是否被done 总体来说，handle_yield返回yielded对象是否被set_done，如果没有则为yielded对象添加回调函数，这个回调函数执行self.run() 还有一个有趣的地方，就是上面代码的第四行： self.io_loop.add_future(self.future, lambda f: self.run()) def add_future(self, future, callback): # 为future添加一个回调函数，这个回调函数的作用是：将参数callback添加至self._callbacks中 # 大家思考一个问题： 如果某个Future对象被set_done,那么他的回调函数应该在什么时候执行？ # 是立即执行亦或者是将回调函数添加到IOLoop实例的_callbacks中进行统一执行？ # 虽然前者更简单，但导致回调函数的执行过于混乱，我们应该让所有满足执行条件的回调函数统一执行。显然后者更合理 # 而add_future()的作用就是这样 future.add_done_callback(lambda future: self.add_callback(callback, future))def add_callback(self, callback, args, *kwargs): # 将callback添加至_callbacks列表中 self._callbacks.append(functools.partial(callback, *args, **kwargs))2.3 run方法 再来看self.run()方法。这个方法实际上就是一个循环，不停的执行generator的send()方法，发送的值就是yielded的result。 我们可以将run()方法简化一下： def run(self): &quot;&quot;&quot;Starts or resumes the generator, running until it reaches a yield point that is not ready. 循环向generator中传递值，直到某个yield返回的yielded还没有被done &quot;&quot;&quot; try: self.running = True while True: future = self.future if not future.done(): return self.future = None # 清空self.future value = future.result() # 获取future对象的结果 try: yielded = self.gen.send(value) # send该结果，并将self.gen返回的值赋值给yielded(一般情况下这也是个future对象) except (StopIteration, Return) as e: self.finished = True self.future = _null_future self.result_future.set_result(_value_from_stopiteration(e)) self.result_future = None self._deactivate_stack_context() return if not self.handle_yield(yielded): # 运行self.handler_yield(yielded)，如果yielded对象没有被done，则直接返回；否则继续循环 return finally: self.running = False总结： 1 每一个Future对应一个异步操作 2 该Future对象可以添加回调函数，当该异步操作完成后，需要对该Future对象设置set_done或者set_result，然后执行其所有的回调函数 3 凡是使用了coroutine装饰器的generator函数都会返回一个Future对象，同时会不断为该generator，该generator每一次运行send()或者next()的返回结果yielded以及future对象运行Runner() 4 Runner()会对generator不断进行send()或者next()操作。具体步骤是：上一个next()或者send()操作返回的yielded(一般是一个Future对象)被set_done后，将该yielded对象的结果send()至generator中，不断循环该操作，直到产生StopIteration或者Return异常(这表示该generator执行结束)，这时会为该generator对应的Future对象set_result。 我们可以看到tornado的协程是基于generator的，generator可以通过yield关键字暂停执行，也可以通过next()或者send()恢复执行，同时send()可以向generator中传递值。 而将协程连接起来的纽带则是Future对象，每一个Future对象都对应着一个异步操作，我们可以为该对象添加许多回调函数，当异步操作完成后通过对Future对象进行set_done或者set_result就可以执行相关的回调函数。 提供动力的则是Runner()，他不停的将generator所yield的每一个future对象的结果send()至generator，当generator运行结束，他会进行最后的包装工作，对该generator所对应的Future对象执行set_result操作。参考： http://blog.csdn.net/wyx819/article/details/45420017 http://www.cnblogs.com/apexchu/p/4226784.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是web框架？]]></title>
    <url>%2F2018%2F04%2F28%2F%E4%BB%80%E4%B9%88%E6%98%AFweb%E6%A1%86%E6%9E%B6%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[什么是web框架？本文转自：http://www.cnblogs.com/hazir/p/what_is_web_framework.html#top 并对原文有微小改动 –引入 Web 应用框架，或者简单的说是“Web 框架”，其实是建立 web 应用的一种方式。从简单的博客系统到复杂的富 AJAX 应用，web 上每个页面都是通过写代码来生成的。我发现很多人都热衷于学习 web 框架技术，例如 Flask 或这 Django 之类的，但是很多人并不理解什么是 web 框架，或者它们是如何工作的。这篇文章中，我将探索反复被忽略的 web 框架基础的话题。阅读完这篇文章，你应该首先对什么是 web 框架以及它们为什么会存在有更深的认识。这会让你学习一个新的 web 框架变得简单的多，还会让你在使用不同的框架的时候做个明智的选择。 –web如何工作？ 在我们讨论框架之前，我们需要理解 Web 如何“工作”的。为此，我们将深入挖掘你在浏览器里输入一个 URL 按下 Enter 之后都发生了什么。 在你的浏览器中打开一个新的标签，输入 http://www.jeffknupp.com 。在这里我们讨论：为了显示这个页面，浏览器都做了什么事情（不涉及 DNS 查询）。 –web服务器 每个页面都以 HTML 的形式传送到你的浏览器中，HTML 是一种浏览器用来描述页面内容和结构的语言。那些负责发送 HTML 到用户浏览器的应用称之为“Web 服务器”，会让你迷惑的是，这些应用运行的机器通常也叫做 web 服务器。 然而，最重要的是要理解，到最后所有的 web 应用要做的事情就是发送 HTML 到浏览器。不管应用的逻辑多么复杂，最终的结果总是将 HTML 发送到浏览器（我故意将应用可以响应像 JSON 或者 CSS 等不同类型的数据忽略掉，因为在概念上是相同的）。 web 应用如何知道发送什么到浏览器呢？这个问题很重要，但我们先存疑，接下来会讲述到的。 –http 浏览器从 web 服务器（或者叫应用服务器）上使用 HTTP 协议下载网页，HTTP 协议是基于一种 请求-响应（request-response）模型的协议。客户端（你的浏览器）从运行在物理机器上的 web 应用（服务器端）请求数据，web 应用反过来对你的浏览器请求进行响应。 重要的一点是，要记住通信总是由客户端（你的浏览器）发起的，服务器（也就是 web 服务器）没有办法创建一个链接，发送没有经过请求的数据给你的浏览器。如果你从 web 服务器上接收到数据，一定是因为你的浏览器显示地发送了请求。 –HTTP Methods 在 HTTP 协议中，每条报文都关联方法（method 或者 verb），不同的 HTTP 方法对应客户端可以发送的逻辑上不同类型的请求，反过来也代表了客户端的不同意图。例如，请求一个 web 页面的 HTML，与提交一个表单在逻辑上是不同的，所以这两种行为就需要使用不同的方法。 这里讲较为常用的、重要的两个方法：GET 和 POST GET： 从 web 服务器上 get（请求）数据，GET 请求是到目前位置最常见的一种 HTTP 请求，在一次 GET 请求过程中，web 应用对请求页面的 HTML 进行响应之外，就不需要做任何事情了。特别的，web 应用在 GET 请求的结果中，不应该改变应用的状态（比如，不能基于 GET 请求创建一个新帐号）。正是因为这个原因，GET 请求通常认为是“安全”的，因为他们不会导致应用的改变。 POST： 显然，除了简单的查看页面之外，应该还有更多与网站进行交互的操作。我们也能够向应用发送数据，例如通过表单。为了达到这样的目的，就需要一种不同类型的请求方法：POST。POST 请求通常携带由用户输入的数据，web 应用收到之后会产生一些行为。通过在表单里输入你的信息登录一个网站，就是 POST 表单的数据给 web 应用的。 不同于 GET 请求，POST 请求通常会导致应用状态的改变。在我们的例子中，当表单 POST 之后，一个新的账户被创建。不同于 GET 请求，POST 请求不总是生成一个新的 HTML 页面发送到客户端，而是客户端使用响应的响应码（response code）来决定对应用的操作是否成功。 HTTTP Response Codes通常来说，web 服务器返回 200 的响应码，意思是，“我已经完成了你要求我做的事情，一切都正常”。响应码总是一个三位数字的代号，web 应用在每个响应的同时都发送一个这样的代号，表明给定的请求的结果。响应码 200 字面意思是“OK”，是响应一个 GET 请求大多情况下都使用的代号。然而对于 POST 请求， 可能会有 204（“No Content”）发送回来，意思是“一切都正常，但是我不准备向你显示任何东西”。 POST 请求仍然会发送一个特殊的 URL，这个 URL 可能和提交数据的页面不同，意识这一点是至关重要的。还是以我们的登录为例，表单可能是在 www.foo.com/signup 页面，然而点击 submit，可能会导致带有表单数据的 POST 请求发送到 www.foo.com/process_sigup 上。POST 请求要发送的位置在表单的 HTML 中有特别标明。 –web应用 1 一个初级的web应用 你可以仅仅使用 HTTP GET 和 POST 做很多事情。一个应用程序负责去接收一个 HTTP 请求，同时给以 HTTP 响应，通常包含了请求页面的 HTML。POST 请求会引起 web 应用做出一些行为，可能是往数据库中添加一条记录这样的。还有很多其它的 HTTP 方法，但是我们目前只关注 GET 和 POST。 那么最简单的 web 应用是什么样的呢？我们可以写一个应用，让它一直监听 80 端口（著名的 HTTP 端口，几乎所有 HTTP 都发送到这个端口上）。一旦它接收到等待的客户端发送的请求连接，然后它就会回复一些简单的 HTML。 下面是程序的代码： import socketHOST = ‘’PORT = 80listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)listen_socket.bind((HOST, PORT))listen_socket.listen(1)connection, address = listen_socket.accept()request = connection.recv(1024)connection.sendall(b”””HTTP/1.1 200 OKContent-type: text/html Hello, World! """) connection.close() 这个代码接收简单的链接和简单的请求，不管请求的 URL 是什么，它都会响应 HTTP 200（所以，这不是一个真正意义上的 web 服务器）。Content-type:text/html 行代码的是 header 字段，header 用来提供请求或者响应的元信息。这样，我们就告诉了客户端接下来的数据是 HTML。 2 优化该web应用 如果我们继续以上面的例子为基础建立 web 应用，我们还需要解决很多问题： 我们怎样检测请求的 URL 并根据不同的URL返回相应的正确的页面？除了简单的 GET 请求之外我们如何处理 POST 请求？我们如何理解更高级的概念，如 session 和 cookie？我们如何扩展程序以使其处理上千个并发连接？围绕建立 web 应用的所有问题中，两个问题尤其突出： 我们如何将请求的 URL 映射到处理它的代码上？我们怎样动态地构造请求的 HTML 返回给客户端，HTML 中带有计算得到的值或者从数据库中取出来的信息？每个 web 框架都以某种方法来解决这些问题，也有很多不同的解决方案。用例子来说明更容易理解，所以我将针对这些问题讨论 Django 和 Flask 的解决方案。但是，首先我们还需要简单讨论一下 MVC 。 –&gt;Django 中的 MVC Django 充分利用 MVC 设计模式。 MVC，也就是 Model-View-Controller （模型-视图-控制器），是一种将应用的不同功能从逻辑上划分开。models 代表的是类似数据库表的资源（与 Python 中用 class 来对真实世界目标建模使用的方法大体相同）。controls 包括应用的业务逻辑，对 models 进行操作。为了动态生成代表页面的 HTML，需要 views 给出所有要动态生成页面的 HTML 的信息。 在 Django 中有点让人困惑的是，controllers 被称做 views，而 views 被称为 templates。除了名字上的有点奇怪，Django 很好地实现了 MVC 的体系架构。 &lt;–如何解决将请求的 URL 映射到处理它的代码上 Django 中的路由 路由是处理请求 URL 到负责生成相关的 HTML 的代码之间映射的过程。在简单的情形下，所有的请求都是有相同的代码来处理（就像我们之前的例子那样）。变得稍微复杂一点，每个 URL 对应一个 view function 。举例来说，如果请求 www.foo.com/bar 这样的 URL，调用 handler_bar() 这样的函数来产生响应。我们可以建立这样的映射表，枚举出我们应用支持的所有 URL 与它们相关的函数。 然而，当 URL 中包含有用的数据，例如资源的 ID（像这样 www.foo.com/users/3/） ，那么这种方法将变得非常臃肿。我们如何将 URL 映射到一个 view 函数，同时如何利用我们想显示 ID 为 3 的用户？ Django 的答案是，将 URL 正则表达式映射到可以带参数的 view 函数。例如，我假设匹配^/users/(?P\d+)/$ 的 URL 调用 display_user(id) 这样的函数，这儿参数 id 是正则表达式中匹配的 id。这种方法，任何 /users// 这样的 URL 都会映射到 display_user 函数。这些正则表达式可以非常复杂，包含关键字和参数。 Flask 中的路由 Flask 采取了一点不同的方法。将一个函数和请求的 URL 关联起来的标准方法是通过使用 route() 装饰器。下面是 Flask 代码，在功能上和上面正则表达式方法相同： @app.route(‘/users/id:int/‘)def display_user(id): # ... 就像你看到的这样，装饰器使用几乎最简单的正则表达式的形式来将 URL 映射到参数。通过传递给route() 的 URL 中包含的 name:type 指令，可以提取到参数。路由像 /info/about_us.html 这样的静态 URL，可以像你预想的这样 @app.route(‘/info/about_us.html’) 处理。 通过 Templates 动态产生 HTML 继续上面的例子，一旦我们有合适的代码映射到正确的 URL，我们如何动态生成 HTML？对于 Django 和 Flask，答案都是通过 HTML Templating。 HTML Templating 和使用 str.format() 类似：需要动态输出值的地方使用占位符填充，这些占位符后来通过 str.format() 函数用参数替换掉。想象一下，整个 web 页面就是一个字符串，用括号标明动态数据的位置，最后再调用 str.format() 。Django 模板和 Flask 使用的模板引擎 Jinja2 都使用的是这种方法。 然而，不是所有的模板引擎都能相同的功能。Django 支持在模板里基本的编程，而 Jinja2 只能让你执行特定的代码（不是真正意义上的代码，但也差不多）。Jinja2 可以缓存渲染之后的模板，让接下来具有相同参数的请求可以直接从缓存中返回结果，而不是用再次花大力气渲染。 数据库交互 Django 有着“功能齐全”的设计哲学，其中包含了一个 ORM(Object Realational Mapper， 对象关系映射)，ORM 的目的有两方面：一是将 Python 的 class 与数据库表建立映射，而是剥离出不同数据库引擎直接的差异。没人喜欢 ORM，因为在不同的域之间映射永远不完美，然而这还在承受范围之内。Django 是功能齐全的，而 Flask 是一个微框架，不包括 ORM，尽管它对 SQLAlchemy 兼容性非常好，SQLAlchemy 是 Django ORM 的最大也是唯一的竞争对手。 内嵌 ORM 让 Django 有能力创建一个功能丰富的 CRUD 应用，从服务器端角度来看，CRUD（CreateRead Update Delete）应用非常适合使用 web 框架技术。Django 和 Flask-SQLchemy 可以直接对每个 model 进行不同的 CRUD 操作。 再谈 web 框架 到现在为止，web 框架的目的应该非常清晰了：向程序员隐藏了处理 HTTP 请求和响应相关的基础代码。至于隐藏多少这取决于不同的框架，Django 和 Flask 走向了两个极端：Django 包括了每种情形，几乎成了它致命的一点；Flask 立足于“微框架”，仅仅实现 web 应用需要的最小功能，其它的不常用的 web 框架任务交由第三方库来完成。 但是最后要记住的是，Python web 框架都以相同的方式工作的：它们接收 HTTP 请求，分派代码，产生 HTML，创建带有内容的 HTTP 响应。事实上，所有主流的服务器端框架都以这种方式工作的（ JavaScript 框架除外）。但愿了解了这些框架的目的，你能够在不同的框架之间选择适合你应用的框架进行开发。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中的文件描述符]]></title>
    <url>%2F2018%2F04%2F19%2FLinux%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[Linux中的文件描述符 概述 在Linux系统中一切皆可以看成是文件，文件又可分为：普通文件、目录文件、链接文件和设备文件。文件描述符（file descriptor）是内核为了高效管理已被打开的文件所创建的索引，其是一个非负整数（通常是小整数），用于指代被打开的文件，所有执行I/O操作的系统调用都通过文件描述符。程序刚刚启动的时候，0是标准输入，1是标准输出，2是标准错误。如果此时去打开一个新的文件，它的文件描述符会是3。POSIX标准要求每次打开文件时（含socket）必须使用当前进程中最小可用的文件描述符号码，因此，在网络通信过程中稍不注意就有可能造成串话。标准文件描述符图如下： 文件描述与打开的文件对应模型如下图： 文件描述限制 在编写文件操作的或者网络通信的软件时，初学者一般可能会遇到“Too many open files”的问题。这主要是因为文件描述符是系统的一个重要资源，虽然说系统内存有多少就可以打开多少的文件描述符，但是在实际实现过程中内核是会做相应的处理的，一般最大打开文件数会是系统内存的10%（以KB来计算）（称之为系统级限制），查看系统级别的最大打开文件数可以使用sysctl -a | grep fs.file-max命令查看。与此同时，内核为了不让某一个进程消耗掉所有的文件资源，其也会对单个进程最大打开文件数做默认值处理（称之为用户级限制），默认值一般是1024，使用ulimit -n命令可以查看。在Web服务器中，通过更改系统默认值文件描述符的最大值来优化服务器是最常见的方式之一，具体优化方式请查看http://blog.csdn.net/kumu_linux/article/details/7877770。 文件描述符合打开文件之间的关系 每一个文件描述符会与一个打开文件相对应，同时，不同的文件描述符也会指向同一个文件。相同的文件可以被不同的进程打开也可以在同一个进程中被多次打开。系统为每一个进程维护了一个文件描述符表，该表的值都是从0开始的，所以在不同的进程中你会看到相同的文件描述符，这种情况下相同文件描述符有可能指向同一个文件，也有可能指向不同的文件。具体情况要具体分析，要理解具体其概况如何，需要查看由内核维护的3个数据结构。 进程级的文件描述符表 系统级的打开文件描述符表 文件系统的i-node表 进程级的描述符表的每一条目记录了单个文件描述符的相关信息。 1. 控制文件描述符操作的一组标志。（目前，此类标志仅定义了一个，即close-on-exec标志） 2. 对打开文件句柄的引用内核对所有打开的文件的文件维护有一个系统级的描述符表格（open file description table）。有时，也称之为打开文件表（open file table），并将表格中各条目称为打开文件句柄（open file handle）。一个打开文件句柄存储了与一个打开文件相关的全部信息，如下所示： 1. 当前文件偏移量（调用read()和write()时更新，或使用lseek()直接修改） 2. 打开文件时所使用的状态标识（即，open()的flags参数） 3. 文件访问模式（如调用open()时所设置的只读模式、只写模式或读写模式） 4. 与信号驱动相关的设置 5. 对该文件i-node对象的引用 6. 文件类型（例如：常规文件、套接字或FIFO）和访问权限 7. 一个指针，指向该文件所持有的锁列表 8. 文件的各种属性，包括文件大小以及与不同类型操作相关的时间戳下图展示了文件描述符、打开的文件句柄以及i-node之间的关系，图中，两个进程拥有诸多打开的文件描述符。 在进程A中，文件描述符1和30都指向了同一个打开的文件句柄（标号23）。这可能是通过调用dup()、dup2()、fcntl()或者对同一个文件多次调用了open()函数而形成的。 进程A的文件描述符2和进程B的文件描述符2都指向了同一个打开的文件句柄（标号73）。这种情形可能是在调用fork()后出现的（即，进程A、B是父子进程关系），或者当某进程通过UNIX域套接字将一个打开的文件描述符传递给另一个进程时，也会发生。再者是不同的进程独自去调用open函数打开了同一个文件，此时进程内部的描述符正好分配到与其他进程打开该文件的描述符一样。 此外，进程A的描述符0和进程B的描述符3分别指向不同的打开文件句柄，但这些句柄均指向i-node表的相同条目（1976），换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了open()调用。同一个进程两次打开同一个文件，也会发生类似情况。 总结 由于进程级文件描述符表的存在，不同的进程中会出现相同的文件描述符，它们可能指向同一个文件，也可能指向不同的文件 两个不同的文件描述符，若指向同一个打开文件句柄，将共享同一文件偏移量。因此，如果通过其中一个文件描述符来修改文件偏移量（由调用read()、write()或lseek()所致），那么从另一个描述符中也会观察到变化，无论这两个文件描述符是否属于不同进程，还是同一个进程，情况都是如此。 要获取和修改打开的文件标志（例如：O_APPEND、O_NONBLOCK和O_ASYNC），可执行fcntl()的F_GETFL和F_SETFL操作，其对作用域的约束与上一条颇为类似。 文件描述符标志（即，close-on-exec）为进程和文件描述符所私有。对这一标志的修改将不会影响同一进程或不同进程中的其他文件描述符]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的os模块]]></title>
    <url>%2F2018%2F03%2F18%2Fpython%E4%B8%AD%E7%9A%84os%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[os模块os模块的作用： os，语义为操作系统，所以肯定就是操作系统相关的功能了，可以处理文件和目录这些我们日常手动需要做的操作，就比如说：显示当前目录下所有文件/删除某个文件/获取文件大小…… 另外，os模块不受平台限制，也就是说：当我们要在linux中显示当前路径时就要用到pwd命令，而Windows中cmd命令行下就要用到这个，额…我擦，我还真不知道，（甭管怎么着，肯定不是pwd），这时候我们使用python中os模块的os.path.abspath(name)功能，甭管是linux或者Windows都可以获取当前的绝对路径。 os模块的常用功能： 1 os.name #显示当前使用的平台 os.name‘nt’ #这表示Windowsos.name‘posix’ #这表示Linux2 os.getcwd() #显示当前python脚本工作路径 os.getcwd()‘C:\Users\Capital-D\PycharmProjects\untitled’ #使用pycharm os.getcwd()‘/root’ #Linux平台在/root目录直接使用python3命令3 os.listdir(‘dirname’) #返回指定目录下的所有文件和目录名 #相对于os.getcwd路径下的文件 os.listdir()[‘.idea’, ‘test’] os.listdir()[‘.bash_logout’, ‘Python-3.4.4’, ‘.mysql_history’, ‘.tcshrc’, ‘Python-3.4.4.tar.xz’, ‘.bash_profile’, ‘.lesshst’, ‘install.log.syslog’, ‘.cshrc’, ‘04.sql’, ‘anaconda-ks.cfg’, ‘test’, ‘.viminfo’, ‘phpMyAdmin-4.4.15-all-languages.tar.bz2’, ‘1test’, ‘.bashrc’, ‘binlog.sql’, ‘back.sql’, ‘install.log’, ‘binlog4.sql’, ‘.bash_history’, ‘backup.sql’, ‘text.py’, ‘.rnd’, ‘test1’] 4 os.remove(‘filename’) #删除一个文件 [root@slyoyo ~]# touch hahaha[root@slyoyo ~]# ls04.sql back.sql binlog.sql install.log.syslog Python-3.4.4.tar.xz text.py1test backup.sql hahaha phpMyAdmin-4.4.15-all-languages.tar.bz2 testanaconda-ks.cfg binlog4.sql install.log Python-3.4.4 test1 #hahaha（粉色字体）存在[root@slyoyo ~]# python3Python 3.4.4 (default, Apr 5 2016, 04:23:19)[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linuxType “help”, “copyright”, “credits” or “license” for more information. import osos.remove(‘hahaha’)exit()[root@slyoyo ~]# ls04.sql anaconda-ks.cfg backup.sql binlog.sql install.log.syslog Python-3.4.4 test text.py1test back.sql binlog4.sql install.log phpMyAdmin-4.4.15-all-languages.tar.bz2 Python-3.4.4.tar.xz test1 #hahaha已被删 5 os.makedirs(‘dirname/dirname’) #可生成多层递规目录 [root@slyoyo ~]# ls04.sql anaconda-ks.cfg backup.sql binlog.sql install.log.syslog Python-3.4.4 test text.py1test back.sql binlog4.sql install.log phpMyAdmin-4.4.15-all-languages.tar.bz2 Python-3.4.4.tar.xz test1[root@slyoyo ~]# python3Python 3.4.4 (default, Apr 5 2016, 04:23:19)[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linuxType “help”, “copyright”, “credits” or “license” for more information. import osos.makedirs(‘hahaha/linghuchong’)exit()[root@slyoyo ~]# ls04.sql back.sql binlog.sql install.log.syslog Python-3.4.4.tar.xz text.py1test backup.sql hahaha phpMyAdmin-4.4.15-all-languages.tar.bz2 testanaconda-ks.cfg binlog4.sql install.log Python-3.4.4 test1[root@slyoyo ~]# ls hahaha/linghuchong[root@slyoyo ~]# ls hahaha/linghuchong/[root@slyoyo ~]# 6 os.rmdir(‘dirname’) #删除单级目录 [root@slyoyo ~]# ls04.sql back.sql binlog.sql install.log.syslog Python-3.4.4.tar.xz text.py1test backup.sql hahaha phpMyAdmin-4.4.15-all-languages.tar.bz2 testanaconda-ks.cfg binlog4.sql install.log Python-3.4.4 test1[root@slyoyo ~]# ls hahaha/linghuchong[root@slyoyo ~]# ls hahaha/linghuchong/[root@slyoyo ~]# python3Python 3.4.4 (default, Apr 5 2016, 04:23:19)[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linuxType “help”, “copyright”, “credits” or “license” for more information. import osos.rmdir(‘hahaha/linghuchong’)exit()[root@slyoyo ~]# ls hahaha/[root@slyoyo ~]# 7 os.rename(“oldname”,”newname”) #重命名文件 os.getcwd()‘/root/hahaha’os.listdir()[‘test’]os.rename(‘test’,’test_new’)os.listdir()[‘test_new’] 8 os.system() #运行shell命令,注意：这里是打开一个新的shell，运行命令，当命令结束后，关闭shell os.system(‘pwd’)/root/hahaha0 9 os.sep #显示当前平台下路径分隔符 os.sep‘/‘ #linux os.sep‘\‘ #windows10 os.linesep #给出当前平台使用的行终止符 os.linesep‘\n’ #linux os.linesep‘\r\n’ #windows11 os.environ #获取系统环境变量 os.environenviron({‘USERPROFILE’: ‘C:\Users\Capital-D’, ‘PROCESSOR_ARCHITECTURE’: ‘x86’, ‘SESSIONNAME’: ‘Console’, ‘UGII_BASE_DIR’: ‘D:\Program Files (x86)\Siemens\NX 8.0’, ‘COMMONPROGRAMW6432’: ‘C:\Program Files\Common Files’, ‘COMMONPROGRAMFILES(X86)’: ‘C:\Program Files (x86)\Common Files’, ‘MOZ_PLUGIN_PATH’: ‘C:\Program Files (x86)\Foxit Software\Foxit Reader\plugins\‘, ‘ALLUSERSPROFILE’: ‘C:\ProgramData’, ‘PYTHONIOENCODING’: ‘UTF-8’, ‘NUMBER_OF_PROCESSORS’: ‘4’, ‘APPDATA’: ‘C:\Users\Capital-D\AppData\Roaming’, ‘TERM’: ‘emacs’, ‘TEMP’: ‘C:\Users\CAPITA1\AppData\Local\Temp’, ‘PROGRAMDATA’: ‘C:\ProgramData’, ‘COMSPEC’: ‘C:\windows\system32\cmd.exe’, ‘WINDIR’: ‘C:\windows’, ‘PROCESSOR_IDENTIFIER’: ‘Intel64 Family 6 Model 58 Stepping 9, GenuineIntel’, ‘PATHEXT’: ‘.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY’, ‘PATH’: ‘C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\NetSarang;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT’, ‘UGII_ROOT_DIR’: ‘D:\Program Files (x86)\Siemens\NX 8.0\UGII\‘, ‘COMPUTERNAME’: ‘IDEA-PC’, ‘USERDOMAIN’: ‘idea-PC’, ‘TMP’: ‘C:\Users\CAPITA1\AppData\Local\Temp’, ‘SYSTEMROOT’: ‘C:\windows’, ‘PROCESSOR_REVISION’: ‘3a09’, ‘FP_NO_HOST_CHECK’: ‘NO’, ‘PROGRAMFILES’: ‘C:\Program Files (x86)’, ‘PYTHONDONTWRITEBYTECODE’: ‘1’, ‘LOCALAPPDATA’: ‘C:\Users\Capital-D\AppData\Local’, ‘PYTHONUNBUFFERED’: ‘1’, ‘LOGONSERVER’: ‘\\IDEA-PC’, ‘UGII_LANG’: ‘simpl_chinese’, ‘SYSTEMDRIVE’: ‘C:’, ‘PUBLIC’: ‘C:\Users\Public’, ‘HOMEPATH’: ‘\Users\Capital-D’, ‘PYTHONPATH’: ‘C:\Program Files (x86)\JetBrains\PyCharm 5.0.2\helpers\pydev’, ‘USERNAME’: ‘Capital-D’, ‘UGS_LICENSE_SERVER’: ‘28000@idea-pc’, ‘USERDOMAIN_ROAMINGPROFILE’: ‘idea-PC’, ‘PYCHARM_HOSTED’: ‘1’, ‘OS’: ‘Windows_NT’, ‘PROCESSOR_ARCHITEW6432’: ‘AMD64’, ‘PROGRAMFILES(X86)’: ‘C:\Program Files (x86)’, ‘PROGRAMW6432’: ‘C:\Program Files’, ‘PSMODULEPATH’: ‘C:\windows\system32\WindowsPowerShell\v1.0\Modules\‘, ‘COMMONPROGRAMFILES’: ‘C:\Program Files (x86)\Common Files’, ‘IPYTHONENABLE’: ‘True’, ‘HOMEDRIVE’: ‘C:’, ‘CONFIGSETROOT’: ‘C:\windows\ConfigSetRoot’, ‘PROCESSOR_LEVEL’: ‘6’}) 12 os.path.abspath(path) #显示当前绝对路径 os.path.abspath(‘test’)‘C:\Users\Capital-D\PycharmProjects\untitled\test’13 os.path.dirname(path) #返回该路径的父目录 os.path.abspath(‘test’)‘C:\Users\Capital-D\PycharmProjects\untitled\test’os.path.dirname(os.path.abspath(‘test’))‘C:\Users\Capital-D\PycharmProjects\untitled’14 os.path.basename(path) #返回该路径的最后一个目录或者文件,如果path以／或\结尾，那么就会返回空值。 os.path.dirname(os.path.abspath(‘test’))‘C:\Users\Capital-D\PycharmProjects\untitled’os.path.basename(os.path.dirname(os.path.abspath(‘test’)))‘untitled’15 os.path.isfile(path) #如果path是一个文件，则返回True [root@slyoyo ~]# ls04.sql back.sql binlog.sql install.log.syslog Python-3.4.4.tar.xz text.py1test backup.sql hahaha phpMyAdmin-4.4.15-all-languages.tar.bz2 testanaconda-ks.cfg binlog4.sql install.log Python-3.4.4 test1[root@slyoyo ~]# python3Python 3.4.4 (default, Apr 5 2016, 04:23:19)[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linuxType “help”, “copyright”, “credits” or “license” for more information. import osos.path.isfile(‘test’)True 16 os.path.isdir(path) #如果path是一个目录，则返回True os.path.isdir(‘hahaha’)True17 os.stat() #获取文件或者目录信息 os.stat(‘test’)os.stat_result(st_mode=33188, st_ino=137149, st_dev=2050, st_nlink=1, st_uid=0, st_gid=0, st_size=85, st_atime=1462373193, st_mtime=1462373186, st_ctime=1462373186)18 os.path.split(path) #将path分割成路径名和文件名。（事实上，如果你完全使用目录，它也会将最后一个目录作为文件名而分离，同时它不会判断文件或目录是否存在） os.path.split(‘/root/test’)(‘/root’, ‘test’)19 os.path.join(path,name) #连接目录与文件名或目录 结果为path/name os.path.join(‘/root/haha’,’test’)‘/root/haha/test’]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[uwsgi多进程配合kafka-python消息无法发送]]></title>
    <url>%2F2018%2F03%2F17%2Fuwsgi%E5%A4%9A%E8%BF%9B%E7%A8%8B%E9%85%8D%E5%90%88kafka-python%E6%B6%88%E6%81%AF%E6%97%A0%E6%B3%95%E5%8F%91%E9%80%81%2F</url>
    <content type="text"><![CDATA[uwsgi多进程配合kafka-python消息无法发送在工作中，使用uwsgi部署项目，其中uwsgi设置为多进程，并且python中使用了kafka-python模块作为生产者不断产生数据，但上线不久后几乎所有的生产者消息都报：KafkaTimeoutError这个错误，并且在kafka服务器中并没有发现收到任何消息。 于是看了看kafka-python源码，发现在执行send方法后，消息并没有立即发送，而是放到本地的缓存中，在生成KafkaProducer实例时，有个选项buffer_memory设置了缓存的大小，默认为32M，然后如果这个buffer满了就会报KafkaTimeoutError，所以初步判断两个原因： 1 生产者消息并没有发送出去， 2 或者消息发送相对于消息生成来说过于缓慢导致 同时又因为看到kafka服务器中并没有接收到任何消息，遂排除第二个原因。也就是说生产者消息没有发送出去。于是采用同样的配置用写了一个脚本发现kafka服务器可以接收到消息，鉴定是我的生产者有问题，遂谷歌解决问题，找到该帖子：https://github.com/dpkp/kafka-python/issues/721。发布人情况和我差不多，作者回复到： You cannot share producer instances across processes, only threads. I expect that is why the master process pattern is failing. Second, producer.send() is async but is not guaranteed to deliver if you close the producer abruptly. In your final example I suspect that your producer instances are so short-lived that they are being reaped before flushing all pending messages. To guarantee delivery (or exception) call producer.send().get(timeout) or producer.flush() . otherwise you’ll need to figure out how to get a producer instance per-uwsgi-thread and have it shared across requests (you would still want to flush before thread shutdown to guarantee no messages are dropped) 大体上说明了两点： 1 多进程共享同一个生产者实例有问题 2 send方法是异步的，当执行完send后立即关闭生产者实例的话可能会导致发送失败。 第二点错误我没有犯，沾沾自喜，继续看评论： Aha, thanks! After looking more closely at uWSGI options I discovered the lazy-apps option, which causes each worker to load the entire app itself. This seems to have resolved my issue. 提问者说他解决了该问题，于是查一查uwsgi中的lazy-apps，发现改文章：https://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/articles/TheArtOfGracefulReloading.html#preforking-vs-lazy-apps-vs-lazy，其中说到： 默认情况下，uWSGI在第一个进程中加载整个应用，然后在加载完应用之后，会多次 fork() 自己。 我看看了我自己的代码我确实是在app生成之前生成了生产者实例，这就导致该实例被父进程与其子进程共享。问题终于明白，开始解决： 1 使用lazy-apps，这样就可以了。 2 不使用lazy-apps，在代码层面解决问题： producer.py文件import jsonfrom kafka import KafkaProducer class Single(object): “””单例模式””” def new(cls, args, **kwargs): if not hasattr(cls, “instance”): cls.instance = super().__new(cls) if hasattr(cls, “initialize”): cls._instance.initialize(args, **kwargs) return cls._instance class MsgQueue(Single): “”” 这个整成单例模式是因为：uwsgi配合kafka-python在多进程下会有问题，这里希望每个进程单独享有一个kafka producer实例, 也就是说当初始化app对象后，并不会生成producer实例，而是在运行时再生成， 具体参考：https://github.com/dpkp/kafka-python/issues/721 “”” app = None def initialize(self): self.producer = KafkaProducer(bootstrap_servers=self.app.config[&quot;MQ_URI&quot;], api_version=self.app.config[&quot;KAFKA_API_VERSION&quot;]) @classmethod def init_app(cls, app): cls.app = app def send(self, topic, data): &quot;&quot;&quot; :param topic: :param data: :return: &quot;&quot;&quot; data = json.dumps(data, ensure_ascii=True) self.producer.send(topic, data.encode())app.py文件from producer import MsgQueue…MsgQueue.init_app(app) 业务逻辑中用到生产者的文件from producer import MsgQueue…MsgQueue().send(msg)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拥有root权限却报错Access denied]]></title>
    <url>%2F2018%2F03%2F09%2F%E6%8B%A5%E6%9C%89root%E6%9D%83%E9%99%90%E5%8D%B4%E6%8A%A5%E9%94%99Access%20denied%2F</url>
    <content type="text"><![CDATA[拥有root权限却报错Access denied起因：我在centos7上根据mysql官网配置安装mysql 5.7.20，安装完成后希望其能够开机自动启动，于是乎运行命令： systemctl enable mysqld结果出现错误： Failed to execute operation: Access denied我勒个去！ 我可是root好不好啊，于是上网查了查，发现没有关闭selinux,关于selinux我一直困惑了很久，只是知道：如果对一个文件没有正确安全上下文配置， 甚至你是root用户，你也不能启动某服务 setenforce 0然后再次运行systemctl enable mysqld成功！]]></content>
      <categories>
        <category>jquery</category>
      </categories>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python time模块和datetime模块详解]]></title>
    <url>%2F2018%2F02%2F25%2Fpython%20time%E6%A8%A1%E5%9D%97%E5%92%8Cdatetime%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[python time模块和datetime模块详解time模块 time模块中时间表现的格式主要有三种： a、timestamp时间戳，时间戳表示的是从1970年1月1日00:00:00开始按秒计算的偏移量 b、struct_time时间元组，共有九个元素组。 c、format time 格式化时间，已格式化的结构使时间更具可读性。包括自定义格式和固定格式。 1、时间格式转换图： 2、主要time生成方法和time格式转换方法实例： #! /usr/bin/env python -- coding:utf-8 --author = “TKQ”import time 生成timestamptime.time() 1477471508.05#struct_time to timestamptime.mktime(time.localtime()) #生成struct_time timestamp to struct_time 本地时间time.localtime()time.localtime(time.time()) time.struct_time(tm_year=2016, tm_mon=10, tm_mday=26, tm_hour=16, tm_min=45, tm_sec=8, tm_wday=2, tm_yday=300, tm_isdst=0)timestamp to struct_time 格林威治时间time.gmtime()time.gmtime(time.time()) time.struct_time(tm_year=2016, tm_mon=10, tm_mday=26, tm_hour=8, tm_min=45, tm_sec=8, tm_wday=2, tm_yday=300, tm_isdst=0)#format_time to struct_timetime.strptime(‘2011-05-05 16:37:06’, ‘%Y-%m-%d %X’) time.struct_time(tm_year=2011, tm_mon=5, tm_mday=5, tm_hour=16, tm_min=37, tm_sec=6, tm_wday=3, tm_yday=125, tm_isdst=-1)#生成format_time #struct_time to format_timetime.strftime(“%Y-%m-%d %X”)time.strftime(“%Y-%m-%d %X”,time.localtime()) 2016-10-26 16:48:41#生成固定格式的时间表示格式time.asctime(time.localtime())time.ctime(time.time()) Wed Oct 26 16:45:08 2016struct_time元组元素结构 属性 值tm_year（年） 比如2011tm_mon（月） 1 - 12tm_mday（日） 1 - 31tm_hour（时） 0 - 23tm_min（分） 0 - 59tm_sec（秒） 0 - 61tm_wday（weekday） 0 - 6（0表示周日）tm_yday（一年中的第几天） 1 - 366tm_isdst（是否是夏令时） 默认为-1 format time结构化表示 格式 含义%a 本地（locale）简化星期名称%A 本地完整星期名称%b 本地简化月份名称%B 本地完整月份名称%c 本地相应的日期和时间表示%d 一个月中的第几天（01 - 31）%H 一天中的第几个小时（24小时制，00 - 23）%I 第几个小时（12小时制，01 - 12）%j 一年中的第几天（001 - 366）%m 月份（01 - 12）%M 分钟数（00 - 59）%p 本地am或者pm的相应符%S 秒（01 - 61）%U 一年中的星期数。（00 - 53星期天是一个星期的开始。）第一个星期天之前的所有天数都放在第0周。%w 一个星期中的第几天（0 - 6，0是星期天）%W 和%U基本相同，不同的是%W以星期一为一个星期的开始。%x 本地相应日期%X 本地相应时间%y 去掉世纪的年份（00 - 99）%Y 完整的年份%Z 时区的名字（如果不存在为空字符）%% ‘%’字符 常见结构化时间组合： print time.strftime(“%Y-%m-%d %X”) #2016-10-26 20:50:13 3、time加减 #timestamp加减单位以秒为单位import timet1 = time.time()t2=t1+10 print time.ctime(t1)#Wed Oct 26 21:15:30 2016print time.ctime(t2)#Wed Oct 26 21:15:40 2016 二、datetime模块datatime模块重新封装了time模块，提供更多接口，提供的类有：date,time,datetime,timedelta,tzinfo。 1、date类 datetime.date(year, month, day) 静态方法和字段 date.max、date.min：date对象所能表示的最大、最小日期；date.resolution：date对象表示日期的最小单位。这里是天。date.today()：返回一个表示当前本地日期的date对象；date.fromtimestamp(timestamp)：根据给定的时间戮，返回一个date对象； from datetime import * import time print ‘date.max:’, date.maxprint ‘date.min:’, date.minprint ‘date.today():’, date.today()print ‘date.fromtimestamp():’, date.fromtimestamp(time.time()) #Output====================== date.max: 9999-12-31date.min: 0001-01-01date.today(): 2016-10-26date.fromtimestamp(): 2016-10-26方法和属性 d1 = date(2011,06,03)#date对象d1.year、date.month、date.day：年、月、日；d1.replace(year, month, day)：生成一个新的日期对象，用参数指定的年，月，日代替原有对象中的属性。（原有对象仍保持不变）d1.timetuple()：返回日期对应的time.struct_time对象；d1.weekday()：返回weekday，如果是星期一，返回0；如果是星期2，返回1，以此类推；d1.isoweekday()：返回weekday，如果是星期一，返回1；如果是星期2，返回2，以此类推；d1.isocalendar()：返回格式如(year，month，day)的元组；d1.isoformat()：返回格式如’YYYY-MM-DD’的字符串；d1.strftime(fmt)：和time模块format相同。 from datetime import * now = date(2016, 10, 26)tomorrow = now.replace(day = 27)print ‘now:’, now, ‘, tomorrow:’, tomorrowprint ‘timetuple():’, now.timetuple()print ‘weekday():’, now.weekday()print ‘isoweekday():’, now.isoweekday()print ‘isocalendar():’, now.isocalendar()print ‘isoformat():’, now.isoformat()print ‘strftime():’, now.strftime(“%Y-%m-%d”) #Output======================== now: 2016-10-26 , tomorrow: 2016-10-27timetuple(): time.struct_time(tm_year=2016, tm_mon=10, tm_mday=26, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=300, tm_isdst=-1)weekday(): 2isoweekday(): 3isocalendar(): (2016, 43, 3)isoformat(): 2016-10-26strftime(): 2016-10-262、time类 datetime.time(hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ) 静态方法和字段 time.min、time.max：time类所能表示的最小、最大时间。其中，time.min = time(0, 0, 0, 0)， time.max = time(23, 59, 59, 999999)；time.resolution：时间的最小单位，这里是1微秒； 方法和属性 t1 = datetime.time(10,23,15)#time对象t1.hour、t1.minute、t1.second、t1.microsecond：时、分、秒、微秒；t1.tzinfo：时区信息；t1.replace([ hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ] )：创建一个新的时间对象，用参数指定的时、分、秒、微秒代替原有对象中的属性（原有对象仍保持不变）；t1.isoformat()：返回型如”HH:MM:SS”格式的字符串表示；t1.strftime(fmt)：同time模块中的format； from datetime import * tm = time(23, 46, 10)print ‘tm:’, tmprint ‘hour: %d, minute: %d, second: %d, microsecond: %d’ % (tm.hour, tm.minute, tm.second, tm.microsecond)tm1 = tm.replace(hour=20)print ‘tm1:’, tm1print ‘isoformat():’, tm.isoformat()print ‘strftime()’, tm.strftime(“%X”) #Output============================================== tm: 23:46:10hour: 23, minute: 46, second: 10, microsecond: 0tm1: 20:46:10isoformat(): 23:46:10strftime() 23:46:103、datetime类 datetime相当于date和time结合起来。datetime.datetime (year, month, day[ , hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ] ) 静态方法和字段 datetime.today()：返回一个表示当前本地时间的datetime对象；datetime.now([tz])：返回一个表示当前本地时间的datetime对象，如果提供了参数tz，则获取tz参数所指时区的本地时间；datetime.utcnow()：返回一个当前utc时间的datetime对象；#格林威治时间datetime.fromtimestamp(timestamp[, tz])：根据时间戮创建一个datetime对象，参数tz指定时区信息；datetime.utcfromtimestamp(timestamp)：根据时间戮创建一个datetime对象；datetime.combine(date, time)：根据date和time，创建一个datetime对象；datetime.strptime(date_string, format)：将格式字符串转换为datetime对象； from datetime import * import time print ‘datetime.max:’, datetime.maxprint ‘datetime.min:’, datetime.minprint ‘datetime.resolution:’, datetime.resolutionprint ‘today():’, datetime.today()print ‘now():’, datetime.now()print ‘utcnow():’, datetime.utcnow()print ‘fromtimestamp(tmstmp):’, datetime.fromtimestamp(time.time())print ‘utcfromtimestamp(tmstmp):’, datetime.utcfromtimestamp(time.time()) #output====================== datetime.max: 9999-12-31 23:59:59.999999datetime.min: 0001-01-01 00:00:00datetime.resolution: 0:00:00.000001today(): 2016-10-26 23:12:51.307000now(): 2016-10-26 23:12:51.307000utcnow(): 2016-10-26 15:12:51.307000fromtimestamp(tmstmp): 2016-10-26 23:12:51.307000utcfromtimestamp(tmstmp): 2016-10-26 15:12:51.307000方法和属性 dt=datetime.now()#datetime对象dt.year、month、day、hour、minute、second、microsecond、tzinfo：dt.date()：获取date对象；dt.time()：获取time对象；dt. replace ([ year[ , month[ , day[ , hour[ , minute[ , second[ , microsecond[ , tzinfo] ] ] ] ] ] ] ])：dt. timetuple ()dt. utctimetuple ()dt. toordinal ()dt. weekday ()dt. isocalendar ()dt. isoformat ([ sep] )dt. ctime ()：返回一个日期时间的C格式字符串，等效于time.ctime(time.mktime(dt.timetuple()))；dt. strftime (format) 4.timedelta类，时间加减 使用timedelta可以很方便的在日期上做天days，小时hour，分钟，秒，毫秒，微妙的时间计算，如果要计算月份则需要另外的办法。 #coding:utf-8from datetime import * dt = datetime.now() #日期减一天dt1 = dt + timedelta(days=-1)#昨天dt2 = dt - timedelta(days=1)#昨天dt3 = dt + timedelta(days=1)#明天delta_obj = dt3-dtprint type(delta_obj),delta_obj#&lt;type ‘datetime.timedelta’&gt; 1 day, 0:00:00print delta_obj.days ,delta_obj.total_seconds()#1 86400.0 5、tzinfo时区类 #! /usr/bin/python coding=utf-8from datetime import datetime, tzinfo,timedelta “””tzinfo是关于时区信息的类tzinfo是一个抽象类，所以不能直接被实例化“””class UTC(tzinfo): “””UTC””” def init(self,offset = 0): self._offset = offset def utcoffset(self, dt): return timedelta(hours=self._offset) def tzname(self, dt): return &quot;UTC +%s&quot; % self._offset def dst(self, dt): return timedelta(hours=self._offset)#北京时间beijing = datetime(2011,11,11,0,0,0,tzinfo = UTC(8))print “beijing time:”,beijing #曼谷时间bangkok = datetime(2011,11,11,0,0,0,tzinfo = UTC(7))print “bangkok time”,bangkok #北京时间转成曼谷时间print “beijing-time to bangkok-time:”,beijing.astimezone(UTC(7)) #计算时间差时也会考虑时区的问题timespan = beijing - bangkokprint “时差:”,timespan #Output================== beijing time: 2011-11-11 00:00:00+08:00bangkok time 2011-11-11 00:00:00+07:00beijing-time to bangkok-time: 2011-11-10 23:00:00+07:00时差: -1 day, 23:00:00]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django之Model（一）--基础篇]]></title>
    <url>%2F2018%2F02%2F18%2FDjango%E4%B9%8BModel%EF%BC%88%E4%B8%80%EF%BC%89--%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Django之Model（一）–基础篇 0、数据库配置django默认支持sqlite，mysql, oracle,postgresql数据库。Django连接数据库默认编码使用UTF8，使用中文不需要特别设置。 sqlite django默认使用sqlite的数据库，默认自带sqlite的数据库驱 引擎名称：django.db.backends.sqlite3mysql 引擎名称：django.db.backends.mysqlmysql引擎配置： ‘defaults’: { ‘ENGINE’: ‘django.db.backends.mysql’, ‘NAME’:’127.0.0.1’, ‘USER’:’root’, ‘PASSWORD’:’’, } mysql引擎底层驱动的py3支持问题： mysql驱动程序 MySQLdb(mysql python),Django默认使用改驱动，但改驱动在python3下存在兼容性问题。因此使用PyMySQL。 PyMySQL(纯python的mysql驱动程序)mysql驱动python3解决方法 找到项目名文件下的init,在里面写入： import pymysql pymysql.install_as_MySQLdb() 一、Model类定义1、Model类创建下面这个模型类将作为本篇博客的基础模型，所有的实例都基于此。 from django.db import models class Publisher(models.Model): name = models.CharField(max_length=30, verbose_name=”名称”) website = models.URLField() # book_set 反向关联一对多字段的Book def __unicode__(self): return self.nameclass Author(models.Model): name = models.CharField(max_length=30) # authordetail 反向关联一对一字段AuthorDetail表 # book_set 反向关联一对多字段的Book def __unicode__(self): return self.nameclass AuthorDetail(models.Model): sex = models.BooleanField(max_length=1, choices=((0, ‘男’),(1, ‘女’),)) author = models.OneToOneField(Author) # author_id隐藏字段，正向关联一对一字段的Author对象的idclass Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher,null=True) # publisher_id隐藏字段，正向关联一对多字段的Publisher对象的id price=models.DecimalField(max_digits=5,decimal_places=2,default=10) def __unicode__(self): return self.title 自定义多对多中间表2、同步数据库模型表创建和更新的时候是要使用数据库迁移命令，使模型类的改变同步到数据库。 makemigrations #创建变更记录migrate #同步到数据库3、Model类（表）关系图：名词说明： 正向查询，从定义关系字段的类中去查询关系对象的值或值的集合。举个栗子：从AuthorDetail类中查询关联字段author对象的值。 反向查询，从本类中查询被关联对象中的值或值的集合。举个栗子：从Author对象中查询被关联字段AuthorDetail对象的值。 4、字段类型： 1、models.AutoField 自增列 = int(11) 如果没有的话，默认会生成一个名称为 id 的列，如果要显示的自定义一个自增列，必须将给列设置为主键 primary_key=True。2、models.CharField 字符串字段 必须 max_length 参数3、models.BooleanField 布尔类型=tinyint(1) 不能为空，Blank=True4、models.ComaSeparatedIntegerField 用逗号分割的数字=varchar 继承CharField，所以必须 max_lenght 参数5、models.DateField 日期类型 date 对于参数，auto_now = True 则每次更新都会更新这个时间；auto_now_add 则只是第一次创建添加，之后的更新不再改变。6、models.DateTimeField 日期类型 datetime 同DateField的参数7、models.Decimal 十进制小数类型 = decimal 必须指定整数位max_digits和小数位decimal_places8、models.EmailField 字符串类型（正则表达式邮箱） =varchar 对字符串进行正则表达式9、models.FloatField 浮点类型 = double10、models.IntegerField 整形11、models.BigIntegerField 长整形 integer_field_ranges = { ‘SmallIntegerField’: (-32768, 32767), ‘IntegerField’: (-2147483648, 2147483647), ‘BigIntegerField’: (-9223372036854775808, 9223372036854775807), ‘PositiveSmallIntegerField’: (0, 32767), ‘PositiveIntegerField’: (0, 2147483647), }12、models.IPAddressField 字符串类型（ip4正则表达式）13、models.GenericIPAddressField 字符串类型（ip4和ip6是可选的） 参数protocol可以是：both、ipv4、ipv6 验证时，会根据设置报错14、models.NullBooleanField 允许为空的布尔类型15、models.PositiveIntegerFiel 正Integer16、models.PositiveSmallIntegerField 正smallInteger17、models.SlugField 减号、下划线、字母、数字18、models.SmallIntegerField 数字 数据库中的字段有：tinyint、smallint、int、bigint19、models.TextField 字符串=longtext20、models.TimeField 时间 HH:MM[:ss[.uuuuuu]]21、models.URLField 字符串，地址正则表达式22、models.BinaryField 二进制23、models.ImageField 图片24、models.FilePathField 文件 5、字段选项： 1、null=True 数据库中字段是否可以为空2、blank=True django的 Admin 中添加数据时是否可允许空值3、primary_key = False 主键，对AutoField设置主键后，就会代替原来的自增 id 列4、auto_now 和 auto_now_add auto_now 自动创建—无论添加或修改，都是当前操作的时间 auto_now_add 自动创建—永远是创建时的时间5、choices 配置可选项GENDER_CHOICE = ( (u’M’, u’Male’), (u’F’, u’Female’), )gender = models.CharField(max_length=2,choices = GENDER_CHOICE)6、max_length 最大长度7、default 默认值8、verbose_name Admin中字段的显示名称9、name|db_column 数据库中的字段名称10、unique=True 不允许重复11、db_index = True 数据库索引12、editable=True 在Admin里是否可编辑13、error_messages=None 错误提示14、auto_created=False 自动创建15、help_text 在Admin中提示帮助信息16、validators=[] 自定义数据格式验证17、upload-to 上传文件路径 6、Model类的Meta（元数据）选项： abstract=False True就表示模型是抽象基类db_table = ‘music_album’ 自定义数库的表名称前缀get_latest_by = “datefield_name” 根据时间字段datefield_name排序，latest()和earliest()方法中使用的默认字段。db_tablespace 当前模型所使用的数据库表空间的名字。默认值是项目设置中的DEFAULT_TABLESPACE。如果后端并不支持表空间，这个选项可以忽略。ordering = [‘-fieldname’] 对象默认的顺序,字段前面带有’-‘符号表示逆序，否则正序。排序会增加查询额外开销。proxy = True 它作为另一个模型的子类，将会作为一个代理模型。unique_together 设置联合唯一。ManyToManyField不能包含在unique_together中。index_together 设置联合索引。 index_together = [ [“pub_date”, “deadline”], ] 方便起见，处理单一字段的集合时index_together = [“pub_date”, “deadline”] verbose_name 在Admin里，个易于理解的表名称，为单数：verbose_name = “pizza”verbose_name_plural 在Admin里显示的表名称，为复数：verbose_name_plural = “stories”，一般同verbose_name一同设置。 二、对象CURD操作2.1 基础对象CURD（无关联字段）1、增 方法一：author= Author(name=”鲁迅”)author.save() 方法二：创建对象并同时保存对象的快捷方法，存在关系字段时无法用此方法创建。create(**kwargs)Author.objects.create(name=u”鲁迅”) 方法三：批量创建bulk_create(objs, batch_size=None) ret=Blog.objects.bulk_create([ Author(name=”徐志摩”), Author(name=”李白”)]) 方法四：存在就获取，不存在就创建get_or_create(defaults=None,kwargs)，defaults必须为一个字典，在创建时生效的默认值；kwargs为查询条件。创建对象时，使用**kwargs和defaults共同作用，取交集，defaults优先。 updated_values={“name”:u”美猴王”}a、存在就获取，查询到结果多余一个出错MultipleObjectsReturnedret=Author.objects.get_or_create(name=u’徐志摩’,defaults=updated_values)ret：(&lt;Author: 徐志摩&gt;, False)b、不存在就创建ret=Author.objects.get_or_create(name=u’徐志摩’,defaults=updated_values)ret：(&lt;Author: 美猴王&gt;, True) 方法五：存在就更新，不存在就创建update_or_create(defaults=None, **kwargs)a、存在就更新updated_values={“name”:u”猴王”}ret=Author.objects.update_or_create(defaults=updated_values,name=u”猴子”)ret：(&lt;Author: 猴王&gt;, False)根据给出的查询条件name=u”猴子”查找对象，查询到结果就使用defaults字典去更新对象。 b、不存在就创建ret=Author.objects.update_or_create(defaults=updated_values,name=u”猴子1”)ret：(&lt;Author: 猴王&gt;, True)defaults必须为一个字典，在创建时生效的默认值；kwargs为查询条件。创建对象时，使用kwargs和defaults共同作用，取交集，defaults优先。 2、删 使用delete会查找出相关表中的有关联的数据行一并删除方法一：Author.objects.filter(name=”徐志摩”).delete()(1, {u’otest.Book_authors’: 0, u’otest.AuthorDetail’: 0, u’otest.Author’: 1})方法二：a9=Author.objects.get(name=”鲁迅”)a9.delete() 3、改 方法一：update(**kwargs)返回更新的行数，批量修改ret=Author.objects.filter(name=’秋雨’).update(name=”陶渊明”) 方法二：单条修改a7=Author.objects.get(name=”清风”)a7.name=u”宋清风”a7.save() 4、查a、查询结果非QuertSet get(**kwargs) 在使用 get() 时，如果符合筛选条件的对象超过一个，就会抛出 MultipleObjectsReturned 异常。 在使用 get() 时，如果没有找到符合筛选条件的对象，就会抛出 DoesNotExist 异常。 from django.core.exceptions import ObjectDoesNotExist try: e = Entry.objects.get(id=3) b = Blog.objects.get(id=1) except ObjectDoesNotExist: print(“Either the entry or blog doesn’t exist.”) in_bulk(id_list) 接收一个主键值列表，然后根据每个主键值所其对应的对象，返回一个主键值与对象的映射字典。 Author.objects.in_bulk([1,2,3]) {1: &lt;Author: 苍松&gt;, 2: &lt;Author: 猴王&gt;, 3: &lt;Author: 宋清风&gt;} first() 查询第一条，一般使用前先排序,或者确定其只有一条数据。last() 查询最后一条，同上 count() 返回数据库中匹配查询(QuerySet)的对象数量。 count() 不会抛出任何异常。 exists() 如果 QuerySet 包含有数据，就返回 True 否则就返回 False。这可能是最快最简单的查询方法了。 latest(field_name=None) ，根据时间字段 field_name 得到最新的对象。earliest(field_name=None)， 根据时间字段 field_name 得到最老的对象。 #F使用查询条件的值,进行数值计算 from django.db.models import FBook.objects.filter(id=1).update(price=F(‘price’)+10)b、查询结果为QuerySet 常用方法： 1、filter(**kwargs) 过滤，返回一个新的QuerySet，包含与给定的查询参数匹配的对象。 &gt;&gt;&gt;q=Author.objects.filter(name=u”苍松”) &gt;&gt;&gt; q [&lt;Author: 苍松&gt;] 2、exclude(**kwargs) 反过滤，返回一个新的QuerySet，它包含不满足给定的查找参数的对象。功能与filter相反。 3、values(*fields) 返回一个新的QuerySet，但迭代时返回字典而不是模型实例对象。 *fields表示需要取哪些字段，空表示取所有字段。 &gt;&gt;&gt; q=Author.objects.values(“id”) &gt;&gt;&gt; q [{‘id’: 1}, {‘id’: 2}, {‘id’: 3}, {‘id’: 4}, {‘id’: 18}, {‘id’: 22}, {‘id’: 24}] 4、values_list(*fields, flat=False) 返回一个新的QuerySet，但迭代时返回元组而不是模型实例对象。 *fields表示需要取哪些字段，空表示取所有字段。flat=True表示返回的结果为单个值而不是元组，多个字段时不能使用flat。 &gt;&gt;&gt; q=Author.objects.values_list(“id”) &gt;&gt;&gt; q [(1,), (2,), (3,), (4,), (18,), (22,), (24,)] &gt;&gt;&gt; q=Author.objects.values_list(“id”,flat=True) &gt;&gt;&gt; q [1, 2, 3, 4, 18, 22, 24] 5、all() 返回当前 QuerySet所有对象。 6、select_related(*field) 返回一个QuerySet，使用JOIN语句连表查询。它会在执行查询时自动跟踪外键关系，一次读取所有外键关联的字段，并尽可能地深入遍历外键连接，以减少数据库的查询。但数据关系链复杂的查询需要慎用。仅对外键生效。 7、prefetch_related(*field) prefetch_related()的解决方法是，分别查询每个表，然后用Python处理他们之间的关系。外键和多对多都生效。 8、order_by(*fields) 排序，返回一个新的QuerySet，隐式的是升序排序，-name表示根据name降序 &gt;&gt;&gt; q=Author.objects.order_by(“name”) &gt;&gt;&gt; q [&lt;Author: au&gt;, &lt;Author: 宋清风&gt;, &lt;Author: 猴王&gt;, &lt;Author: 美猴王&gt;, &lt;Author: 苍松&gt;, &lt;Author: 赵清风&gt;, &lt;Author: 陶渊明&gt;] &gt;&gt;&gt; q=Author.objects.order_by(“-name”) &gt;&gt;&gt; q [&lt;Author: 陶渊明&gt;, &lt;Author: 赵清风&gt;, &lt;Author: 苍松&gt;, &lt;Author: 美猴王&gt;, &lt;Author: 猴王&gt;, &lt;Author: 宋清风&gt;, &lt;Author: au&gt;] 9、reverse() reverse()方法返回反向排序的QuerySet。 必须对一个已经排序过的queryset(也就是q.ordered=True)执行reverse()才有效果。 10、distinct([*fields]) 去重复。返回一个在SQL 查询中使用SELECT DISTINCT 的新QuerySet。 其他方法特别的：QuerySet可以使用切片限制查询集。 切片后依旧获得QuerySet，并且不会触发数据库查询。 a=Author.objects.all()[0:2]type(a)&lt;class ‘django.db.models.query.QuerySet’&gt; 设置步长值后，获得到List类型值。触发数据库查询 a=Author.objects.all()[::2]type(a)&lt;type ‘list’&gt; 使用索引，数据对象，触发数据库查询 Author.objects.all()[0] #等价于Author.objects.all()[0:1].get()&lt;Author: 苍松&gt; 查看QuerySet的原始SQL语句关于查看QuerySet的原始SQL语句，使用查询集的query对象。 &gt;&gt;&gt; q=Author.objects.all() &gt;&gt;&gt; print q.query SELECT “otest_author”.”id”, “otest_author”.”name” FROM “otest_author” c、查询条件（双下划线） 所有使用查询条件的查询和查询集过滤的方法(get,get_or_create,filter,exclude等)都可以使用双下划线组合出更复杂的查询条件。 另一种使用双下划线的情况就是跨表条件查询单情况，见下一节关联字段中跨表查询。 查询条件格式 field__条件类型，例如a=Author.objects.get(id__exact=1) 默认为精确匹配 例如：Author.objects.get(id=1)等价于Author.objects.get(id__exact=1) 一、精确匹配 exact 精确匹配: Blog.objects.get(id__exact=1) iexact 忽略大小写的精确匹配，Blog.objects.filter(name__iexact=’blog7’)二、模糊匹配（模糊匹配，仅PostgreSQL 和 MySQL支持. SQLite的LIKE 语句不支持大小写敏感特性，因此模糊匹配对于 SQLite无法对大敏感） contains 大小写敏感的内容包含测试:Blog.objects.filter(name__contains=’blog7’) icontains 大小写不敏感的内容包含测试: startswith 大小写敏感的内容开头 Blog.objects.filter(name__startswith=”blog”) endswith 大小写敏感的内容结尾 endswith. istartswith 大小写不敏感的内容开头 startswith. iendswith 大小写不敏感的内容结尾 endswith. 三、正则匹配regex 大小写敏感的正则表达式匹配。 它要求数据库支持正则表达式语法，而 SQLite 却没有内建正则表达式支持，因此 SQLite 的这个特性是由一个名为 REGEXP 的 Python 方法实现的，所以要用到 Python 的正则库 re. Entry.objects.get(title__regex=r’^(An?|The) +’) 等价于 SQL： SELECT … WHERE title REGEXP BINARY ‘^(An?|The) +’; – MySQL SELECT … WHERE REGEXP_LIKE(title, ‘^(an?|the) +’, ‘c’); – Oracle SELECT … WHERE title ~ ‘^(An?|The) +’; – PostgreSQL SELECT … WHERE title REGEXP ‘^(An?|The) +’; – SQLite iregex 忽略大小写的正则表达式匹配。 四、范围匹配 gt 大于: Blog.objects.filter(id__gt=3) gte 大于等于. lt 小于. lte 小于等于. ne 不等于. in 位于给定列表中: Blog.objects.filter(id__in=[1,3,5]) range 范围测试: Blog.objects.filter(name__range=(‘blog1’,’blog5’)) 日期匹配： year 对 date/datetime 字段, 进行精确的 年 匹配:Polls.objects.filter(pub_date__year=2005). month day hour minute second 空值匹配 isnull True/False; 做 IF NULL/IF NOT NULL 查询:Blog.objects.filter(name__isnull=True) d、 复杂查询条件，使用Q 对象进行复杂的查询 filter() 等方法中的关键字参数查询都是一起进行“AND” 的。 如果需要执行更复杂的查询（例如OR 语句），你可以使用Q 对象。 Q对象有两种使用方式，一种使用Tree模式。另一种是使用”|”和”&amp;”符号进行与或操作。 Q构建搜索条件 from django.db.models import Q con = Q() q1 = Q() q1.connector = &apos;OR&apos; q1.children.append((&apos;id&apos;, 1)) q1.children.append((&apos;id&apos;, 2)) #等价于Q(id=1) | Q(id=2) q2 = Q() q2.connector = &apos;AND&apos; q2.children.append((&apos;id&apos;, 1)) q2.children.append((&apos;name__startswith&apos;, &apos;a&apos;)) #等价于Q(id=1) | Q(name__startswith=&apos;a&apos;) con.add(q1, &apos;AND&apos;) con.add(q2, &apos;AND&apos;)Q搜索可以和普通查询参数一起使用，但查询参数需要在最后。Author.objects.filter(q1,id=26) 2.2关联字段CURD操作（一对一，一对多，多对多表关系操作） 一对一和多对多的表关系的增删改查 a1 = Author.objects.get(name=”猴子”) a2 = Author.objects.get(name=”苍松”) a3 = Author.objects.get(name=”鲁迅”) p1=Publisher.objects.get(name=”机械出版社”) p2=Publisher.objects.get(name=”av”) ==============正向关系操作===================================================增 b1=Book(title=”红楼梦”,price=10) b1.publisher=p1或者b1.publisher_id=1 #一对多 b1.save() 先保存book对象之后才能添加多对多关系 b1.authors.add(a1,a2)或者b1.authors=[a1,a2] #多对多 b1.save() 删 b1.publisher=None或者b1.publisher_id=None #一对多 b1.authors.remove(a1,a2) 实际上是删除关系表otest_book_authors中的一条数据 #多对多 b1.authors.clear() 清空所有关系 #多对多 查询 Book.objects.filter(publisher__name=u”机械出版社”) #一对多，使用双下划线 Book.objects.filter(authors__name=u”苍松”) #多对多，使用双下划线获取字段值对象 b2.publisher #一对多，对象下面的字段值继续使用.获取。例如b2.publisher.name b1.authors.all() #多对多 ==============反向关系操作===================================================增 p1.book_set.add(b1) #一对多，会更新现有的关系。（一个对象只能有一个外键） a3.book_set.add(b1) #多对多 删 p1.book_set.remove(b1) #一对多 a3.book_set.remove(b1) #多对多 a3.book_set.clear() #多对多，清空所有关系 获取字段值对象 a2.book_set.all() #多对多 p1.book_set.all() #一对多 =============自定义中介模型方法===================================================中介模型add、create 、remove方法不可用。但是clear() 方法却是可用的，它可以清空某个实例所有的多对多关系。 三、使用原始SQL语句 Django提供两种方法使用原始SQL进行查询：一种是使用Manager.raw()方法，进行原始查询并返回模型实例；另一种直接执行自定义的SQL语句。 1、使用Manager.raw()方法Manager.raw(raw_query, params=None, translations=None) raw_query SQL查询语句。 params 查询条件参数，是list或者dict translations 字段映射表，是一个dict。Manager.raw()将查询结果映射到类字段，默认情况下映射到同名字段。返回结果是一个RawQuerySet。如果在其他的表中有一些Author数据，你可以很容易地把它们映射成Author实例。 手动指定字段映射字典。 方法一：使用AS，其他字段自动应设至Author表中的同名字段。 na=Author.objects.raw(“select name AS newname, id from otest_author”) &gt;&gt;&gt; na[0] &lt;Author_Deferred_name: 苍松&gt;. 方法二：使用translations name_map={“name”:”newname} na=Author.objects.raw(“select * from otest_author”,translations=name_map) params参数防止SQL注入 方法一：使用list &gt;&gt;&gt; na=Author.objects.raw(“select * from otest_author where id =%s”,[id]) &gt;&gt;&gt; na[0] &lt;Author: 苍松&gt; 方法二：使用dict 注意：SQLite后端不支持字典，你必须以列表的形式传递参数。 字典使用%(key)s占位符（key替换成字典中相应的key值） p_dict={&quot;id&quot;:1} na=Author.objects.raw(&quot;select * from otest_author where id =%(id)s&quot;,p_dict)2.直接执行自定义的SQL有时Manager.raw()方法并不十分好用，你不需要将查询结果映射成模型，或者你需要执行UPDATE、 INSERT以及DELETE查询。 #单数据库from django.db import connectiondef my_custom_sql(self): cursor = connection.cursor() cursor.execute(“UPDATE bar SET foo = 1 WHERE baz = %s”, [self.baz]) cursor.execute(“SELECT foo FROM bar WHERE baz = %s”, [self.baz]) row = cursor.fetchone() connection.close() return row #多数据库 from django.db import connections cursor = connections[‘my_db_alias’].cursor() # Your code here...默认情况下，Python DB API会返回不带字段的结果，这意味着你得到的是一个列表，而不是一个字典。def dictfetchall(cursor): “Returns all rows from a cursor as a dict” desc = cursor.description return [ dict(zip([col[0] for col in desc], row)) for row in cursor.fetchall() ] &gt;&gt;&gt; cursor.execute(&quot;SELECT id, parent_id FROM test LIMIT 2&quot;); &gt;&gt;&gt; cursor.fetchall() ((54360982L, None), (54360880L, None)) &gt;&gt;&gt; cursor.execute(&quot;SELECT id, parent_id FROM test LIMIT 2&quot;); &gt;&gt;&gt; dictfetchall(cursor) [{&apos;parent_id&apos;: None, &apos;id&apos;: 54360982L}, {&apos;parent_id&apos;: None, &apos;id&apos;: 54360880L}]四、分组和聚合Avg 平均值Count(expression, distinct=False） 计算个数。如果distinct=True，Count将只计算唯一的值。默认值为False。Max 最大值Min 最小值Sum 求和 方法一：使用annotate方法，先分组(group by)再聚合 from django.db.models import Count, Min, Max, Sum,Avg &gt;&gt;&gt; Book.objects.values(‘publisher’).annotate(counts_num=Count(“*”)) [{‘publisher’: 1, ‘counts_num’: 2}, {‘publisher’: 3, ‘counts_num’: 1}] &gt;&gt;&gt; Book.objects.values(&apos;publisher&apos;).annotate(Avg(&quot;price&quot;)) [{&apos;publisher&apos;: 1, &apos;price__avg&apos;: 12.5}, {&apos;publisher&apos;: 3, &apos;price__avg&apos;: 11.0}] #得到分组的多个值列表 使用values(&apos;publisher&apos;)进行group by分组后，在使用聚合函数才有意义。 默认聚合名称filedname__聚合函数名,作为聚合字段名。方法二：使用aggregate方法，先过滤再聚合 Book.objects.filter(publisher_id=1).aggregate(Count(“id”)) {‘id__count’: 2} #得到单个值 参考文档： http://python.usyiyi.cn/django/index.html中文翻译1.8.2版中文不好的同学可以看这个]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改sqlarchemy源码使其支持jdbc连接mysql]]></title>
    <url>%2F2018%2F02%2F13%2F%E4%BF%AE%E6%94%B9sqlarchemy%E6%BA%90%E7%A0%81%E4%BD%BF%E5%85%B6%E6%94%AF%E6%8C%81jdbc%E8%BF%9E%E6%8E%A5mysql%2F</url>
    <content type="text"><![CDATA[修改sqlarchemy源码使其支持jdbc连接mysql注意：本文不会将所有完整源码贴出，只是将具体的思路以及部分源码贴出，需要感兴趣的读者自己实验然后实现吆。 缘起 公司最近的项目需要将之前的部分业务的数据库连接方式改为jdbc，但由于之前的项目都使用sqlarchemy作为orm框架，该框架似乎没有支持jdbc，为了能做最小的修改并满足需求，所以需要修改sqlarchemy的源码。 基本配置介绍 sqlalchemy 版本：1.1.15 使用jaydebeapi模块调用jdbc连接mysql 前提： 1 学会使用jaydebeapi模块，使用方法具体可以参考： https://pypi.python.org/pypi/JayDeBeApi 介绍的比较详细的可以参考：http://shuaizki.github.io/language_related/2013/06/22/introduction-to-jpype.html jaydebeapi是一个基于jpype的在Cpython中可以通过jdbc连接数据库的模块。该模块的python代码很少，基本上可以分为连接部分、游标部分、结果转换部分这三个。一般来说我们可能需要修改的就是结果转换部分，比如说sqlalchemy查询时如果某条记录中含TIME字段，那么该字段一般要表现为timedelta对象。而在jaydebeapi中则返回的是字符串对象，这样在sqlalchemy中会报错的。sqlarchemy为我们实现了ORM对象与语句的转换，连接池，session(包括对线程的支持scope_session)等较为上层的逻辑，但这些东西在这里我们不需要考虑(当然创建一个连接，生成curcor还是要考虑的)，我们要考虑的仅仅是当sqlarchemy把sql语句以及参数传过来的时候我们该怎么做，以及当sql语句执行后如何对结果进行转换。 所需注意的问题1 sql语句以及参数传过来的时候我们该怎么做： 1.1 对参数进行转义，防止sql注入 2 执行完sql语句后对结果如何处理： 2.1 我们知道python的基础sql模块会对结果进行处理，比如说把NUll转换为None，把数据库中的date字段转换为python的date对象等等 2.2 一些不知道该怎么形容的数据： 当我们查询时，获取的数据对应字段的元信息 当我们update或者delete等操作时需要获取影响了多少行 当我们插入数据后，如果主键是自增字段，我们一般(可以说在sqlarchemy中这是必须)需要获取该记录的主键值 3 sqlalchemy增加代码，使其支持我们修改后的jaydebeapi 如何解决1.1解决方案： 人家pymysql咋搞，我就咋搞！ 在pymysql.corsors文件中Cursor类中有一个叫做mogrify的方法，这个方法不仅对参数转义，而且会将参数放置到sql语句中组成完整的可执行sql语句。所以偷一些代码然后稍加修改就是这样： #!/usr/bin/env python -- coding: utf-8 --from functools import partialfrom pymysql.converters import escape_item, escape_stringimport sys PY2 = sys.version_info[0] == 2 if PY2: import builtin range_type = xrange text_type = unicode long_type = long str_type = basestring unichr = builtin.unichrelse: range_type = range text_type = str long_type = int str_type = str unichr = chr def _ensure_bytes(x, encoding=”utf8”): if isinstance(x, text_type): x = x.encode(encoding) return x def _escape_args(args, encoding): ensure_bytes = partial(_ensure_bytes, encoding=encoding) if isinstance(args, (tuple, list)): if PY2: args = tuple(map(ensure_bytes, args)) return tuple(escape(arg, encoding) for arg in args) elif isinstance(args, dict): if PY2: args = dict((ensure_bytes(key), ensure_bytes(val)) for (key, val) in args.items()) return dict((key, escape(val, encoding)) for (key, val) in args.items())def escape(obj, charset, mapping=None): if isinstance(obj, str_type): return “‘“ + escape_string(obj) + “‘“ return escape_item(obj, charset, mapping=mapping) def mogrify(query, encoding, args=None): if PY2: # Use bytes on Python 2 always query = _ensure_bytes(query, encoding=encoding) if args is not None: # r = _escape_args(args, encoding) query = query % _escape_args(args, encoding) return query调用一下mogrigy函数print(mogrify(“select * from ll where a in %s and b = %s”, “utf8”, [[2, 1], 3]))2.1解决方案： 人家pymysql咋搞，我就咋搞！ 在pymysql.converters中有一个名为decoders的字典，这里面存放了mysql字段与python对象的转换关系！大概是这样 def _convert_second_fraction(s): if not s: return 0 # Pad zeros to ensure the fraction length in microseconds s = s.ljust(6, &apos;0&apos;) return int(s[:6])DATETIME_RE = re.compile(r”(\d{1,4})-(\d{1,2})-(\d{1,2})T :(\d{1,2}):(\d{1,2})(?:.(\d{1,6}))?”) def convert_datetime(obj): “””Returns a DATETIME or TIMESTAMP column value as a datetime object: &gt;&gt;&gt; datetime_or_None(&apos;2007-02-25 23:06:20&apos;) datetime.datetime(2007, 2, 25, 23, 6, 20) &gt;&gt;&gt; datetime_or_None(&apos;2007-02-25T23:06:20&apos;) datetime.datetime(2007, 2, 25, 23, 6, 20) Illegal values are returned as None: &gt;&gt;&gt; datetime_or_None(&apos;2007-02-31T23:06:20&apos;) is None True &gt;&gt;&gt; datetime_or_None(&apos;0000-00-00 00:00:00&apos;) is None True &quot;&quot;&quot; if not PY2 and isinstance(obj, (bytes, bytearray)): obj = obj.decode(&apos;ascii&apos;) m = DATETIME_RE.match(obj) if not m: return convert_date(obj) try: groups = list(m.groups()) groups[-1] = _convert_second_fraction(groups[-1]) return datetime.datetime(*[ int(x) for x in groups ]) except ValueError: return convert_date(obj)TIMEDELTA_RE = re.compile(r”(-)?(\d{1,3}):(\d{1,2}):(\d{1,2})(?:.(\d{1,6}))?”) def convert_timedelta(obj): “””Returns a TIME column as a timedelta object: &gt;&gt;&gt; timedelta_or_None(&apos;25:06:17&apos;) datetime.timedelta(1, 3977) &gt;&gt;&gt; timedelta_or_None(&apos;-25:06:17&apos;) datetime.timedelta(-2, 83177) Illegal values are returned as None: &gt;&gt;&gt; timedelta_or_None(&apos;random crap&apos;) is None True Note that MySQL always returns TIME columns as (+|-)HH:MM:SS, but can accept values as (+|-)DD HH:MM:SS. The latter format will not be parsed correctly by this function. &quot;&quot;&quot; if not PY2 and isinstance(obj, (bytes, bytearray)): obj = obj.decode(&apos;ascii&apos;) m = TIMEDELTA_RE.match(obj) if not m: return None try: groups = list(m.groups()) groups[-1] = _convert_second_fraction(groups[-1]) negate = -1 if groups[0] else 1 hours, minutes, seconds, microseconds = groups[1:] tdelta = datetime.timedelta( hours = int(hours), minutes = int(minutes), seconds = int(seconds), microseconds = int(microseconds) ) * negate return tdelta except ValueError: return NoneTIME_RE = re.compile(r”(\d{1,2}):(\d{1,2}):(\d{1,2})(?:.(\d{1,6}))?”) def convert_time(obj): “””Returns a TIME column as a time object: &gt;&gt;&gt; time_or_None(&apos;15:06:17&apos;) datetime.time(15, 6, 17) Illegal values are returned as None: &gt;&gt;&gt; time_or_None(&apos;-25:06:17&apos;) is None True &gt;&gt;&gt; time_or_None(&apos;random crap&apos;) is None True Note that MySQL always returns TIME columns as (+|-)HH:MM:SS, but can accept values as (+|-)DD HH:MM:SS. The latter format will not be parsed correctly by this function. Also note that MySQL&apos;s TIME column corresponds more closely to Python&apos;s timedelta and not time. However if you want TIME columns to be treated as time-of-day and not a time offset, then you can use set this function as the converter for FIELD_TYPE.TIME. &quot;&quot;&quot; if not PY2 and isinstance(obj, (bytes, bytearray)): obj = obj.decode(&apos;ascii&apos;) m = TIME_RE.match(obj) if not m: return None try: groups = list(m.groups()) groups[-1] = _convert_second_fraction(groups[-1]) hours, minutes, seconds, microseconds = groups return datetime.time(hour=int(hours), minute=int(minutes), second=int(seconds), microsecond=int(microseconds)) except ValueError: return Nonedef convert_date(obj): “””Returns a DATE column as a date object: &gt;&gt;&gt; date_or_None(&apos;2007-02-26&apos;) datetime.date(2007, 2, 26) Illegal values are returned as None: &gt;&gt;&gt; date_or_None(&apos;2007-02-31&apos;) is None True &gt;&gt;&gt; date_or_None(&apos;0000-00-00&apos;) is None True &quot;&quot;&quot; if not PY2 and isinstance(obj, (bytes, bytearray)): obj = obj.decode(&apos;ascii&apos;) try: return datetime.date(*[ int(x) for x in obj.split(&apos;-&apos;, 2) ]) except ValueError: return Nonedef convert_mysql_timestamp(timestamp): “””Convert a MySQL TIMESTAMP to a Timestamp object. MySQL &gt;= 4.1 returns TIMESTAMP in the same format as DATETIME: &gt;&gt;&gt; mysql_timestamp_converter(&apos;2007-02-25 22:32:17&apos;) datetime.datetime(2007, 2, 25, 22, 32, 17) MySQL &lt; 4.1 uses a big string of numbers: &gt;&gt;&gt; mysql_timestamp_converter(&apos;20070225223217&apos;) datetime.datetime(2007, 2, 25, 22, 32, 17) Illegal values are returned as None: &gt;&gt;&gt; mysql_timestamp_converter(&apos;2007-02-31 22:32:17&apos;) is None True &gt;&gt;&gt; mysql_timestamp_converter(&apos;00000000000000&apos;) is None True &quot;&quot;&quot; if not PY2 and isinstance(timestamp, (bytes, bytearray)): timestamp = timestamp.decode(&apos;ascii&apos;) if timestamp[4] == &apos;-&apos;: return convert_datetime(timestamp) timestamp += &quot;0&quot;*(14-len(timestamp)) # padding year, month, day, hour, minute, second = \ int(timestamp[:4]), int(timestamp[4:6]), int(timestamp[6:8]), \ int(timestamp[8:10]), int(timestamp[10:12]), int(timestamp[12:14]) try: return datetime.datetime(year, month, day, hour, minute, second) except ValueError: return Nonedef convert_set(s): if isinstance(s, (bytes, bytearray)): return set(s.split(b”,”)) return set(s.split(“,”)) def through(x): return x #def convert_bit(b): b = “\x00” * (8 - len(b)) + b # pad w/ zeroesreturn struct.unpack(“&gt;Q”, b)[0]# the snippet above is right, but MySQLdb doesn’t process bits,so we shouldn’t eitherconvert_bit = through def convert_characters(connection, field, data): field_charset = charset_by_id(field.charsetnr).name encoding = charset_to_encoding(field_charset) if field.flags &amp; FLAG.SET: return convert_set(data.decode(encoding)) if field.flags &amp; FLAG.BINARY: return data if connection.use_unicode: data = data.decode(encoding) elif connection.charset != field_charset: data = data.decode(encoding) data = data.encode(connection.encoding) return dataencoders = { bool: escape_bool, int: escape_int, long_type: escape_int, float: escape_float, str: escape_str, text_type: escape_unicode, tuple: escape_sequence, list: escape_sequence, set: escape_sequence, frozenset: escape_sequence, dict: escape_dict, bytearray: escape_bytes, type(None): escape_None, datetime.date: escape_date, datetime.datetime: escape_datetime, datetime.timedelta: escape_timedelta, datetime.time: escape_time, time.struct_time: escape_struct_time, Decimal: escape_object,} if not PY2 or JYTHON or IRONPYTHON: encoders[bytes] = escape_bytes decoders = { FIELD_TYPE.BIT: convert_bit, FIELD_TYPE.TINY: int, FIELD_TYPE.SHORT: int, FIELD_TYPE.LONG: int, FIELD_TYPE.FLOAT: float, FIELD_TYPE.DOUBLE: float, FIELD_TYPE.LONGLONG: int, FIELD_TYPE.INT24: int, FIELD_TYPE.YEAR: int, FIELD_TYPE.TIMESTAMP: convert_mysql_timestamp, FIELD_TYPE.DATETIME: convert_datetime, FIELD_TYPE.TIME: convert_timedelta, FIELD_TYPE.DATE: convert_date, FIELD_TYPE.SET: convert_set, FIELD_TYPE.BLOB: through, FIELD_TYPE.TINY_BLOB: through, FIELD_TYPE.MEDIUM_BLOB: through, FIELD_TYPE.LONG_BLOB: through, FIELD_TYPE.STRING: through, FIELD_TYPE.VAR_STRING: through, FIELD_TYPE.VARCHAR: through, FIELD_TYPE.DECIMAL: Decimal, FIELD_TYPE.NEWDECIMAL: Decimal,} 而在jaydebeapi中也有一些相似的代码： def _to_datetime(rs, col): java_val = rs.getTimestamp(col) if not java_val: return d = datetime.datetime.strptime(str(java_val)[:19], “%Y-%m-%d %H:%M:%S”) d = d.replace(microsecond=int(str(java_val.getNanos())[:6])) return str(d) def _to_time(rs, col): java_val = rs.getTime(col) if not java_val: return return str(java_val) def _to_date(rs, col): java_val = rs.getDate(col) if not java_val: return # The following code requires Python 3.3+ on dates before year 1900. # d = datetime.datetime.strptime(str(java_val)[:10], &quot;%Y-%m-%d&quot;) # return d.strftime(&quot;%Y-%m-%d&quot;) # Workaround / simpler soltution (see # https://github.com/baztian/jaydebeapi/issues/18): return str(java_val)[:10]def _to_binary(rs, col): java_val = rs.getObject(col) if java_val is None: return return str(java_val) def _java_to_py(java_method): def to_py(rs, col): java_val = rs.getObject(col) if java_val is None: return if PY2 and isinstance(java_val, (string_type, int, long, float, bool)): return java_val elif isinstance(java_val, (string_type, int, float, bool)): return java_val return getattr(java_val, java_method)() return to_py _to_double = _java_to_py(‘doubleValue’) _to_int = _java_to_py(‘intValue’) _to_boolean = _java_to_py(‘booleanValue’) _DEFAULT_CONVERTERS = { # see # http://download.oracle.com/javase/8/docs/api/java/sql/Types.html # for possible keys &apos;TIMESTAMP&apos;: _to_datetime, &apos;TIME&apos;: _to_time, &apos;DATE&apos;: _to_date, &apos;BINARY&apos;: _to_binary, &apos;DECIMAL&apos;: _to_double, &apos;NUMERIC&apos;: _to_double, &apos;DOUBLE&apos;: _to_double, &apos;FLOAT&apos;: _to_double, &apos;TINYINT&apos;: _to_int, &apos;INTEGER&apos;: _to_int, &apos;SMALLINT&apos;: _to_int, &apos;BOOLEAN&apos;: _to_boolean, &apos;BIT&apos;: _to_boolean} 然后我们稍微修改一下即可。 2.2解决方案 在jaydebeapi中的Cursor类中，有一个属性叫做description这个属性，通过他我们就能获取查询时表的字段的元信息 在jaydebeapi中的Cursor类中，是有rowcount这个属性的，他表示当我们进行插入更新删除操作时受影响的行数。 而在pymysql的cursors文件中的Cursor类中的_do_get_result方法中不仅仅有受影响的行数rowcount，还有lastrowid这个属性，他表示当我们插入数据且对应主键是自增字段时，最后一条数据的主键值。但是在jaydebeapi中是没有的，而这个属性在sqlalchemy中恰恰是需要的，所以我们要为jaydebeapi的Cursor类加上这个属性。代码如下： class Cursor(object): lastrowid = None rowcount = -1 _meta = None _prep = None _rs = None _description = None …此处省略部分不相关代码…def execute(self, operation, parameters=None): if self._connection._closed: raise Error() if not parameters: parameters = () self._close_last() self._prep = self._connection.jconn.prepareStatement(operation) self._set_stmt_parms(self._prep, parameters) try: is_rs = self._prep.execute() # print is_rs except: _handle_sql_exception() # print(dir(self._prep)) # 如果是查询的话 is_rs就是1 if is_rs: self._rs = self._prep.getResultSet() self._meta = self._rs.getMetaData() self.rowcount = -1 self.lastrowid = None # 插入/修改/删除时 is_rs都为0 else: self.rowcount = self._prep.getUpdateCount() self.lastrowid = int(self._prep.lastInsertID)注意：上面的代码中红色的代码是我新增的 3解决方案 sqlarchemy中底层数据库连接模块都放在dialects这个包中，这个包里面有多个包分别是mysql oracle等数据库的基本数据库连接类，因为公司只使用mysql数据库，所以仅仅做了mysql的jdbc扩展，就放到了mysql包中。大体介绍一下我们将要修改的或者用到的类： MySQLDialect 位置：sqlarchemy.dialects.mysql.base 描述：它是一个提供了对mysql数据库的连接、语句的执行等操作的基类，所以我们需要新写一个jdbcdialect类并继承它，然后重写某些方法。 为什么会用到：这个就不用多说了 ExecutionContext 位置：sqlarchemy.engine.interface 描述：通过这个东西我们可以获取当前游标的执行环境，比如说本次sql语句的执行影响了多少行，我们刚插入的一行的自增主键值是多少。他也负责把我们所写的python ORM语句转换为可以被底层数据库模块比如pymysql可以执行的东西。 创建dialect类： 我们知道使用sqlalchemy时首先需要创建一个engine，engine的第一个参数是一个URL，就像这样：mysql+pymysql://user:password@host:port/db?charset=utf8 这段URL主要配置了三项： 配置1 首先声明了我们要连接mysql数据库 配置2 然后配置了底层连接数据库的dialect(这个单词翻译过来叫方言，就好比同是汉语(连接mysql)，我们可以说山东话(pymysql)也可以说湖南话(mysqldb))模块是pymysql 配置3 配置了用户名，密码，主机地址，端口，数据库名等信息 通过查看代码我们可以看到： 上面中的配置1实际上就是说接下来要在 sqlalchemy.dialects.mysql包中获取提供数据库操作等方法的class了。 配置2实际上就是说 配置1想要找的的class我定义在了sqlalcehmy.dialects.mysql.pymysql中 配置3会作为URL类包装解析，然后作为参数传入dialect实例的create_connect_args方法，以获取数据库连接参数。 然后创建engine时还可以指定许多额外的参数，比如说连接池的配置等，这里面有几个我们需要注意的参数： 假如我们没有指定module(数据库连接底层模块)，默认会调用dialect类的类方法dbapi。 假如我们没有指定creator(与数据库建立连接的方法，一般是个函数)这个参数的话默认建立连接时会调用dialect实例的connect方法，并把create_connect_args返回的连接参数传入。 当我们第一次与数据库建立连接时，会调用dialect实例的initialize方法，这个方法会做一系列操作，比如说获取当前数据库的版本信息：dialect实例的_get_server_version_info方法；获取当前isolation级别：dialect实例的get_isolation_level方法 然后就很简单了：在sqlalchemy中找到sqlalchemy.dialects.mysql这个目录，然后新建一个名叫jaydebeapi的文件，并找到该目录下的pymysql文件，你会看到： from .mysqldb import MySQLDialect_mysqldbfrom …util import langhelpers, py3k class MySQLDialect_pymysql(MySQLDialect_mysqldb): driver = ‘pymysql’ description_encoding = None # generally, these two values should be both True # or both False. PyMySQL unicode tests pass all the way back # to 0.4 either way. See [ticket:3337] supports_unicode_statements = True supports_unicode_binds = True def __init__(self, server_side_cursors=False, **kwargs): super(MySQLDialect_pymysql, self).__init__(**kwargs) self.server_side_cursors = server_side_cursors @langhelpers.memoized_property def supports_server_side_cursors(self): try: cursors = __import__(&apos;pymysql.cursors&apos;).cursors self._sscursor = cursors.SSCursor return True except (ImportError, AttributeError): return False @classmethod def dbapi(cls): return __import__(&apos;pymysql&apos;) if py3k: def _extract_error_code(self, exception): if isinstance(exception.args[0], Exception): exception = exception.args[0] return exception.args[0]dialect = MySQLDialect_pymysql 就这一个类，我们只需要继承这个类并重写某些方法就是了。就像这样： #!/usr/bin/env python -- coding: utf-8 --import refrom .pymysql import MySQLDialect_mysqldb class MySQLDialect_jaydebeapi(MySQLDialect_mysqldb): driver = ‘jaydebeapi’ @classmethod def dbapi(cls): return __import__(&apos;jaydebeapi&apos;) def connect(self, *cargs, **cparams): # get_jdbc_conn这个方法就自己写吧，实际上就是用jaydebeapi生成一个连接，但需要注意，连接的autocommit要设置为False return get_jdbc_conn(self.dbapi, **cparams) def _get_server_version_info(self, connection): dbapi_con = connection.connection cursor = dbapi_con.cursor() cursor.execute(&quot;select version()&quot;) version = str(cursor.fetchone()[0]) cursor.close() version_list = [] r = re.compile(r&apos;[.\-]&apos;) for n in r.split(version): try: version_list.append(int(n)) except ValueError: version_list.append(n) return tuple(version_list) def _detect_charset(self, connection): &quot;&quot;&quot;Sniff out the character set in use for connection results.&quot;&quot;&quot; try: # note: the SQL here would be # &quot;SHOW VARIABLES LIKE &apos;character_set%%&apos;&quot; # print dir(connection.connection) cset_name = connection.connection.character_set_name except AttributeError: return &apos;utf8&apos; else: return cset_name()个人在修改源码中获取的知识点点1： com.mysql.jdbc.exceptions.MySQLNonTransientConnectionException: Can’t call rollback when autocommit=true 1. 当开启autocommit=true时，回滚没有意义，无论成功/失败都已经已经将事务提交 2. autocommit=false，我们需要运行conn.commit()执行事务, 如果失败则需要conn.rollback()对事务进行回滚; 点2： 尝试连接mysql时报错：Unknown system variable ‘transaction_isolation’ 这是因为我的MySQLDialect_jaydebeapi类中的_get_server_version_info方法返回写死为5.7.21版本，而在mysql的Mysqldialect类的get_isolation_level中，会判断如果版本大于等于5.7.20的话执行SELECT @@transaction_isolation，反之会执行SELECT @@tx_isolation。 于是看了看自己的mysql版本是5.7.11 ，遂改变版本号。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2018%2F01%2F28%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[python多进程首先进程是资源调度的一个最小集合，通常起一个进程，然后通过操作系统完成资源的调度。具体的细节还需要进修。。。。。 总之，通过python也可以实现多进程的。 通常，我们启动一个进程的时候，都是通过父进程来启动这个对应的子进程，在python中，我们可以通过os模块，通过os.getpid()&amp;os.getppid()来查看当前进程以及父进程的进程ID。或者我们可以通过multiprocessing模块中的创建一个进程的实例化对象后，通过调用pid这个方法来查看这个进程的进程ID。 当我们利用多进程的时候，就可以实现同一时间内做多件事情。在python中，如果我们启动多个子进程，主进程的执行和子进程的执行是没有影响的，但是，如果你想实现等待子进程执行完毕后才允许主进程执行完毕，可以利用join()方法，这里的join就相当于wait，就是，等待这个进程执行完毕，才进行下一步。 同样，当主进程执行完毕后，程序并不会退出，而是等待子进程也执行完毕才会退出，这时候，如果我们想实现主进程执行完毕后，某些子进程必须跟随者主进程的结束而结束，就可以设置某个子进程p，令其p.daemon = True，使这个进程变成守护进程，这样，当主进程执行完毕后，守护进程便会跟着退出（不管它执行完毕与否）。 更直白一点，就像古代皇帝死后，一些妃子需要陪葬，大臣不需要陪葬，这时候，这些妃子就相当于守护进程。。。 插入一段代码。。研究一下。 from multiprocessing import Processimport time def func(): time.sleep(5) print(‘^^’*5) def run(): time.sleep(1) print(‘i am running’) if name == ‘main‘: p1 = Process(target=run) p1.daemon = True p1.start() p2 = Process(target=func) p2.start() print(p2.pid) for i in range(5): time.sleep(0.1) print(&apos;i am chief process&apos;)除了这些之外，多进程还有某些特性。假设你要实现一个抢票功能，这时候，你想着so easy，每个人抢票的行为作为一个进程，不就可以实现同时抢票了吗？于是开始bangbangbang敲好了代码，测试的时候发现为什么只放出去一张票，好几个人抢到了，这是因为，起多个进程，他们有可能同时读入这个数据，导致疯狂被投诉。。。这时候，不要慌，进程里面还有进程锁这个东西，什么意思呢，就是好比你去拉屎，拉屎的时候肯定只能一人一个坑位，你进去了把门锁住，防止别人进来（除非你不是在拉屎，里面有俩人。。咳咳）。进程所就是，你要获取这个数据，你刚刚得到这个数据要进行处理，这个时候，你就把门反锁住，等你处理完了再把门打开，钥匙挂到门上，这样就避免了bug。 如果你说，一次只能进一个人这样太不友好了，我就是喜欢群P，怎么办？你可以在门上挂多点钥匙啊，这样不就可以好多人进去，限制了进去的人的数量。。。。这种可以有多个钥匙的锁称为“信号量”（semaphore）。 给你一个抢票的游戏： from multiprocessing import Processfrom multiprocessing import Lockimport timeimport randomimport json def search_ticket(): with open(‘ticket’) as f: ticket = (json.load(f)[‘count’]) print(‘there is %s tickets’%ticket) return ticketdef get_ticket(i,lock): count_ticket = search_ticket() lock.acquire() if count_ticket: print(‘%s has gotten a ticket’%i) count_ticket -= 1 with open(‘ticket’,’w’) as f: json.dump({‘count’:count_ticket},f) else: print(‘there is no ticket’) lock.release()if name == “main“: lock = Lock() for i in range(10): time.sleep(random.randint(0,1)) p = Process(target=get_ticket,args=(i,lock)) p.start() 这里用json文件代表抢票的数据库，插入random和time模块模拟实际抢票过程中的网络时延。 接下来用信号量来模拟一下拉屎这个环节，每个人都有0-3秒的时间，毕竟超过三秒都是病。。。 from multiprocessing import Processfrom multiprocessing import Semaphoreimport timeimport randomdef toilet(i,sem): sem.acquire() print(‘%s 进去拉屎了’%i) time.sleep(random.randint(0,3)) sem.release() print(‘%s 已经拉完出来了，很饱’%i) if name == ‘main‘: sem = Semaphore(4) for i in range(10): p = Process(target=toilet,args=(i,sem)) p.start() 除了锁这个概念外，进程还有一些小知识点需要掌握，队列和事件。队列很简单，无非就是先进先出，进去出去的原则，除此之外，还有qsize这个用法，但是再多进程中，这个方法有可能不准确，因为如果你读取一个数据的size的同时，又put进去了一个数据，通过队列可以实现子进程和主进程以及子进程和子进程之间的通信。 事件的话，就是可以模拟红绿灯，Event只需要记住这几个就行，clear/set/wait/is_set，当is_set为True的时候，程序是不阻塞的，默认情况下，wait是阻塞的，这样说也不大明白，还是插入一段红绿灯的例子： from multiprocessing import Eventfrom multiprocessing import Processimport time def traffic_light(e): while True: if e.is_set(): print(‘绿灯，可以通过’) time.sleep(2) e.clear() else: print(‘红灯，不能通过’) time.sleep(3) e.set()def car(i,e): e.wait() print(‘%s 车可以通过’%i) if name == ‘main‘: e = Event() p1 = Process(target=traffic_light,args=(e,)) p1.start() for i in range(20): if i % 3 == 0: time.sleep(3) else: p = Process(target=car,args=(i,e)) p.start()此例中，假设车流量比较小，每三秒过来两辆车，红绿灯启用一个进程，每辆车也视为一个进程，通过事件e的is_set的布尔值的变化实现了进程间的通信。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常处理]]></title>
    <url>%2F2018%2F01%2F13%2F%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[异常处理一 什么是异常异常就是程序运行时发生错误的信号（在程序出现错误时，则会产生一个异常，若程序没有处理它，则会抛出该异常，程序的运行也随之终止），在python中,错误触发的异常如下而错误分成两种 #语法错误示范一if #语法错误示范二def test: pass #语法错误示范三class Foo pass #语法错误示范四print(haha #TypeError:int类型不可迭代for i in 3: pass #ValueErrornum=input(“&gt;&gt;: “) #输入helloint(num) #NameErroraaa #IndexErrorl=[‘egon’,’aa’]l[3] #KeyErrordic={‘name’:’egon’}dic[‘age’] #AttributeErrorclass Foo:passFoo.x #ZeroDivisionError:无法完成计算res1=1/0res2=1+’str’ 二 异常的种类在python中不同的异常可以用不同的类型（python中统一了类与类型，类型即类）去标识，一个异常标识一种错误AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性xIOError 输入/输出异常；基本上是无法打开文件ImportError 无法引入模块或包；基本上是路径问题或名称错误IndentationError 语法错误（的子类） ；代码没有正确对齐IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5]KeyError 试图访问字典里不存在的键KeyboardInterrupt Ctrl+C被按下NameError 使用一个还未被赋予对象的变量SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了）TypeError 传入对象类型与要求的不符合UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它ValueError 传入一个调用者不期望的值，即使值的类型是正确的 ArithmeticErrorAssertionErrorAttributeErrorBaseExceptionBufferErrorBytesWarningDeprecationWarningEnvironmentErrorEOFErrorExceptionFloatingPointErrorFutureWarningGeneratorExitImportErrorImportWarningIndentationErrorIndexErrorIOErrorKeyboardInterruptKeyErrorLookupErrorMemoryErrorNameErrorNotImplementedErrorOSErrorOverflowErrorPendingDeprecationWarningReferenceErrorRuntimeErrorRuntimeWarningStandardErrorStopIterationSyntaxErrorSyntaxWarningSystemErrorSystemExitTabErrorTypeErrorUnboundLocalErrorUnicodeDecodeErrorUnicodeEncodeErrorUnicodeErrorUnicodeTranslateErrorUnicodeWarningUserWarningValueErrorWarningZeroDivisionError 三 异常处理为了保证程序的健壮性与容错性，即在遇到错误时程序不会崩溃，我们需要对异常进行处理， 如果错误发生的条件是可预知的，我们需要用if进行处理：在错误发生之前进行预防 View Code如果错误发生的条件是不可预知的，则需要用到try…except：在错误发生之后进行处理 #基本语法为try: 被检测的代码块except 异常类型： try中一旦检测到异常，就执行这个位置的逻辑 #举例try: f=open(‘a.txt’) g=(line.strip() for line in f) print(next(g)) print(next(g)) print(next(g)) print(next(g)) print(next(g))except StopIteration: f.close() #1 异常类只能用来处理指定的异常情况，如果非指定异常则无法处理。s1 = ‘hello’try: int(s1)except IndexError as e: # 未捕获到异常，程序直接报错 print e #2 多分支s1 = ‘hello’try: int(s1)except IndexError as e: print(e)except KeyError as e: print(e)except ValueError as e: print(e) #3 万能异常Exceptions1 = ‘hello’try: int(s1)except Exception as e: print(e) #4 多分支异常与万能异常 #4.1 如果你想要的效果是，无论出现什么异常，我们统一丢弃，或者使用同一段代码逻辑去处理他们，那么骚年，大胆的去做吧，只有一个Exception就足够了。 #4.2 如果你想要的效果是，对于不同的异常我们需要定制不同的处理逻辑，那就需要用到多分支了。 #5 也可以在多分支后来一个Exceptions1 = ‘hello’try: int(s1)except IndexError as e: print(e)except KeyError as e: print(e)except ValueError as e: print(e)except Exception as e: print(e) #6 异常的其他机构s1 = ‘hello’try: int(s1)except IndexError as e: print(e)except KeyError as e: print(e)except ValueError as e: print(e) #except Exception as e: print(e)else: print(‘try内代码块没有异常则执行我’)finally: print(‘无论异常与否,都会执行该模块,通常是进行清理工作’) #7 主动触发异常try: raise TypeError(‘类型错误’)except Exception as e: print(e) #8 自定义异常class EgonException(BaseException): def init(self,msg): self.msg=msg def str(self): return self.msg try: raise EgonException(‘类型错误’)except EgonException as e: print(e) #9 断言:assert 条件assert 1 == 1assert 1 == 2 #10 总结try..except 1：把错误处理和真正的工作分开来2：代码更易组织，更清晰，复杂的工作任务更容易实现；3：毫无疑问，更安全了，不至于由于一些小的疏忽而使程序意外崩溃了；四 什么时候用异常处理有的同学会这么想，学完了异常处理后，好强大，我要为我的每一段程序都加上try…except，干毛线去思考它会不会有逻辑错误啊，这样就很好啊，多省脑细胞===》2B青年欢乐多 首先try…except是你附加给你的程序的一种异常处理的逻辑，与你的主要的工作是没有关系的，这种东西加的多了，会导致你的代码可读性变差 然后异常处理本就不是你2b逻辑的擦屁股纸，只有在错误发生的条件无法预知的情况下，才应该加上try…except]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python通过future处理并发]]></title>
    <url>%2F2017%2F12%2F20%2FPython%E9%80%9A%E8%BF%87future%E5%A4%84%E7%90%86%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[future初识通过下面脚本来对future进行一个初步了解：例子1：普通通过循环的方式 1 import os 2 import time 3 import sys 4 5 import requests 6 7 8 POP20_CC = ( 9 “CN IN US ID BR PK NG BD RU JP MX PH VN ET EG DE IR TR CD FR”10 ).split()111213 BASE_URL = ‘http://flupy.org/data/flags&#39;1415 DEST_DIR = ‘downloads/‘161718 def save_flag(img,filename):19 path = os.path.join(DEST_DIR,filename)20 with open(path,’wb’) as fp:21 fp.write(img)222324 def get_flag(cc):25 url = “{}/{cc}/{cc}.gif”.format(BASE_URL,cc=cc.lower())26 resp = requests.get(url)27 return resp.content282930 def show(text):31 print(text,end=” “)32 sys.stdout.flush()333435 def download_many(cc_list):36 for cc in sorted(cc_list):37 image = get_flag(cc)38 show(cc)39 save_flag(image,cc.lower()+”.gif”)4041 return len(cc_list)424344 def main(download_many):45 t0 = time.time()46 count = download_many(POP20_CC)47 elapsed = time.time()-t048 msg = “\n{} flags downloaded in {:.2f}s”49 print(msg.format(count,elapsed))505152 if name == ‘main‘:53 main(download_many) 例子2：通过future方式实现，这里对上面的部分代码进行了复用 1 from concurrent import futures 2 3 from flags import save_flag, get_flag, show, main 4 5 6 MAX_WORKERS = 20 7 8 9 def download_one(cc):10 image = get_flag(cc)11 show(cc)12 save_flag(image, cc.lower()+”.gif”)13 return cc141516 def download_many(cc_list):17 workers = min(MAX_WORKERS,len(cc_list))18 with futures.ThreadPoolExecutor(workers) as executor:19 res = executor.map(download_one, sorted(cc_list))2021 return len(list(res))222324 if name == ‘main‘:25 main(download_many) 分别运行三次，两者的平均速度：13.67和1.59s，可以看到差别还是非常大的。 futurefuture是concurrent.futures模块和asyncio模块的重要组件从python3.4开始标准库中有两个名为Future的类：concurrent.futures.Future和asyncio.Future这两个类的作用相同：两个Future类的实例都表示可能完成或者尚未完成的延迟计算。与Twisted中的Deferred类、Tornado框架中的Future类的功能类似 注意：通常情况下自己不应该创建future，而是由并发框架(concurrent.futures或asyncio)实例化 原因：future表示终将发生的事情，而确定某件事情会发生的唯一方式是执行的时间已经安排好，因此只有把某件事情交给concurrent.futures.Executor子类处理时，才会创建concurrent.futures.Future实例。如：Executor.submit()方法的参数是一个可调用的对象，调用这个方法后会为传入的可调用对象排定时间，并返回一个future 客户端代码不能应该改变future的状态，并发框架在future表示的延迟计算结束后会改变期物的状态，我们无法控制计算何时结束。 这两种future都有.done()方法，这个方法不阻塞，返回值是布尔值，指明future链接的可调用对象是否已经执行。客户端代码通常不会询问future是否运行结束，而是会等待通知。因此两个Future类都有.add_done_callback()方法，这个方法只有一个参数，类型是可调用的对象，future运行结束后会调用指定的可调用对象。 .result()方法是在两个Future类中的作用相同：返回可调用对象的结果，或者重新抛出执行可调用的对象时抛出的异常。但是如果future没有运行结束，result方法在两个Futrue类中的行为差别非常大。对concurrent.futures.Future实例来说，调用.result()方法会阻塞调用方所在的线程，直到有结果可返回，此时，result方法可以接收可选的timeout参数，如果在指定的时间内future没有运行完毕，会抛出TimeoutError异常。而asyncio.Future.result方法不支持设定超时时间，在获取future结果最好使用yield from结构，但是concurrent.futures.Future不能这样做 不管是asyncio还是concurrent.futures.Future都会有几个函数是返回future，其他函数则是使用future,在最开始的例子中我们使用的Executor.map就是在使用future，返回值是一个迭代器，迭代器的next方法调用各个future的result方法，因此我们得到的是各个futrue的结果，而不是future本身 关于future.as_completed函数的使用，这里我们用了两个循环，一个用于创建并排定future,另外一个用于获取future的结果 1 from concurrent import futures 2 3 from flags import save_flag, get_flag, show, main 4 5 6 MAX_WORKERS = 20 7 8 9 def download_one(cc):10 image = get_flag(cc)11 show(cc)12 save_flag(image, cc.lower()+”.gif”)13 return cc141516 def download_many(cc_list):17 cc_list = cc_list[:5]18 with futures.ThreadPoolExecutor(max_workers=3) as executor:19 to_do = []20 for cc in sorted(cc_list):21 future = executor.submit(download_one,cc)22 to_do.append(future)23 msg = “Secheduled for {}:{}”24 print(msg.format(cc,future))2526 results = []27 for future in futures.as_completed(to_do):28 res = future.result()29 msg = “{}result:{!r}”30 print(msg.format(future,res))31 results.append(res)3233 return len(results)343536 if name == ‘main‘:37 main(download_many) 结果如下： 注意：Python代码是无法控制GIL，标准库中所有执行阻塞型IO操作的函数，在等待操作系统返回结果时都会释放GIL.运行其他线程执行，也正是因为这样，Python线程可以在IO密集型应用中发挥作用 以上都是concurrent.futures启动线程，下面通过它启动进程 concurrent.futures启动进程concurrent.futures中的ProcessPoolExecutor类把工作分配给多个Python进程处理，因此，如果需要做CPU密集型处理，使用这个模块能绕开GIL，利用所有的CPU核心。其原理是一个ProcessPoolExecutor创建了N个独立的Python解释器，N是系统上面可用的CPU核数。使用方法和ThreadPoolExecutor方法一样]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python协程深入理解]]></title>
    <url>%2F2017%2F12%2F10%2FPython%E5%8D%8F%E7%A8%8B%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[从语法上来看，协程和生成器类似，都是定义体中包含yield关键字的函数。yield在协程中的用法： 在协程中yield通常出现在表达式的右边，例如：datum = yield,可以产出值，也可以不产出–如果yield关键字后面没有表达式，那么生成器产出None.协程可能从调用方接受数据，调用方是通过send(datum)的方式把数据提供给协程使用，而不是next(…)函数，通常调用方会把值推送给协程。协程可以把控制器让给中心调度程序，从而激活其他的协程所以总体上在协程中把yield看做是控制流程的方式。 了解协程的过程先通过一个简单的协程的例子理解： 对上述例子的分析：yield 的右边没有表达式，所以这里默认产出的值是None刚开始先调用了next(…)是因为这个时候生成器还没有启动，没有停在yield那里，这个时候也是无法通过send发送数据。所以当我们通过next(…)激活协程后，程序就会运行到x = yield，这里有个问题我们需要注意，x = yield这个表达式的计算过程是先计算等号右边的内容，然后在进行赋值，所以当激活生成器后，程序会停在yield这里，但并没有给x赋值。当我们调用send方法后yield会收到这个值并赋值给x,而当程序运行到协程定义体的末尾时和用生成器的时候一样会抛出StopIteration异常 如果协程没有通过next(…)激活(同样我们可以通过send(None)的方式激活)，但是我们直接send，会提示如下错误： 关于调用next(…)函数这一步通常称为”预激(prime)“协程，即让协程向前执行到第一个yield表达式，准备好作为活跃的协程使用 协程在运行过程中有四个状态： GEN_CREATE:等待开始执行GEN_RUNNING:解释器正在执行，这个状态一般看不到GEN_SUSPENDED:在yield表达式处暂停GEN_CLOSED:执行结束通过下面例子来查看协程的状态： 接着再通过一个计算平均值的例子来继续理解： 这里是一个死循环，只要不停send值给协程，可以一直计算下去。通过上面的几个例子我们发现，我们如果想要开始使用协程的时候必须通过next(…)方式激活协程，如果不预激，这个协程就无法使用，如果哪天在代码中遗忘了那么就出问题了，所以有一种预激协程的装饰器，可以帮助我们干这件事 预激协程的装饰器下面是预激装饰器的演示例子： 1 from functools import wraps 2 3 4 def coroutine(func): 5 @wraps(func) 6 def primer(args,**kwargs): 7 gen = func(args,**kwargs) 8 next(gen) 9 return gen10 return primer111213 @coroutine14 def averager():15 total = 0.016 count = 017 average = None18 while True:19 term = yield average20 total += term21 count += 122 average = total/count232425 coro_avg = averager()26 from inspect import getgeneratorstate27 print(getgeneratorstate(coro_avg))28 print(coro_avg.send(10))29 print(coro_avg.send(30))30 print(coro_avg.send(5)) 关于预激，在使用yield from句法调用协程的时候，会自动预激活，这样其实与我们上面定义的coroutine装饰器是不兼容的，在python3.4里面的asyncio.coroutine装饰器不会预激协程，因此兼容yield from 终止协程和异常处理协程中为处理的异常会向上冒泡,传给next函数或send函数的调用方(即触发协程的对象)拿上面的代码举例子，如果我们发送了一个字符串而不是一个整数的时候就会报错，并且这个时候协程是被终止了 从python2.5开始客户端代码在生成器对象上调用两个方法，显示的把异常发送给协程分别为：throw和closegenerator.throw:会让生成器在暂停的yield表达式处抛出指定的异常，如果生成器处理了抛出的异常，代码会向前执行到下一个yield表达式，而产出的值会成为调用generator.throw方法代码的返回值。如果生成器没有处理抛出的异常，异常会向上冒泡，传到调用方的上下文中。generator.close:会让生成器在暂停的yield表达式处抛出GeneratorExit异常。如果生成器没有处理这个异常，或者抛出了StopIteration异常，调用方不会报错，如果收到GeneratorExit异常，生成器一定不能产出值，否则解释器会抛出RuntimeError异常。生成器抛出的异常会向上冒泡，传给调用方。下面是一个例子： 当传入我们定义的异常时不会影响协程，协程不会停止，可以继续send,但是如果是没有处理的异常的时候，就会报错，并且协程会被终止 让协程返回值通过下面的例子进行演示如何获取协程的返回值： 1 from collections import namedtuple 2 3 4 Result = namedtuple(“Result”,”colunt average”) 5 6 7 def averager(): 8 total = 0.0 9 count = 010 average = None11 while True:12 term = yield13 if term is None:14 break15 total += term16 count+=117 average = total/count18 return Result(count,average)1920 coro_avg = averager()21 next(coro_avg)22 coro_avg.send(10)23 coro_avg.send(30)24 coro_avg.send(5)25 try:26 coro_avg.send(None)27 except StopIteration as e:28 result = e.value29 print(result) 这样就可以获取到最后的结果： 其实相对来说上面这种方式获取返回值比较麻烦，而yield from 结构会自动捕获StopIteration异常，这种处理方式与for循环处理StopIteration异常的方式一样，循环机制使我们更容易理解处理异常，对于yield from来说，解释器不仅会捕获StopIteration异常，还会把value属性的值变成yield from表达式的值 关于yield from在生成器gen中使用yield from subgen()时，subgen会获得控制权，把产出的值传给gen的调用方，即调用方可以直接控制subgen,同时，gen会阻塞，等待subgen终止 yield from x表达式对x对象所做的第一件事是，调用iter(x),从中获取迭代器，因此x可以是任何可迭代的对象 下面是yield from可以简化yield表达式的例子： 1 def gen(): 2 for c in “AB”: 3 yield c 4 for i in range(1,3): 5 yield i 6 7 print(list(gen())) 8 9 def gen2():10 yield from “AB”11 yield from range(1,3)1213 print(list(gen2())) 这两种的方式的结果是一样的，但是这样看来yield from更加简洁，但是yield from的作用可不仅仅是替代产出值的嵌套for循环。yield from的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器连接起来，这样二者可以直接发送和产出值，还可以直接传入异常，而不用再像之前那样在位于中间的协程中添加大量处理异常的代码 通过yield from还可以链接可迭代对象委派生成器在yield from 表达式处暂停时，调用方可以直接把数据发给子生成器，子生成器再把产出产出值发给调用方，子生成器返回之后，解释器会抛出StopIteration异常，并把返回值附加到异常对象上，此时委派生成器会恢复。 下面是一个完整的例子代码 1 from collections import namedtuple 2 3 4 Result = namedtuple(‘Result’, ‘count average’) 5 6 7 # 子生成器 8 def averager(): 9 total = 0.010 count = 011 average = None12 while True:13 term = yield14 if term is None:15 break16 total += term17 count += 118 average = total/count19 return Result(count, average)202122 # 委派生成器23 def grouper(result, key):24 while True:25 result[key] = yield from averager()262728 # 客户端代码，即调用方29 def main(data):30 results = {}31 for key,values in data.items():32 group = grouper(results,key)33 next(group)34 for value in values:35 group.send(value)36 group.send(None) #这里表示要终止了3738 report(results)394041 # 输出报告42 def report(results):43 for key, result in sorted(results.items()):44 group, unit = key.split(‘;’)45 print(‘{:2} {:5} averaging {:.2f}{}’.format(46 result.count, group, result.average, unit47 ))4849 data = {50 ‘girls;kg’:51 [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5],52 ‘girls;m’:53 [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43],54 ‘boys;kg’:55 [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3],56 ‘boys;m’:57 [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],58 }596061 if name == ‘main‘:62 main(data) 关于上述代码着重解释一下关于委派生成器部分，这里的循环每次迭代时会新建一个averager实例，每个实例都是作为协程使用的生成器对象。 grouper发送的每个值都会经由yield from处理，通过管道传给averager实例。grouper会在yield from表达式处暂停，等待averager实例处理客户端发来的值。averager实例运行完毕后，返回的值会绑定到results[key]上，while 循环会不断创建averager实例，处理更多的值 并且上述代码中的子生成器可以使用return 返回一个值，而返回的值会成为yield from表达式的值。 关于yield from的意义关于yield from 六点重要的说明： 子生成器产出的值都直接传给委派生成器的调用方(即客户端代码)使用send()方法发送给委派生成器的值都直接传给子生成器。如果发送的值为None,那么会给委派调用子生成器的next()方法。如果发送的值不是None,那么会调用子生成器的send方法，如果调用的方法抛出StopIteration异常，那么委派生成器恢复运行，任何其他异常都会向上冒泡，传给委派生成器生成器退出时，生成器(或子生成器)中的return expr表达式会出发StopIteration(expr)异常抛出yield from表达式的值是子生成器终止时传给StopIteration异常的第一个参数。yield from 结构的另外两个特性与异常和终止有关。传入委派生成器的异常，除了GeneratorExit之外都传给子生成器的throw()方法。如果调用throw()方法时抛出StopIteration异常，委派生成器恢复运行。StopIteration之外的异常会向上冒泡，传给委派生成器如果把GeneratorExit异常传入委派生成器，或者在委派生成器上调用close()方法，那么在子生成器上调用clsoe()方法，如果它有的话。如果调用close()方法导致异常抛出，那么异常会向上冒泡，传给委派生成器，否则委派生成器抛出GeneratorExit异常]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于python协程中aiorwlock 使用问题]]></title>
    <url>%2F2017%2F11%2F20%2F%E5%85%B3%E4%BA%8Epython%E5%8D%8F%E7%A8%8B%E4%B8%ADaiorwlock%20%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[最近工作中多个项目都开始用asyncio aiohttp aiomysql aioredis ,其实也是更好的用python的协程，但是使用的过程中也是遇到了很多问题，最近遇到的就是 关于aiorwlock 的问题，在使用中碰到了当多个协程同时来请求锁的时候 在其中一个还没释放锁的时候，另外一个协程也获取到锁，这里进行整理，也希望知道问题你解决方法的，一起讨论一下，正好最近经常用到协程的东西，所以准备建一个群，也欢迎大家一起进来讨论python协程的内容，群号：692953542 关于场景描述数据库的要操作的表的信息为： id name nickname count flag crdate1 800100 aa 100 1 2018-11-18 10:07:222 800101 bb 200 1 2018-11-18 10:07:23 当多个请求都到数据库操作接口程序的时候，针对同一个name的count进行增加或者减少，就要保证操作的同一个时刻只有一个可以去获取count的值并进行update操作，所以我是在这一步增加了锁，因为使用aiohttp写的，所以想要在这里也用了aiorwlock，但是在我测试的过程中发现了，当一个协程获取锁还没释放锁的时候，另外一个协程也获取到锁，下面我是具体的代码 程序代码核心的处理类： class CntHandler(object): def __init__(self, db, loop): self.db = db self.loop = loop self.company_lock = {} def response(self, request, msg): peer = request.transport.get_extra_info(&apos;peername&apos;) logging.info(&quot;request url[%s] from[%s]: %s&quot;, request.raw_path, peer, msg) origin = request.headers.get(&quot;Origin&quot;) if origin is not None: headers = {&quot;Access-Control-Allow-Origin&quot;: origin, &quot;Access-Control-Allow-Credentials&quot;: &quot;true&quot;} resp = web.Response(text=util.dictToJson(msg), content_type=&apos;application/json&apos;, headers=headers) else: resp = web.Response(text=util.dictToJson(msg), content_type=&apos;application/json&apos;) return resp async def cnt_set(self, request): &quot;&quot;&quot; 用于设置company表中的count值 :param request: :return: &quot;&quot;&quot; post = await request.post() logging.info(&apos;post %s&apos;, post) company_name = post.get(&quot;company&quot;) cnt = post.get(&quot;cnt&quot;) sql = &quot;update shield.company set count=%s where name=%s&quot; args_values = [cnt, company_name] rwlock = self.company_lock.get(company_name, &quot;&quot;) if not rwlock: rwlock = aiorwlock.RWLock(loop=self.loop) self.company_lock[company_name] = rwlock async with rwlock.writer: msg = dict() po_sql = &quot;select * from shield.company where name=%s&quot; po = await self.db.get(po_sql, company_name) if not po: # 找不到企业 logging.error(&quot;not found company name [%s]&quot;, company_name) msg[&quot;code&quot;] = 404 msg[&quot;code&quot;] = &quot;not found company&quot; return self.response(request, msg) res = await self.db.execute(sql, args_values) if not isinstance(res, int): logging.error(&quot;sql update is err:&quot;, res) msg[&quot;code&quot;] = 403 msg[&quot;reason&quot;] = &quot;set fail&quot; return self.response(request, msg) logging.info(&quot;company [%s] set cnt [%s] is success&quot;, company_name, cnt) msg[&quot;code&quot;] = 200 msg[&quot;reason&quot;] = &quot;ok&quot; return self.response(request, msg) async def cnt_inc(self, request): &quot;&quot;&quot; 用于增加company表中的count值 :param request: :return: &quot;&quot;&quot; post = await request.post() logging.info(&apos;post %s&apos;, post) company_name = post.get(&quot;company&quot;) cnt = int(post.get(&quot;cnt&quot;, 0)) rwlock = self.company_lock.get(company_name, &quot;&quot;) if not rwlock: rwlock = aiorwlock.RWLock(loop=self.loop) self.company_lock[company_name] = rwlock async with rwlock.writer: uuid_s = uuid.uuid1().hex logging.debug(&quot;[%s]---[%s]&quot;, uuid_s, id(rwlock)) msg = dict() sql = &quot;select * from shield.company where name=%s&quot; po = await self.db.get(sql, company_name) if not po: # 找不到企业 logging.error(&quot;not found company name [%s]&quot;, company_name) msg[&quot;code&quot;] = 404 msg[&quot;code&quot;] = &quot;not found company&quot; return self.response(request, msg) old_cnt = po.get(&quot;count&quot;) po_cnt = int(po.get(&quot;count&quot;)) res = po_cnt + cnt update_sql = &quot;update shield.company set count=%s where name=%s&quot; args_values = [res, company_name] update_res = await self.db.execute(update_sql, args_values) if not isinstance(update_res, int): # 数据库update失败 logging.error(&quot;sql update is err:&quot;, update_res) msg[&quot;code&quot;] = 403 msg[&quot;reason&quot;] = &quot;inc fail&quot; return self.response(request, msg) logging.info(&quot;uuid [%s] lock [%s] company [%s] inc cnt [%s] old cnt [%s] true will is [%s] success&quot;, uuid_s,id(rwlock), company_name, cnt, old_cnt, res) msg[&quot;code&quot;] = 200 msg[&quot;reason&quot;] = &quot;ok&quot; return self.response(request, msg) async def cnt_dec(self, request): &quot;&quot;&quot; 用于减少company表中count的值 :param request: :return: &quot;&quot;&quot; post = await request.post() logging.info(&apos;post %s&apos;, post) company_name = post.get(&quot;company&quot;) cnt = int(post.get(&quot;cnt&quot;, 0)) rwlock = self.company_lock.get(company_name, &quot;&quot;) if not rwlock: rwlock = aiorwlock.RWLock(loop=self.loop) self.company_lock[company_name] = rwlock async with rwlock.writer: uuid_s = uuid.uuid1().hex logging.debug(&quot;[%s]---[%s]&quot;, uuid_s, id(rwlock)) msg = dict() sql = &quot;select * from shield.company where name=%s&quot; po = await self.db.get(sql, company_name) if not po: # 找不到企业 logging.error(&quot;not found company name [%s]&quot;, company_name) msg[&quot;code&quot;] = 404 msg[&quot;code&quot;] = &quot;not found company&quot; return self.response(request, msg) po_cnt = int(po.get(&quot;count&quot;)) old_cnt = po.get(&quot;count&quot;) if po_cnt == 0: logging.error(&quot;company [%s] cnt is 0&quot;, company_name) msg[&quot;code&quot;] = 400 msg[&quot;reason&quot;] = &quot;cnt is 0&quot; return self.response(request, msg) if po_cnt &lt; cnt: # 数据库余额不足 logging.error(&quot;company [%s] count is not enough&quot;, company_name) msg[&quot;code&quot;] = 405 msg[&quot;reason&quot;] = &quot;count is not enough&quot; return self.response(request, msg) res = po_cnt - cnt update_sql = &quot;update shield.company set count=%s where name=%s&quot; args_values = [res, company_name] update_res = await self.db.execute(update_sql, args_values) if not isinstance(update_res, int): # 执行update 失败 logging.error(&quot;sql update is err:&quot;, update_res) msg[&quot;code&quot;] = 403 msg[&quot;reason&quot;] = &quot;inc fail&quot; return self.response(request, msg) logging.info(&quot;uuid [%s] lock [%s] company [%s] dec cnt [%s] old cnt [%s] true will is [%s] success&quot;,uuid_s,id(rwlock), company_name, cnt, old_cnt, res) msg[&quot;code&quot;] = 200 msg[&quot;reason&quot;] = &quot;ok&quot; return self.response(request, msg)上面代码出问题的代码是在增加和减少的时候： async with rwlock.writer:在一个协程还没有释放锁的时候，另外一个操作也就进来了，到之后我在测试并发的时候，对同一个name的count进行操作导致最后的count值不符合的问题 可能是我本身代码的问题，或者我哪里处理的不对，欢迎大家一起讨论 这个完整的代码地址：https://github.com/pythonsite/test_aiorwlock]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一篇文章让你明白python的装饰器]]></title>
    <url>%2F2017%2F11%2F12%2F%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0%E8%AE%A9%E4%BD%A0%E6%98%8E%E7%99%BDpython%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[在看闭包问题之前先来看看关于python中作用域的问题 变量作用域 对于上述代码中出现错误，肯定没什么疑问了，毕竟b并没有定义和赋值，当我们把代码更改如下后： 再看一个例子： 首先这个错误已经非常明显：说在赋值之前引用了局部变量b 可能很多人觉得会打印10然后打印6，其实这里就是涉及到变量作用域的问题当Python编译函数的的定义体的时候，它判断b是局部变量，毕竟在函数中有b = 9表示给b赋值了，所以python会从本地环境获取b，当我们调用方法执行的时候，定义体会获取并打印变量a的值，但是当尝试获取b的值的时候发现b没有绑定值，所以要想让上述代码运行还可以把b设置为全局变量，或者把b赋值放到调用之前 函数对象的作用域python中一切皆对象，同其他对象一样,函数对象也有其使用的范围即函数对象的作用域。在python中我们通过def定义函数，函数对象的作用域与def所在的层级相同，通过下面代码进行理解： def func1(): def func2(x): return 2*x print(func2(5)) func1()print(func2(5)) 这个例子中我们在def func1函数内可以调用fun2,但是我们在外面是无法调用到func2的，所以结果为看到如下： 闭包关于闭包主要有下面两种说法： 闭包是符合一定条件的函数，定义为：闭包是在其词法上下文中引用了自由变量的函数闭包是由函数与其相关的引用环境组合而成的实体。定义为：在实现绑定时，需要创建一个能显示表示引用环境的东西，并将它与相关的子程序捆绑在一起，这样捆绑起来的整体称为闭包个人觉得第二种说法更准确，闭包只是在形式上表现像函数，实际不是函数。我们对函数的定义是：一些可执行的代码，这些代码在函数定义后就确定了，不会在执行时发生变化，所以一个函数只有一个实例。 闭包在运行的时候可以有多个实例，不同的引用环境和相同的环境组合可以产生不同的实例。 这里有一个词：引用环境，其实引用环境就是在执行运行的某个时间点，所有处于活跃状态的变量所组成的集合，这里的变量是指变量的名字和其所代表的对象之间的联系。 可以使用闭包语言的特点： 函数可以作为另外一个函数的返回值或者参数，还可以作为一个变量的值。函数可以嵌套使用而认为闭包是函数的有一句话是：闭包是指延伸了作用域的函数，其中包含函数定义体中引用。但是不在定义体中定义的非全局变量。 上面这种说法个人觉得也是一种理解方式 相信看了这些概念也还是不好理解，还是通过下面例子更好理解： 先实现一种计算平均值的方法： 从结果我们可以看出这里保存了每次的历史值换一种方法实现： 实现了第一种相同的效果，对这种方法分析：通常我们会认为我们调用avg(10)的时候make_averager函数已经返回了，而它的本地作用域也一去不复返，但这里其实series是自由变量，是指未在本地作用域绑定的变量我们可以通过print(dir(avg)),看到如下结果：其实这里面保存着均布变量和自由变量的名称，我们可以通过下面方法查看： eries的绑定在返回的avg函数的closure属性中这或许就是有的人会认为闭包一种函数。闭包会保留定义函数时存在的自由变量的绑定，这样调用函数时虽然定义作用域不能用了，但是仍能使用那些绑定 关于nonlocal刚开始了解闭包之后，如果尝试使用这种编程方式容易出现以下错误使用例子： def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averager先来看一下错误提示： 这个例子中和我们上面使用的不同之处是：这里的count和total是数字，是不可变类型，而之前的例子中series是一个列表是可变类型所以这里重新回到了最开始说的作用域问题了，当我们在averager中使用count += 1的时候其实就是count = count + 1,这样就是在averager函数定义体中对count进行赋值，count就变成了局部变量。 问题小结：当时数字，字符串，元组等不可变类型时，只能读取不能更新，如果使用类似count += 1就会隐式的把count变成局部变量，所以开始例子中使用series，我们后面的操作是append并且列表还是可变对象 不过python3引入了一个新的关键词nonlocal，通过它把变量标记为自由变量，这样我们把上面这个错误的例子简单更改： def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count,total count += 1 total += new_value return total / count return averager到这里装饰器的前奏就说完了，下面就是装饰器，我个人觉得装饰器只是闭包的一种应用，闭包在很多情况下都是一种非常好的变成技巧 装饰器关于装饰器本来是想重新整理一下，看了自己之前整理的博客，已经挺详细的，就把连接直接放这里了http://www.pythonsite.com/?p=113]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫从入门到放弃（二）之爬虫的原理]]></title>
    <url>%2F2017%2F10%2F25%2Fpython%E7%88%AC%E8%99%AB%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E7%88%AC%E8%99%AB%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[在上文中我们说了：爬虫就是请求网站并提取数据的自动化程序。其中请求，提取，自动化是爬虫的关键！下面我们分析爬虫的基本流程 爬虫的基本流程发起请求通过HTTP库向目标站点发起请求，也就是发送一个Request，请求可以包含额外的header等信息，等待服务器响应 获取响应内容如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型 解析内容得到的内容可能是HTML,可以用正则表达式，页面解析库进行解析，可能是Json,可以直接转换为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理 保存数据保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件 什么是Request,Response浏览器发送消息给网址所在的服务器，这个过程就叫做HTPP Request 服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应的处理，然后把消息回传给浏览器，这个过程就是HTTP Response 浏览器收到服务器的Response信息后，会对信息进行相应的处理，然后展示 Request中包含什么？请求方式 主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONSGET和POST的区别就是：请求的数据GET是在url中，POST则是存放在头部 GET:向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中。其中一个原因是GET可能会被网络蜘蛛等随意访问 POST:向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。 HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。 PUT：向指定资源位置上传其最新内容。 OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*’来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。 DELETE：请求服务器删除Request-URI所标识的资源。 请求URL URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 URL的格式由三个部分组成：第一部分是协议(或称为服务方式)。第二部分是存有该资源的主机IP地址(有时也包括端口号)。第三部分是主机资源的具体地址，如目录和文件名等。 爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。 请求头 包含请求时的头部信息，如User-Agent,Host,Cookies等信息，下图是请求请求百度时，所有的请求头部信息参数 请求体请求是携带的数据，如提交表单数据时候的表单数据（POST） Response中包含了什么所有HTTP响应的第一行都是状态行，依次是当前HTTP版本号，3位数字组成的状态代码，以及描述状态的短语，彼此由空格分隔。 响应状态 有多种响应状态，如：200代表成功，301跳转，404找不到页面，502服务器错误 1xx消息——请求已被服务器接收，继续处理2xx成功——请求已成功被服务器接收、理解、并接受3xx重定向——需要后续操作才能完成这一请求4xx请求错误——请求含有词法错误或者无法被执行5xx服务器错误——服务器在处理某个正确请求时发生错误 常见代码： 200 OK 请求成功 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在，eg：输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 301 目标永久性转移 302 目标暂时性转移响应头 如内容类型，类型的长度，服务器信息，设置Cookie,如下图 响应体 最主要的部分，包含请求资源的内容，如网页HTMl,图片，二进制数据等 能爬取什么样的数据网页文本：如HTML文档，Json格式化文本等图片：获取到的是二进制文件，保存为图片格式视频:同样是二进制文件其他：只要请求到的，都可以获取 如何解析数据直接处理Json解析正则表达式处理BeautifulSoup解析处理PyQuery解析处理XPath解析处理 关于抓取的页面数据和浏览器里看到的不一样的问题出现这种情况是因为，很多网站中的数据都是通过js，ajax动态加载的，所以直接通过get请求获取的页面和浏览器显示的不同。 如何解决js渲染的问题？ 分析ajaxSelenium/webdriverSplashPyV8,Ghost.py 怎样保存数据文本：纯文本，Json,Xml等 关系型数据库：如mysql,oracle,sql server等结构化数据库 非关系型数据库：MongoDB,Redis等key-value形式存储]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫从入门到放弃（一）之初识爬虫]]></title>
    <url>%2F2017%2F10%2F15%2Fpython%E7%88%AC%E8%99%AB%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E5%88%9D%E8%AF%86%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[整理这个文档的初衷是自己开始学习的时候没有找到好的教程和文本资料，自己整理一份这样的资料希望能对小伙伴有帮助 什么是爬虫？网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。 其实通俗的讲就是通过程序去获取web页面上自己想要的数据，也就是自动抓取数据 爬虫可以做什么？你可以爬去妹子的图片，爬取自己想看看的视频。。等等你想要爬取的数据，只要你能通过浏览器访问的数据都可以通过爬虫获取 爬虫的本质是什么？模拟浏览器打开网页，获取网页中我们想要的那部分数据 浏览器打开网页的过程：当你在浏览器中输入地址后，经过DNS服务器找到服务器主机，向服务器发送一个请求，服务器经过解析后发送给用户浏览器结果，包括html,js,css等文件内容，浏览器解析出来最后呈现给用户在浏览器上看到的结果 所以用户看到的浏览器的结果就是由HTML代码构成的，我们爬虫就是为了获取这些内容，通过分析和过滤html代码，从中获取我们想要资源（文本，图片，视频…..）]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从yield 到yield from再到python协程]]></title>
    <url>%2F2017%2F09%2F25%2F%E4%BB%8Eyield%20%E5%88%B0yield%20from%E5%86%8D%E5%88%B0python%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[从yield 到yield from再到python协程yield 关键字def fib(): a, b = 0, 1 while 1: yield b a, b = b, a+b yield 是在：PEP 255 – Simple Generators 这个pep引入的 yield 只能在函数内部使用，包含yield语句的函数称为生成器函数 当调用生成器函数时，并不会执行函数体中的代码，而是返回一个生成器对象 每次调用生成器对象的next()方法时，才会执行生成器函数中的代码，直到遇到yield 或者return 语句。 如果遇到yield 语句， 怎会挂起函数的运行状态，并将yield 右边的表达式的值返回给next()的调用者， 挂起的时候会保存所有本地状态，包括局部变量，指令指针和内部堆栈信息，这样当下次再次调用next()时, 看起来yield 部分就像是调用了一个外部调用一样，可以接着往下执行 注意：try/ finnally 结构中的try子句中不允许使用yield语句， 问题是因为无法保证生成器被恢复，因此无法保证finally块将被执行 yield from 关键字yield from关键字是在：PEP 380 – Syntax for Delegating to a Subgenerator 中提出的 用于生成器将其部分操作委托给另外一个生成器，这允许将包含yield的一段代码分解出来并放在另外一个生成器中，此外，允许子生成器返回一个值，这个值可供委派生成器使用 上述描述听起来可能还是不是特别清楚，我们先看一下语法： yield from yield from expr 表达式中，做的第一件事就是调用iter(expr) 从中获取迭代器，因此expr可以是任何可迭代的对象 通过下面的下例子把yield 和yield from 做对比 from collections import namedtuple Result = namedtuple(“Result”, “count average”) li = [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5] 子生成器def averager(): total = 0.0 count = 0 average = None while True: term = yield if term is None: break total += term count += 1 average = total/count return Result(count, average) 委派生成器def grouper(result, key): while True: result[key] = yield from averager() 调用方def main(): results = {} group = grouper(results, “kg”) next(group) for value in li: group.send(value) group.send(None) if name == “main“: main() yield from的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器连接起来，这样二者可以直接发送和产出值，还可以直接传入异常 yield from 的六个重要意义关于yield from 六点重要的说明： 子生成器产出的值都直接传给委派生成器的调用方(即客户端代码)使用send()方法发送给委派生成器的值都直接传给子生成器。如果发送的值为None,那么会给委派调用子生成器的next()方法。如果发送的值不是None,那么会调用子生成器的send方法，如果调用的方法抛出StopIteration异常，那么委派生成器恢复运行，任何其他异常都会向上冒泡，传给委派生成器生成器退出时，生成器(或子生成器)中的return expr表达式会出发StopIteration(expr)异常抛出yield from表达式的值是子生成器终止时传给StopIteration异常的第一个参数。yield from 结构的另外两个特性与异常和终止有关。传入委派生成器的异常，除了GeneratorExit之外都传给子生成器的throw()方法。如果调用throw()方法时抛出StopIteration异常，委派生成器恢复运行。StopIteration之外的异常会向上冒泡，传给委派生成器如果把GeneratorExit异常传入委派生成器，或者在委派生成器上调用close()方法，那么在子生成器上调用clsoe()方法，如果它有的话。如果调用close()方法导致异常抛出，那么异常会向上冒泡，传给委派生成器，否则委派生成器抛出GeneratorExit异常 python协程Python的生成器函数和python的协程非常接近 ,但并不完全 - 因为生成器然允许暂停执行以生成值，但是不提供在执行恢复时传递的值或异常。 并且生成器不允许在try / finally块的try部分中暂停执行，因此使中止的协程很难在其自身之后进行清理。 将yield重新定义为表达式，而不是语句。当前的yield语句将成为一个yield值表达式，其值将被丢弃。每当通过正常的next（）调用恢复生成器时，yield表达式的值为None为generator-iterators 添加了一个新的方法send(), 它可以恢复生成器并发送给生成一个值，该值称为yield - expression的结果，send（）方法返回生成器产生的下一个值，如果生成器退出而不产生另一个值，则引发StopIteration。为generator-iterators 添加了一个新的方法throw(), 它在生成器暂停时引发异常，并返回生成器产生的下一个值，如果生成器退出而不产生另一个值，则引发StopIteration（如果生成器没有捕获传入的异常，或者引发另外的一个异常，那么该异常会传播给调用者）为generator-iterators 添加了一个新的方法close(), 在生成器暂停的位置引发一个GeneratorExit 异常，如果一个生成器引发了StopIteration 异常或者GeneratorExit 异常， close()方法将返回给它的调用者，如果生成是yield 一个值，会引发RuntimeError 异常。如果一个生成器引发了任何其他异常，则会传给他的调用者 ，如果生成器，由于异常退出或者已经正常退出，那么close()不执行任何操作。确保了当生成器被垃圾回收的时候执行close()因为垃圾回收或者clsoe被调用将允许允许yield在try / finally块中使用。send方法send方法只有一个参数，就是发送值到生成器，调用send(None)相当于调用生成器的next()方法 因为我们开始执行生成器函数的时候，并没有实际执行生成器函数中的代码而是返回一个生成器对象，所以我们需要调用next()或者send(None)来激活协程 与next（）方法一样，send（）方法返回generator-iterator产生的下一个值，如果生成器正常退出或已经退出，则引发StopIteration。如果生成器引发未捕获的异常，它将传播到send（）的调用者 throw方法让生成器在被挂起的位置抛出指定的异常，如果生成器捕获了异常并且返回的另外一个值，那么这个值就是g.throw()返回的值 如果生成器没有捕获异常，那么throw()将会引发传递相同的异常，如果生成器引发了另外一个异常，throw调用将引发异常，总之throw()的行为类似next()或者send() 除了它在挂起的时候引发异常。如果生成器已经处于关闭状态，throw() 只会引发它传递的异常，而不执行任何生成器的代码 generator.throw:会让生成器在暂停的yield表达式处抛出指定的异常，如果生成器处理了抛出的异常，代码会向前执行到下一个yield表达式，而产出的值会成为调用generator.throw方法代码的返回值。如果生成器没有处理抛出的异常，异常会向上冒泡，传到调用方的上下文中。generator.close:会让生成器在暂停的yield表达式处抛出GeneratorExit异常。如果生成器没有处理这个异常，或者抛出了StopIteration异常，调用方不会报错，如果收到GeneratorExit异常，生成器一定不能产出值，否则解释器会抛出RuntimeError异常。生成器抛出的异常会向上冒泡，传给调用方。 早期的python协程，语法上协程和生成器看起来也非常类似，也是通过yield关键字如：num = yield def simple_coroutine(): print(“coroutine start”) x = yield print(“coroutine receive [%s]” %x) coroutine = simple_coroutine()print(coroutine)next(coroutine)coroutine.send(888) 上面的例子中yield 的右边没有表达式，所以默认产出的值为None,通过之前将yield 关键字的时候我们已经知道当我们执行函数的时候 并不会运行生成器函数中的代码，而是返回一个生成器对象，所以我们需要通过调用next(…)来激活协程，这个时候开始运行生成器函数， 当运行到x = yield的时候，yield的右边如果有表达式，则会先进行右边表达式的计算，然后再进行赋值，所以当上面函数执行next()之后， 程序会停在yield那里，当我们调用send方法后yield会收到这个值并赋值给x,而当程序运行到协程定义体的末尾时和用生成器的时候一样会抛出StopIteration异常 如果协程没有通过next(…)激活(同样我们可以通过send(None)的方式激活)，但是我们直接send，则会出错 关于调用next(…)函数这一步通常称为”预激(prime)“协程，即让协程向前执行到第一个yield表达式，准备好作为活跃的协程使用 协程在运行过程中有四个状态： GEN_CREATE:等待开始执行GEN_RUNNING:解释器正在执行，这个状态一般看不到GEN_SUSPENDED:在yield表达式处暂停GEN_CLOSED:执行结束]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于python单例的常用几种实现方法]]></title>
    <url>%2F2017%2F09%2F06%2F%E5%85%B3%E4%BA%8Epython%E5%8D%95%E4%BE%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关于python单例的常用几种实现方法这两天在看自己之前写的代码，所以正好把用过的东西整理一下，单例模式，在日常的代码工作中也是经常被用到， 所以这里把之前用过的不同方式实现的单例方式整理一下 装饰器的方式这种方式也是工作中经常用的一种，用起来也比较方便，代码实现如下 def Singleton(cls): _instance = {} def _singleton(*args, **kwargs): if cls not in _instance: _instance[cls] = cls(*args, **kwargs) return _instance[cls] return _singleton如果我们工作的一个类需要用单例就通过类似下面的方式实现即可： @Singletonclass A(object): def __init__(self, x): self.x = x我个人还是挺喜欢这种方式的 类的方式实现这里其实有一些问题就需要注意了，先看一下可能出现的错误代码 class Member(object): @classmethod def instance(cls, *args, **kwargs): if not hasattr(Member, &quot;_instance&quot;): Member._instance = Member(*args, **kwargs) return Member._instance乍一看这个类好像已经实现了单例，但是这里有一个潜在的问题，就是如果是多线程的情况，这样写就会有问题了，尤其是在当前类的初始化对象里有一些耗时操作时候 例如下面代码： #! /usr/bin/env python3 .-- coding:utf-8 .--import timeimport threadingimport random class Member(object): def __init__(self): time.sleep(random.randint(1,3)) @classmethod def instance(cls, *args, **kwargs): if not hasattr(Member, &quot;_instance&quot;): Member._instance = Member(*args, **kwargs) return Member._instancedef task(arg): obj = Member.instance() print(obj) for i in range(5): t = threading.Thread(target=task, args=[i,]) t.start()这段代码的执行结果会出现实例化了多个对象，导致你写的单例就没起到作用 当然自然而然我们会想起加锁，通过锁来控制，所以我们将上面代码进行更改： #! /usr/bin/env python3 .-- coding:utf-8 .--import timeimport threadingimport random class Member(object): _instance_lock = threading.Lock() def __init__(self): i = random.randint(1, 3) print(i) time.sleep(i) @classmethod def instance(cls, *args, **kwargs): with Member._instance_lock: if not hasattr(Member, &quot;_instance&quot;): Member._instance = Member(*args, **kwargs) return Member._instancedef task(): obj = Member.instance() print(obj) for i in range(5): threading.Thread(target=task,).start() 但是上面的代码还有一个问题，就是当我们已经实例化过之后每次调用instance都会去请求锁，所以这点并不好，所以我们将这部分代码再次更改： @classmethod def instance(cls, *args, **kwargs): if not hasattr(Member, &quot;_instance&quot;): with Member._instance_lock: if not hasattr(Member, &quot;_instance&quot;): Member._instance = Member(*args, **kwargs) return Member._instance这样就很好的实现一个可以多线程使用的单例]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket 理论知识整理]]></title>
    <url>%2F2017%2F08%2F31%2FWebSocket%20%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近工作用到websocket, 之前虽然也用到了一些简单的东西，但是并没有认真整理一下。所以这次准备了解一下WebSocket. WebSocket产生的背景WebSocket是一种在单个TCP连接上进行全双工通信的协议. 这意味着双方可以同时进行通信和交换数据 对于我们都非常熟悉的HTTP协议，通信只能通过客户端发起，无法做到服务器主动向客户端推送消息这样如果在服务端出问题的时候，客户端想要知道就比较麻烦，笨的办法就是我们采用轮询的方式，每隔一段时间问一下服务端：“喂，你还在么，你怎么样了，还没死吧？” 从而来确定服务端的一些状态变化。 关于轮询：其实就是客户端在指定的时间间隔向服务器发送请求 但是我们都知道这种笨办法是非常浪费资源的。而WebSocket也可以说就是这样诞生了 为什么我们需要web socketInternet was conceived to be a collection of Hypertext Mark-up Language (HTML) pages linking one another to form a conceptual web of information. During the course of time, static resources increased in number and richer items, such as images and began to be a part of the web fabric. Server technologies advanced which allowed dynamic server pages - pages whose content was generated based on a query. Soon, the requirement to have more dynamic web pages lead to the availability of Dynamic Hypertext Mark-up Language (DHTML). All thanks to JavaScript. Over the following years, we saw cross frame communication in an attempt to avoid page reloads followed by HTTP Polling within frames. However, none of these solutions offered a truly standardized cross browser solution to real-time bi-directional communication between a server and a client. This gave rise to the need of Web Sockets Protocol. It gave rise to full-duplex communication bringing desktop-rich functionality to all web browsers. WebSocket 长啥样 ？我们还是用HTTP来对比，我们通常访问一个网站如google,我们会在浏览器中输入：http://www.google.com或者：https://www.google.com 其实webSocket和http也非常类似，如下图： web socket 是HTML5 规范的一部分， 允许网页和远程主机之间进行全双工通信，该协议实现以下好处： 通过单个连接而不是两个连接使用全双工减少不必要的网络流量和延迟通过代理和防火墙进行流式传输，同时支持上游和下游通信 websocket 和http 对比 websockets 角色Events and Actions有四个主要的API eventsOpenMessageCloseError 每一个事件都分别通过实现onopen onmessage onclose 和onerror函数来处理 Open一旦客户端和服务器之间建立了连接，就会从web socket 实例触发open 事件，这个被称为客户端和服务器之间的初始握手一旦建立连接就会触发的事件称为onopen事件 Message通常发生在服务器发送一些数据的时候触发该消息事件服务器发送给客户端的消息可以包括纯文本消息，二进制数据或者图像。但是无论哪种数据都会触发onmessage函数 close该事件标志着服务器和客户端之间通信结束当触发onclose事件之后可以关闭连接，同时标记中通信结束，服务器和客户端之间无法进一步传输消息 erroronerror 事件之后总是随后终止连接 Actions当我们想要发生某事件的时候做一些操作，通过用户显示调用的方法有：send()close() 参考连接：http://www.ruanyifeng.com/blog/2017/05/websocket.htmlhttps://www.tutorialspoint.com/websockets/index.htm]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次python 内存泄漏解决过程]]></title>
    <url>%2F2017%2F08%2F20%2F%E4%B8%80%E6%AC%A1python%20%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E8%A7%A3%E5%86%B3%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[最近工作中慢慢开始用python协程相关的东西，所以用到了一些相关模块，如aiohttp, aiomysql, aioredis等，用的过程中也碰到的很多问题，这里整理了一次内存泄漏的问题 通常我们写python程序的时候也很少关注内存这个问题（当然可能我的能力还有待提升），可能写c和c++的朋友会更多的考虑这个问题，但是一旦我们的python程序出现了 内存泄漏的问题，也将是一件非常麻烦的事情了，而最近的一次代码中也碰到了这个问题，不过好在最后内存溢出不是我代码的问题，而是所用到的一个包出现了内存的问题，下面我通过一个简单的代码模拟出内存的问题，然后也会将解决的过程描述一下，希望能帮助到遇到同样问题的朋友。 一、复现问题其实这次主要是在使用aiohttp写一个接口的时候出现的问题，其实复现出问题非常容易，我们实现一个简单的接受post请求接口的服务端，然后实现一个并发的客户端来访问这个接口，来查看内存的情况 注意： 这个问题是在一个包的特定版本出现的：multidict==4.5.1,我在整理这个文章2个小时前作者已经修复了这个问题发布了4.5.2版本，已经修复了内存的问题，并且我也进行了测试验证 服务端代码： from aiohttp import web async def hello(request): return web.json_response(await request.json()) app = web.Application()app.add_routes([web.post(‘/‘, hello)])web.run_app(app) 客户端代码： import asyncioimport aiohttp async def foo(times): data = {‘foo’: 1} async with aiohttp.ClientSession() as session: for x in range(times): resp = await session.post(‘http://localhost:8080&#39;, json=data) if not x % 100: print(await resp.json()) loop = asyncio.get_event_loop()loop.run_until_complete(foo(100000))loop.close() 因为我的代码是在linux上跑的，或者mac上我们都可以通过htop非常方面的实时查看我们程序内存的占用情况，我们先将服务端启动，查看一下我们此时的内存情况可以看到占用的 非常少，当我们打开客户端之后，再次观察我们可以看到内存不断增长，及时我们客户端运行完毕内存也不会降低。 当客户端结束之后的内存： 如果客户端不停止的话内存会一直涨，最后的结果就是把你的系统内存吃完，然后被系统杀掉你的进程。 二、解决内存泄漏的过程像上面的例子是一个非常简单的程序，不复杂我们也并没有做上面复杂的操作就是一个简单的接受post请求的服务端，但是如果是在实际的项目中我们可能会写非常复杂的业务逻辑，那到时候我们又如何找到是哪里导致的内存问题，当我碰到这个问题的时候，其实我和很多接触python不久的人差不多，也是不知道怎么查这种问题，各种百度各种查，也找到了好多推荐的工具，memory_profiler库，objgraph库，graphviz工具，但是都没有帮助我迅速的找到问题点在哪里，最后看到标准库中的tracemalloc,地址：https://docs.python.org/3/library/tracemalloc.html 通过这个包很快帮我找到了内存泄漏的地方 接下来按照官网的方法我将代码进行改写，来测试到底哪里的问题导致的内存泄漏，更改后的服务端代码为： from aiohttp import webimport tracemalloc async def hello(request): return web.json_response(await request.json()) async def get_info(request): snapshot2 = tracemalloc.take_snapshot() top_stats = snapshot2.compare_to(snapshot1, ‘lineno’) print(top_stats) return web.Response(text=”ok”) if name == ‘main‘: app = web.Application() app.add_routes( [ web.post(‘/‘, hello), web.get(“/get_info”, get_info) ] ) tracemalloc.start() snapshot1 = tracemalloc.take_snapshot() web.run_app(app) 注意print(top_stats)这行打印的结果最后要关注 其实这里就是新增加了一个路由get_info, 我们启动服务端之后开启客户端，当我们客户端运行完毕之后，可以看到内存已经涨上去了，并且没有不会释放，这个时候，可以直接通过浏览器访问get_info这个路由看看print打印的内容，这里将会打印出你程序运行到这个时候那一行的代码内存增长的比较多，进行一次排序，前面的几个其实都是需要你关注的，因为这里数据较多，我就只打印如下前几个数据 &lt;StatisticDiff traceback=&lt;Traceback (,)&gt; size=116500672 (+116500672) count=300004 (+300004)&gt;, &lt;StatisticDiff traceback=&lt;Traceback (,)&gt; size=11400000 (+11400000) count=200000 (+200000)&gt;, &lt;StatisticDiff traceback=&lt;Traceback (,)&gt; size=8000000 (+8000000) count=100000 (+100000)&gt;, &lt;StatisticDiff traceback=&lt;Traceback (,)&gt; size=5500000 (+5500000) count=100000 (+100000)&gt;, &lt;StatisticDiff traceback=&lt;Traceback (,)&gt; size=5300608 (+5300608) count=100001 (+100001)&gt;, 我们拿第一行来说，我们可以非常清楚的指导web_response的56行代码导致内存增长的最多，当然如果是我们复杂的项目也可以通过类似的方法，这样就可以非常快捷的找到我们代码中哪些地方会造成内存溢出，便于排查问题，我们点进去看看这行代码： 我们找到最终行，这个时候我们大致就可以看出哪里的问题了，我们接着看 CIMultiDict class CIMultiDict(MultiDict): def _title(self, key): return key.title()我们可以看到这个它继承 MultiDict 其实这里我们已经应该知道问题就是处在这个MultiDict上了 而这个最终其实最终就是MultiDict这个包，问题出在了这个包上，这个项目是在这里维护的：https://github.com/aio-libs/multidict 查看这个包的时候看到了，果然有人和我遇到了同样的问题，问题就是出在这里了，已经有人提交了bug https://github.com/aio-libs/multidict/issues/307 不过不得不说国外的程序员真的是热爱自己的职业，很快这个问题得到了aio-libs小组中人的回应，问题也在我整理这个博客的时候被修复了，在最新的版本：4.5.2中已经测试没有内存泄漏的问题 总结在这里处理的过程中，其实发现了自己很多的不足，查找问题的方式，以及遇到这种问题的解决思路，不过经过这次，至少下次遇到同样的问题，自己能很快的去查找 以及解决问题，还有就是针对https://docs.python.org/3/library/tracemalloc.html这个库的使用，也推荐大家多了解一下。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境快速安装python3]]></title>
    <url>%2F2017%2F08%2F11%2Flinux%E7%8E%AF%E5%A2%83%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85python3%2F</url>
    <content type="text"><![CDATA[linux环境快速安装python3之前在linux上安装python3的时候，为了让不影响linux环境原有的python2的环境，选择的方法都是下载对应的linux环境的python包，不过 这里需要注意的是,不要更改linux默认输入python 进入python2的方法，自己安装python3的时候做软链接的时候要做成python3,否则系统的很多工具依赖于python2, 而他们使用的是python2 但是编译安装还是非常慢的，推荐下面一种快速的方法 快速安装python3这里以centos 为例子，先执行： yum install python36因为我这里环境用的是python3.6 你通过上面这个命令就可以把最新python3.6.6安装到linux。接着执行 yum install python36-devel这个是安装一些依赖包，这个命令执行完毕之后，python3环境就安装成功了，但是这个时候你在linux上输入python3 是不可以进入python3的， 这种方法是默认安装到了/usr/bin/目录下，需要做软链接 ln -s /usr/bin/python3.6 /usr/bin/python3还差最后一步，安装pip python3 -m ensurepip到此为止我们的python3环境就安装好了，相对于编译安装的方法还是快了很多的，尤其当你需要同时对多台机器同时安装python3的环境时候，通过这种方式就可以快速批量安装 所有的努力都值得期许，每一份梦想都应该灌溉！]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git & github]]></title>
    <url>%2F2017%2F07%2F28%2Fgit%20%26%20github%2F</url>
    <content type="text"><![CDATA[为什么要使用版本控制 1、举例说明： 1）假设你在的公司要上线一个新功能，你们开发团队为实现这个新功能，写了大约5000行代码，上线没2 天，就发现这个功能用户并不喜欢，你老板让你去掉这个功能，你怎么办？ 2）你说简单，直接把5000行代码去掉就行了，但是我的亲，说的简单，你的这个功能写了3周时间，但你 还能记得你是新增加了哪5000行代码么？ 3）所以你急需要一个工具，能帮你记录每次对代码做了哪些修改，并且可以轻易的把代码回滚到历史上的 某个状态。 这个神奇的工具就叫做版本控制 2、版本控制工具主要实现2个功能 1）版本管理 在开发中，这是刚需，必须允许可以很容易对产品的版本进行任意回滚，版本控制工具实现这个功能的 原理简单来讲，就是你每修改一次代码，它就帮你做一次快照 2）协作开发 a. 一个复杂点的软件，往往不是一个开发人员可以搞定的，公司为加快产品开发速度，会招聘一堆跟 你一样的开发人员开发这个产品 b. 拿微信来举例，现在假设3个人一起开发微信，A开发联系人功能，B开发发文字、图片、语音通讯 功能，C开发视频通话功能， B和C的功能都是要基于通讯录的 c. 你说简单，直接把A开发的代码copy过来，在它的基础上开发就好了，可以，但是你在他的代码基 础上开发了2周后，这期间A没闲着，对通讯录代码作了更新，此时怎么办？你和他的代码不一致 d. 此时我们知道，你肯定要再把A的新代码拿过来替换掉你手上的旧通讯录功能代码， 现在人少，3 个人之间沟通很简单，但想想，如果团队变成30个人呢？ e. 来回这样copy代码，很快就乱了， 所以此时亟需一个工具，能确保一直存储最新的代码库，所有 人的代码应该和最新的代码库保持一致 2. 常见版本管理工具介绍 1、SVN –CollabNet Subversion 1. SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑， 2. 所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。 3. 集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。 2、GIT 1. Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库， 2. 这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。 3. 你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了，当然也可以推送到git的仓库中，比如GitHub git、GitHub和SVN比较1． Git 1、git是一个版本管理工具，是可以在你电脑不联网的情况下，只在本地使用的一个版本管理工具 2、其作用就是可以让你更好的管理你的程序，比如你原来提交过的内容，以后虽然修改了，但是通过git这个 工具，可以把你原来提交的内容重现出来 2. GitHub 1、github是一个网站，就是每个程序员自己写的程序，可以在github上建立一个网上的仓库， 2、你每次提交的时候可以把代码提交到网上，，这样你的每次提交，别人也都可以看到你的代码，同时别人也 可以帮你修改你的代码，这种开源的方式非常方便程序员之间的交流和学习 3、github是一个非常适合程序员交流的网站，很多国际上的技术大牛都在github上有自己的开源代码，其他 人只要申请个账号就可以随意的看到这些大牛写的程序 总结： git可以认为是一个软件，能够帮你更好的写程序，github则是一个网站，这个网站可以帮助程序员之间互相交流和学习。 3. SVN与git比较 1、Git是分布式的，SVN是集中式的，好处是跟其他同事不会有太多的冲突，自己写的代码放在自己电脑上， 一段时间后再提交、合并，也可以不用联网在本地提交 2、Git下载下来后，在本地不必联网就可以看到所有的log，很方便学习，SVN却需要联网； 3、Git鼓励分Branch，而SVN，说实话，我用Branch的次数还挺少的，SVN自带的Branch merge我还真没用过 4、SVN在Commit前，我们都建议是先Update一下，跟本地的代码编译没问题，并确保开发的功能正常后再 提交，这样其实挺麻烦的，有好几次同事没有先Updat，Commit了，发生了一些错误，Git可能这种情况 本地git基本使用命令 创建git版本库 1、版本库又名仓库，英文名repository，你可以简单理解成一个目录， 2、这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都 可以追踪历史，或者在将来某个时刻可以“还原”。 3、所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目 4、瞬间Git就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository） 5、细心的读者可以发现当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，没事千万不要 手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 mkdir s15_gitpro #先创建一个项目 cd s15_gitpro/ #切换到这个项目目录 git init #初始化这个 2. 工作区、暂存区、代码仓库 1、工作区： 就是你在电脑上看到的目录，比如目录下testgit里的文件(.git隐藏目录版本库除外)。 2、暂存区 : 暂存区就是文件夹 .git中的一个小部分（.git文件夹就是版本库） 3、版本库：工作区有一个隐藏目录.git,这个不属于工作区，这是版本库， 版本库中还有Git为我们 自动创建了第一个分支master,以及指向master的一个指针HEAD 4、 把文件添加到版本库分为以下三步： 1）vim Readme #工作区（Working Zone） 比如在mkdir s15_gitpro下执行创建文件命令 2）git add #暂存区（Stage zone） 3）git commit #代码仓库（Repository master） 只有提交到代码库才能被git管理 3、本地git基本命令 1、将文件添加到仓库 git add Readme #指定将Readme文件添加到暂存区 git add . #将当前目录中的所有文件全部添加到暂存区 git status #查看更改了哪些，创建了哪些，哪些没有添加到仓库，哪些添加到了仓库 git status diff readme #查看readme文件具体修改了哪些 git commit -m “commit tag” # git commit告诉Git，把文件提交到仓库-m后面输入的是本次提交的说明(版本名字) 说明： # 执行git commit 命令时必须配置用户信息 git config –global user.name “Tom Git” git config –global user.email tom@example.com 2、回滚 git log #查看所有提交到仓库的版本记录: git log -2 git reflog #查看所有操作记录（状态的md5值和改变的值） git reset –hard d9e0ed0 #回到指定版本（d9e0ed0是创建版本的MD5值得前6位或者7位） git reset –hard HEAD^ #回到上一个版本 注：这样可以回到第一次提交到仓库的状态，但再使用git log看不到其他几次的md5值了 3、撤销修改 vim Readme #我们在Readme文件中写了一些错误的代码 git add . #然后又一不小心将文件从工作区提交到了 stage区 git reset HEAD Readme #将Readme中刚提交到 stage区 的代码撤回到工作区 git status #查看目前工作区状态 git checkout – Readme #将Readme在工作区错误的代码丢弃 4、删除操作（两种方法） 方法1：这种方法需要执行git add . rm Readme git add . git commit -m “delete file by git rm” git reset –hard HEAD^ 方法2：这种方法可以省去执行git add . git rm Readme git commit -m “delete file by git rm” git reset –hard HEAD^ 注： 在没有git commit前，使用 git checkout – Readme 可以恢复删除的文件（Readme） 5、强制使用master覆盖本地代码 $ git fetch –all $ git reset –hard origin/master $ git pull 使用git操作GitHub1、登录https://github.com 创建一个github项 2、选择创建一个新项目，还是将本地的项目推到github这个项目里 3、将本地已有的项目上传到GitHub中 1）这里我们选择使用HTTPS的方法，讲本地已有项目提交到GitHub中 2）在本地Git对应的项目下执行这条命令，配置，将以后的内容提交到这个路径下即：GitHub对应的项目中 git remote add origin https://github.com/Tom7481079/s15_proj.git #设置代码提交url路径 git remote rm origin # 删除设置的代码提交url路径 3）将本地的项目推到GitHub中（需要输入GitHub网站的用户名和密码） git push -u origin master # 将本地代码push到GitHub中 4）然后刷新页面即可在网页中看到我们本地的项目上传成功了 4、GitHub中文件与本地项目不一致时上传到GitHub报错解决方法 方法一：使用强制push的方法，这样会使远程修改丢失，一般是不可取的，尤其是多人协作开发的时候。 git push -u origin master -f 方法二：push前先将远程repository修改pull下来 git pull origin master # 先将GitHub中的文件下载到本地 git push -u origin master # 然后在push到GitHub中 注：执行git pull是如果只有GitHub中修改，会自动合并，如果本地也有修改必须手动合并才能正常git push 方法三：若不想merge远程和本地修改，可以先创建新的分支： git branch [name] # 新建一个分支 git push -u origin [name] # 提交到分支中 配置win10当前用户对GitHub所有项目有权限（git push不必输入密码）1、使用命令生成公钥和私钥（在git命令行中执行） ssh-keygen.exe #生成一对公钥和私钥 C:\Users\tom.ssh #这里是生成的秘钥地址 2、将公钥上传到GitHub中 1. 第一步 2. 第二步 3. 第三步 4. 第四步 3. 更改本地push方式为ssh 1. 配置完公钥后还需要将GitHub项目的push方式改成ssh 更改本地push方式为ssh 1） vim .git/config 2） 将已有的https的路径替换成ssh模式，即上面复制的地址 #url = https://github.com/Tom7481079/s15_proj.git url = git@github.com:Tom7481079/s15_proj.git 3、此时再执行品push命令时就不会再让输入用户名和密码了 git push origin master]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask在虚拟环境中做开发]]></title>
    <url>%2F2017%2F07%2F21%2FFlask%20%E5%9C%A8%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%B9%8B%E4%B8%AD%E5%81%9A%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[Flask 在虚拟环境之中做开发虚拟环境可以搭建独立的python运行环境, 使得单个项目的运行环境与其它项目互不影响比如一些项目基于python2.0，而另外一些项目却基于python3.0,同时这些项目必须在同一台服务器上部署，使用virtualenv就可以完美解决这个问题 虚拟环境安装命令pip install virtualenv 进入项目安装虚拟目录的命令virtualenv venv 激活虚拟环境activate 离开虚拟环境deactivate]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket模块]]></title>
    <url>%2F2017%2F07%2F13%2Fsocket%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[socket模块网络编程其他篇目录：1.1 socket理论部分1.2 socket处理单个连接 和 同时接受多个连接1.3 socket实现远程执行命令，下载文件1.4 通过socket实现简单的ssh 和 处理连包问题 1.1 socket理论部分 返回顶部 1、socket起源 1. socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，对于文件用【打开】【读写】【关闭】模式来操作。 2. socket就是该模式的一个实现，socket即是一种特殊的文件，一些socket函数就是对其进行的操作（读/写IO、打开、关闭） 3. 基本上，Socket 是任何一种计算机网络通讯中最基础的内容 4. 例如当你在浏览器地址栏中输入 http://www.cnblogs.com/ 时，你会打开一个套接字，然后连接到 http://www.cnblogs.com/ 并读取响应的页面然后然后显示出来 5. socket和file的区别： 1) file模块是针对某个指定文件进行【打开】【读写】【关闭】 2) socket模块是针对 服务器端 和 客户端Socket 进行【打开】【读写】【关闭】 2、socket.socket( family, type ) 实例化一个socket类 1. 实例化需要3个参数 1） 第一个是地址簇(默认是socket.AF_INET) 2） 第二个是流(socket.SOCK_STREAM, 默认值)或数据报(socket.SOCK_DGRAM)套接字 3） 第三个是实用的协议（默认是0 使用默认即可），对于一个普通的套接字不需要提供任何参数 2. 第一个参数: Socket Families(地址簇) 类似于OSI七层的网络层 1）socket.AF_UNIX unix本机进程间通信(本机没有网卡时本机件进程自己起socket通信) 2）socket.AF_INET IPV4 3）socket.AF_INET6 IPV6 3. 第二个参数: Socket Families(地址簇) 类似于OSI七层的传输层 1）socket.SOCK_STREAM #for tcp 2）socket.SOCK_DGRAM #for udp 3）socket.SOCK_RAW #原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以； 其次，SOCK_RAW也可以处理特殊的IPv4报文；此外利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头 4）socket.SOCK_RDM #是一种可靠的UDP形式，即保证交付数据报但不保证顺序。SOCK_RAM用来提供对原始协议低级访问， 在需要执行某些特殊操作时使用，如发送ICMP报文。SOCK_RAM通常仅限于高级用户或管理员运行的程序使用 3、socket对象可以使用的所有方法 1、 sk.bind(address) s.bind(address) 将套接字绑定到地址。address地址的格式取决于地址族。在AF_INET下，以元组（host,port）的形式表示地址。 2、 sk.listen(backlog) 开始监听传入连接。backlog指定在拒绝连接之前，可以挂起的最大连接数量。 backlog等于5，表示内核已经接到了连接请求，但服务器还没有调用accept进行处理的连接个数最大为5 这个值不能无限大，因为要在内核中维护连接队列 3、 sk.setblocking(bool) 是否阻塞（默认True），如果设置False，那么accept和recv时一旦无数据，则报错。 4、 sk.accept() 接受连接并返回（conn,address）,其中conn是新的套接字对象，可以用来接收和发送数据。address是连接客户端的地址。 接收TCP 客户的连接（阻塞式）等待连接的到来 5、 sk.connect(address) 连接到address处的套接字。一般，address的格式为元组（hostname,port）,如果连接出错，返回socket.error错误。 6、 sk.connect_ex(address) 同上，只不过会有返回值，连接成功时返回 0 ，连接失败时候返回编码，例如：10061 7、 sk.close() 关闭套接字 8、 sk.recv(bufsize[,flag]) 接受套接字的数据。数据以字符串形式返回，bufsize指定最多可以接收的数量。flag提供有关消息的其他信息，通常可以忽略。 9、 sk.recvfrom(bufsize[.flag]) 与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 10、 sk.send(string[,flag]) 将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。即：可能未将指定内容全部发送。 11、 sk.sendall(string[,flag]) 将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。 内部通过递归调用send，将所有内容发送出去。 12、 sk.sendto(string[,flag],address) 将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。该函数主要用于UDP协议。 13、 sk.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如 client 连接最多等待5s ） 14、 sk.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 15、 sk.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) 16、 sk.fileno() 套接字的文件描述符 4、TCP三层握手 1、第一次握手建立连接时，客户端发送SYN包到服务器，其中包含客户端的初始序号seq=x，并进入SYN_SENT状态，等待服务器确认。 2、第二次握手服务器收到请求后，必须确认客户的数据包。同时自己也发送一个SYN包，即SYN+ACK包，此时服务器进入SYN_RECV状态。 3、第三次握手客户端收到服务器的SYN+ACK包，向服务器发送一个序列号(seq=x+1)，确认号为ack(客户端)=y+1，此包发送完毕，客户端和服务器进入ESTAB_LISHED(TCP连接成功)状态，完成三次握手。 1、第一次挥手首先，客户端发送一个FIN，用来关闭客户端到服务器的数据传送，然后等待服务器的确认。其中终止标志位FIN=1，序列号seq=u。 2、第二次挥手服务器收到这个FIN，它发送一个ACK，确认ack为收到的序号加一。 3、第三次挥手关闭服务器到客户端的连接，发送一个FIN给客户端。 4、第四次挥手客户端收到FIN后，并发回一个ACK报文确认，并将确认序号seq设置为收到序号加一。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 1.2 socket处理单个连接 和 同时接受多个连接 返回顶部 1、socket通信原理图解 2、简单的socket列子：处理单个连接 123456789101112import socket#服务器端server = socket.socket() # 创建一个socket对象server.bind((&apos;&apos;,6969)) # host=’ ‘或者‘0.0.0.0’都表示所有计算机都可以访问服务器server.listen() # server.listen(5) 表示同时可以有五个客户连接到服务器conn,addr = server.accept() # c, addr = s.accept() 这是一种赋变量的方法，会组成一个元组。 # 代表只要客户端提供地址（addr）就可以接受外部的链接data = conn.recv(1024) # 收到客户端数据：recv默认是阻塞的，收到数据为空则死循环conn.send(data.upper()) # 服务器返回数据给client端server.close() 123456789import socket#客户端client = socket.socket() # 创建socket对象client.connect((&apos;localhost&apos;,6969)) # 连接到服务端client.send(&quot;在python3中socket仅可以传送byte类型&quot;.encode(&quot;utf-8&quot;)) # 发送数据到服务端data = client.recv(1024) # 接收服务端返回数据print(&apos;客户端收到返回数据为：&apos;,data.decode())client.close() 3、socket实现同时连接多个客户端 1. 这里仅能实现，同时连接多个客户端，但同一时刻仅能有一个客户端与服务的通信，其他客户端阻塞状态 2. 只有当前一个客户端断开连接后，后面的客户端才能与服务端发消息 3. 经测试，只能在Linux服务器中实现，在Windows中，断开客户端后服务端就会应发异常 #-- coding:utf-8 -- import socket #服务器端python3.5 server = socket.socket()server.bind((‘’,6969)) #绑定要监听的端口server.listen(5) #我要监听这个端口，最大连接数为：5print(“我要开始等电话了”)while True: #当一个客户端断开后程序就回到这里，等待下一个连接 conn,addr = server.accept() #等电话打进来 conn是打电话实例，addr电话号码 #conn就是客户端连过来而在服务器端为其生成的连接实例，这里其实就是为每个客户端生成一个实例区分 while True: data = conn.recv(1024) #data是接收到的从客户端发送过来的数据 print(&quot;rect:&quot;,data) #将客户端发送过来的数据在服务器端打印 if not data: #如果客户端断开连接，data就为空，就跳出内层while循环，到达外层循环等待下一个客户端连接 print(&quot;host has lost!!!&quot;) break conn.send(data.upper()) #将客户端发送来的数据变成大写，然后再发送给客户端server.close() #-- coding:utf-8 -- import socket #客户端python3.5 client = socket.socket() #声明socket类型，同时生成socket连接对象client.connect((‘1.1.1.3’,6969)) #指定要连接的服务器地址和端口号 while True: msg = input(“&gt;&gt;:”).strip() #输入要发送给服务器的消息 if len(msg) == 0:continue #如果客户端输入空格，让客户端继续输入 client.send(msg.encode(“utf-8”)) #将输入的消息发送到服务器端 data = client.recv(1024) #接收服务器端发送过来的数据，每次1024字节 print(‘client_recv:’,data) #将从服务器端接收的数据在客户端打印出来client.close() 1.3 socket实现远程执行命令，下载文件 返回顶部 1、使用socket实现远程执行命令 #-- coding:utf-8 -- import socket,os #服务器端python3.5server = socket.socket()server.bind((‘1.1.1.3’,6969)) #绑定要监听的端口server.listen(5) #我要监听这个端口print(“我要开始等电话了”)while True: conn,addr = server.accept() #等电话打进来 conn是打电话实例，addr电话号码 while True: #conn就是客户端连过来而在服务器端为其生成的连接实例 data = conn.recv(1024) data = data.decode() print(&quot;接收的命令:&quot;,data) if not data: print(&quot;host has lost!!!&quot;) break res = os.popen(data).read() conn.send(res.encode(&quot;utf-8&quot;))server.close() #-- coding:utf-8 -- import socket #客户端python3.5client = socket.socket() #声明socket类型，同时生成socket连接对象client.connect((‘1.1.1.3’,6969)) while True: msg = input(“&gt;&gt;:”).strip() client.send(msg.encode(“utf-8”)) data = client.recv(1024) print(&apos;客户端接收到服务端命令结果:&apos;,data.decode())client.close() 2、 socket实现下载文件 1）先在服务器上把要传送的视屏文件打开，用send发送给客户端 2）在客户端上打开一个新文件，将收到数据写入到文件中 3）测试结果是：在客户端每次输入任意符号服务器端就会发送一个1024字节的文件到客户端 4）可以看到的结果是，文件以1024的倍数增加 import socket,os #socket服务器端server = socket.socket()server.bind((‘0.0.0.0’,2222))server.listen(5)while True: conn,addr = server.accept() while True: file_name = conn.recv(1024).decode() file_size = os.stat(file_name).st_size conn.send(str(file_size).encode(“utf-8”)) with open(file_name,’rb’) as f: for line in f: conn.send(line)server.close() import socket #socket客户端client = socket.socket()client.connect((‘1.1.1.3’,2222))while True: file_name = input(“please input you need to download file:”) client.send(file_name.encode(“utf-8”)) file_size = client.recv(1024).decode() print(type(file_size),file_size) file_size = int(file_size) recv_size = 0 with open(file_name + ‘.new’,’wb’) as f: while recv_size &lt; file_size: data = client.recv(1024) f.write(data) recv_size += 1024client.close() 1.4 通过socket实现简单的ssh 和 处理连包问题 返回顶部 1、说明 1. 先运行服务器端，再运行客户端，客户端连接后发送数据给服务器端，服务器端将结果返回给客户端 2. 这段代码在windows python3.5中运行和 centos6.5系统中运行都正常 3. 下面用Windows作为客户端，centos6.5作为服务器端演示操作结果 4. 运行发现如果客户端服务端都在centos中运行时，客户端断开服务器端会继续监听下一个连接 5. 但是如果客户端是Windows无论服务器端是什么系统客户端断开服务器端就会自动断开 2、socket实现ssh远程执行命令 及 处理连包问题 import socket,os #ssh服务器端代码 server = socket.socket()server.bind((‘0.0.0.0’,9999)) #0.0.0.0表示允许所有主机连接server.listen()print(“服务器端ssh开始监听客户端的链接啦！！”) while True: conn,addr = server.accept() #每次客户端断开，就会到这里等待新的连接 while True: print(“等待新指令！！”) client_cmd = conn.recv(1024).decode() #服务器端接收客户端发送过来的命令 if not client_cmd: #1 如果客户端断开连接收到的数据为空就会卡在这里 print(“客户端已断开连接”) break print(“执行命令：”,client_cmd) cmd_res = os.popen(client_cmd).read() #接收字符串，执行结果也是字符串 print(“before send:”,len(cmd_res)) #打印出命令执行结果的长度 if len(cmd_res) == 0: #2 如果命令错误则执行结果长度为 0 cmd_res = “The command is wrong!!” conn.send( str(len(cmd_res.encode())).encode(“utf-8”) ) #发送命令结果长度 client_ack = conn.recv(1024) #3 为了解决连包问题可以在两个send中间加一个recv conn.send(cmd_res.encode(“utf-8”)) #将命令执行结果发送给客户端 print(“send done”)server.close() import socket #ssh 客户端程序代码 client = socket.socket()client.connect((‘localhost’,9999)) while True: cmd = input(“请输入要执行的命令：”) if len(cmd) == 0:continue #1 客户端输入命令为空就不发送让他继续输入命令 client.send(cmd.encode(“utf-8”)) #将要执行命令发送到服务器 cmd_res_size = client.recv(1024).decode() #接收命令结果的长度 print(“执行命令结果的长度：”,cmd_res_size) #打印要接收结果的数据长度 client.send(‘ok’.encode(‘utf-8’)) #2 为解决连包问题可以在连续两次send中间加一个recv received_size = 0 #接收数据的长度初始值为 0 received_data = b’’ #接收到的数据初始值为 空字符串 while received_size &lt; int(cmd_res_size): data = client.recv(1024) #每次从服务器端接收1024的数据 received_size += len(data) #接收到的数据长度累加 received_data += data #每次接收的数据累加到 received_data变量中 else: print(“The command receive done:”,received_size) #打印最终接收的长度 print(received_data.decode()) #将接收到的命令执行结果打印出来client.close() #1 在centos6.5 10.1.0.50服务器端运行结果：[root@localhost python]# python35 ssh_server.sh服务器端ssh开始监听客户端的链接啦！！等待新指令！！执行命令： dfbefore send: 429send done等待新指令！！ #2 在Windows 7中运行客户端的结果：请输入要执行的命令：df执行命令结果的长度： 429The command receive done: 429Filesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/VolGroup-lv_root 38744716 4118012 32658576 12% /tmpfs 510148 224 509924 1% /dev/shm/dev/sda1 495844 34846 435398 8% /boot/dev/sr0 4363088 4363088 0 100% /media/CentOS_6.5_Final/dev/sr0 4363088 4363088 0 100% /mnt]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python设计模型]]></title>
    <url>%2F2017%2F06%2F28%2Fpython%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[mvc和mvt的区别mvc设计模式核心：解耦，让不同的代码块之间降低耦合，增强代码的可扩展和可移植性，实现向后兼容。MVC各部分的功能M全拼为Model，主要封装对数据库层的访问，对数据库中的数据进行增、删、改、查操作。 V全拼为View，用于封装结果，生成页面展示的html内容。 C全拼为Controller，用于接收请求，处理业务逻辑，与Model和View交互，返回结果。 Django中MVT设计模式Django框架遵循MVC设计MVT各部分的功能M全拼为Model，与MVC中的M功能相同，负责和数据库交互，进行数据处理。 V全拼为View，与MVC中的C功能相同，接收请求，进行业务处理，返回应答。 T全拼为Template，与MVC中的V功能相同，负责封装构造要返回的html。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask SQLAlchemy 的安装和基础应用]]></title>
    <url>%2F2017%2F06%2F28%2Fflask%20SQLAlchemy%20%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[SQLAlchemy是一个基于Python实现的ORM框架。该框架建立在 DB API之上，使用关系对象映射进行数据库操作，简言之便是：将类和对象转换成SQL，然后使用数据API执行SQL并获取执行结果。 安装命令 pip install flask-sqlalchemy 注意sqlalchemy 依赖于 pymysql 模块，确保pymysql 被正确安装 pip install pymysql #导入第三方连接库sql点金术 from flask_sqlalchemy import SQLAlchemy #建立对象 app = Flask(name) #载入配置文件 app.config.from_pyfile(‘config.ini’) #指定数据库连接还有库名app.config[‘SQLALCHEMY_DATABASE_URI’] = ‘mysql+pymysql://root:123456@127.0.0.1:3306/myflask?charset=utf8’#指定配置，用来省略提交操作 #app.config[‘SQLALCHEMY_COMMIT_ON_TEARDOWN’] = True #建立数据库对象 db = SQLAlchemy(app) #建立数据库类，用来映射数据库表,将数据库的模型作为参数传入 class User(db.Model): #声明表名 __tablename__ = &apos;user&apos; #建立字段函数 id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(200))password = db.Column(db.String(200))]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
</search>
